D:\Program\Anaconda\envs\pyg\python.exe F:/Project/BernNet/GraphClassification/main.py
--
IMDB-MULTI - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 1.1225502840677897, 'val_loss': 1.0994599533081055, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 2, 'train_loss': 1.099472336769104, 'val_loss': 1.098989855448405, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 3, 'train_loss': 1.099365963935852, 'val_loss': 1.0986195119222004, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 4, 'train_loss': 1.098416763941447, 'val_loss': 1.0986190032958985, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 5, 'train_loss': 1.0999998140335083, 'val_loss': 1.0986375935872397, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 6, 'train_loss': 1.098773627281189, 'val_loss': 1.0986319986979167, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 7, 'train_loss': 1.0989911588033041, 'val_loss': 1.0986817932128907, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 8, 'train_loss': 1.09916472752889, 'val_loss': 1.0986877314249675, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 9, 'train_loss': 1.0977402130762737, 'val_loss': 1.098662134806315, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 10, 'train_loss': 1.0991428327560424, 'val_loss': 1.0986999893188476, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 11, 'train_loss': 1.0998431412378946, 'val_loss': 1.0986141967773437, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 12, 'train_loss': 1.100187241236369, 'val_loss': 1.098689651489258, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 13, 'train_loss': 1.098201862970988, 'val_loss': 1.098664436340332, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 14, 'train_loss': 1.0989535236358643, 'val_loss': 1.0987643559773763, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 15, 'train_loss': 1.0983190854390463, 'val_loss': 1.0987887954711915, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 16, 'train_loss': 1.10088072458903, 'val_loss': 1.0988143539428712, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 17, 'train_loss': 1.0982849454879762, 'val_loss': 1.0987296040852865, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 18, 'train_loss': 1.0990475479761759, 'val_loss': 1.0988614781697592, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 19, 'train_loss': 1.1008837842941284, 'val_loss': 1.0987729517618816, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 20, 'train_loss': 1.0992029746373495, 'val_loss': 1.0988240432739258, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 21, 'train_loss': 1.0985657215118407, 'val_loss': 1.0986202494303385, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 22, 'train_loss': 1.0990595801671346, 'val_loss': 1.0986346435546874, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 23, 'train_loss': 1.09870130221049, 'val_loss': 1.0986267471313476, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 24, 'train_loss': 1.0989728212356566, 'val_loss': 1.0987657674153646, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 25, 'train_loss': 1.098865219751994, 'val_loss': 1.09866397857666, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 26, 'train_loss': 1.098798796335856, 'val_loss': 1.0986330286661783, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 27, 'train_loss': 1.0985209973653158, 'val_loss': 1.0986142222086588, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 28, 'train_loss': 1.0989686918258668, 'val_loss': 1.0988183085123697, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 29, 'train_loss': 1.0991395457585653, 'val_loss': 1.0986253102620442, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 30, 'train_loss': 1.098712059656779, 'val_loss': 1.0986261494954428, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 31, 'train_loss': 1.0986830631891886, 'val_loss': 1.0987207794189453, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 32, 'train_loss': 1.0987187242507934, 'val_loss': 1.098694928487142, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 33, 'train_loss': 1.0989548063278198, 'val_loss': 1.098652458190918, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 34, 'train_loss': 1.0988588698705037, 'val_loss': 1.0986638259887695, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 35, 'train_loss': 1.0988664229710896, 'val_loss': 1.098664410909017, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 36, 'train_loss': 1.0988951126734416, 'val_loss': 1.0987574768066406, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 37, 'train_loss': 1.0988949124018352, 'val_loss': 1.0986427052815755, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 38, 'train_loss': 1.0994836648305257, 'val_loss': 1.0986802546183267, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 39, 'train_loss': 1.0988341029485067, 'val_loss': 1.0986784235636393, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 40, 'train_loss': 1.098800902366638, 'val_loss': 1.0987662633260091, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 41, 'train_loss': 1.0986336469650269, 'val_loss': 1.0988276926676432, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 42, 'train_loss': 1.0988662687937418, 'val_loss': 1.098697992960612, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 43, 'train_loss': 1.0987813107172648, 'val_loss': 1.098680508931478, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 44, 'train_loss': 1.0982096449534098, 'val_loss': 1.0986388905843099, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 45, 'train_loss': 1.0987047386169433, 'val_loss': 1.0986666997273764, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 46, 'train_loss': 1.098344276746114, 'val_loss': 1.0986342493693033, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 47, 'train_loss': 1.0988987588882446, 'val_loss': 1.098632164001465, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 48, 'train_loss': 1.1003421290715536, 'val_loss': 1.09869322458903, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 49, 'train_loss': 1.0990506140391032, 'val_loss': 1.0986618932088217, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 50, 'train_loss': 1.0986879587173461, 'val_loss': 1.0986324946085613, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 51, 'train_loss': 1.0990920893351237, 'val_loss': 1.098631846110026, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 52, 'train_loss': 1.098621260325114, 'val_loss': 1.0986517969767253, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 53, 'train_loss': 1.0987851889928182, 'val_loss': 1.098681131998698, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 54, 'train_loss': 1.098709143002828, 'val_loss': 1.098653335571289, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 55, 'train_loss': 1.0985903231302898, 'val_loss': 1.0986169942220052, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 56, 'train_loss': 1.0989485057195028, 'val_loss': 1.0986449686686197, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 57, 'train_loss': 1.0989834547042847, 'val_loss': 1.0986884053548176, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 58, 'train_loss': 1.098629298210144, 'val_loss': 1.0986288833618163, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 59, 'train_loss': 1.0986855872472128, 'val_loss': 1.0987205123901367, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 60, 'train_loss': 1.0988915077845256, 'val_loss': 1.0987200037638347, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 61, 'train_loss': 1.0989260689417522, 'val_loss': 1.0986919403076172, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 62, 'train_loss': 1.0987725655237834, 'val_loss': 1.0986745834350586, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 63, 'train_loss': 1.0987603982289633, 'val_loss': 1.0986354064941406, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 64, 'train_loss': 1.0988028160730998, 'val_loss': 1.0986320622762045, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 65, 'train_loss': 1.0987828095753989, 'val_loss': 1.0987019856770834, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 66, 'train_loss': 1.0988200648625692, 'val_loss': 1.0986216990152995, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 67, 'train_loss': 1.098720334370931, 'val_loss': 1.0986163584391275, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 68, 'train_loss': 1.09883904616038, 'val_loss': 1.0986389923095703, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 69, 'train_loss': 1.0987311093012493, 'val_loss': 1.098651835123698, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 70, 'train_loss': 1.0992614618937175, 'val_loss': 1.098689422607422, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 71, 'train_loss': 1.0987635151545208, 'val_loss': 1.0986312993367513, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 72, 'train_loss': 1.098805702527364, 'val_loss': 1.0986362838745116, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 73, 'train_loss': 1.0988332064946493, 'val_loss': 1.0986895243326822, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 74, 'train_loss': 1.0987378517786661, 'val_loss': 1.098639882405599, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 75, 'train_loss': 1.0987972164154052, 'val_loss': 1.098638687133789, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 76, 'train_loss': 1.0987170394261678, 'val_loss': 1.0986555862426757, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 77, 'train_loss': 1.0988400316238403, 'val_loss': 1.098735122680664, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 78, 'train_loss': 1.0988198359807333, 'val_loss': 1.0987200419108072, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 79, 'train_loss': 1.0988547309239705, 'val_loss': 1.0986321767171223, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 80, 'train_loss': 1.0987328863143921, 'val_loss': 1.0986345291137696, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 81, 'train_loss': 1.0986993233362834, 'val_loss': 1.0986124420166015, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 82, 'train_loss': 1.0987755918502808, 'val_loss': 1.0986216481526692, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 83, 'train_loss': 1.098578683535258, 'val_loss': 1.098622169494629, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 84, 'train_loss': 1.0986979246139525, 'val_loss': 1.098724733988444, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 85, 'train_loss': 1.0989028724034626, 'val_loss': 1.098637835184733, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 86, 'train_loss': 1.0988458967208863, 'val_loss': 1.0986525090535482, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 87, 'train_loss': 1.09880655447642, 'val_loss': 1.0986181767781575, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 88, 'train_loss': 1.0986637004216513, 'val_loss': 1.098614018758138, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 89, 'train_loss': 1.0987616109848022, 'val_loss': 1.098645477294922, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 90, 'train_loss': 1.0987734492619832, 'val_loss': 1.0986521021525064, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 91, 'train_loss': 1.0986502663294475, 'val_loss': 1.0986217625935872, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 92, 'train_loss': 1.0986911137898763, 'val_loss': 1.0987108357747395, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 93, 'train_loss': 1.0987460803985596, 'val_loss': 1.0986690902709961, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 94, 'train_loss': 1.0991894785563152, 'val_loss': 1.0986179224650066, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 95, 'train_loss': 1.0987244526545206, 'val_loss': 1.0986302947998048, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 96, 'train_loss': 1.098806118965149, 'val_loss': 1.0986340967814128, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 97, 'train_loss': 1.0987308597564698, 'val_loss': 1.0986185963948567, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 98, 'train_loss': 1.09872687180837, 'val_loss': 1.0986466471354166, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 99, 'train_loss': 1.0988542477289835, 'val_loss': 1.098616854349772, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 100, 'train_loss': 1.098876231511434, 'val_loss': 1.0986305491129558, 'test_acc': 0.3333333333333333}
Val Loss: 1.0171, Test Accuracy: 0.439 ± 0.073, Duration: 11.446
Best result - 0.439 ± 0.073
--
IMDB-MULTI - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 1.093848991394043, 'val_loss': 1.0235558573404948, 'test_acc': 0.48}
{'fold': 9, 'epoch': 2, 'train_loss': 1.0197692028681438, 'val_loss': 1.0556324513753255, 'test_acc': 0.38}
{'fold': 9, 'epoch': 3, 'train_loss': 1.0057386795679728, 'val_loss': 1.0032933807373048, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 4, 'train_loss': 0.9760237534840902, 'val_loss': 0.9934592437744141, 'test_acc': 0.5133333333333333}
{'fold': 9, 'epoch': 5, 'train_loss': 0.9534811560312907, 'val_loss': 0.9816400782267253, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 6, 'train_loss': 0.9437866067886352, 'val_loss': 1.012229461669922, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 7, 'train_loss': 0.9694165666898091, 'val_loss': 1.0004940923055012, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 8, 'train_loss': 0.939475736618042, 'val_loss': 1.0057534662882488, 'test_acc': 0.5}
{'fold': 9, 'epoch': 9, 'train_loss': 0.9327049700419108, 'val_loss': 1.0307622273763022, 'test_acc': 0.52}
{'fold': 9, 'epoch': 10, 'train_loss': 0.9262559080123901, 'val_loss': 1.0577498881022136, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 11, 'train_loss': 0.9096364053090413, 'val_loss': 1.0463721084594726, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 12, 'train_loss': 0.9060239362716674, 'val_loss': 1.0447762044270834, 'test_acc': 0.5}
{'fold': 9, 'epoch': 13, 'train_loss': 0.8872481528917948, 'val_loss': 1.0903985468546549, 'test_acc': 0.5}
{'fold': 9, 'epoch': 14, 'train_loss': 0.8918959577878316, 'val_loss': 1.133429667154948, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 15, 'train_loss': 0.8771960202852885, 'val_loss': 1.1238629913330078, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 16, 'train_loss': 0.8768569882710775, 'val_loss': 1.1528103510538736, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 17, 'train_loss': 0.8591159256299337, 'val_loss': 1.1745675404866536, 'test_acc': 0.5133333333333333}
{'fold': 9, 'epoch': 18, 'train_loss': 0.8535832174619039, 'val_loss': 1.2006703948974609, 'test_acc': 0.48}
{'fold': 9, 'epoch': 19, 'train_loss': 0.8454843584696452, 'val_loss': 1.1751367441813152, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 20, 'train_loss': 0.85204239209493, 'val_loss': 1.2104246266682943, 'test_acc': 0.52}
{'fold': 9, 'epoch': 21, 'train_loss': 0.8380093502998353, 'val_loss': 1.2137178802490234, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 22, 'train_loss': 0.8467972000439962, 'val_loss': 1.2569030634562175, 'test_acc': 0.52}
{'fold': 9, 'epoch': 23, 'train_loss': 0.8315581862131755, 'val_loss': 1.2726533762613932, 'test_acc': 0.5133333333333333}
{'fold': 9, 'epoch': 24, 'train_loss': 0.8221746150652568, 'val_loss': 1.369176788330078, 'test_acc': 0.5133333333333333}
{'fold': 9, 'epoch': 25, 'train_loss': 0.8268099967638651, 'val_loss': 1.4004125467936197, 'test_acc': 0.52}
{'fold': 9, 'epoch': 26, 'train_loss': 0.8231489936510722, 'val_loss': 1.3458162307739259, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 27, 'train_loss': 0.8158541138966878, 'val_loss': 1.2887737274169921, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 28, 'train_loss': 0.8297437675793966, 'val_loss': 1.418003184000651, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 29, 'train_loss': 0.8132627749443054, 'val_loss': 1.463077850341797, 'test_acc': 0.52}
{'fold': 9, 'epoch': 30, 'train_loss': 0.790885481039683, 'val_loss': 1.49237912495931, 'test_acc': 0.5}
{'fold': 9, 'epoch': 31, 'train_loss': 0.7947391859690348, 'val_loss': 1.5898514302571614, 'test_acc': 0.48}
{'fold': 9, 'epoch': 32, 'train_loss': 0.7880102626482646, 'val_loss': 1.6114769999186198, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 33, 'train_loss': 0.7814894731839498, 'val_loss': 1.6104580688476562, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 34, 'train_loss': 0.7749421095848084, 'val_loss': 1.7009674835205078, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 35, 'train_loss': 0.7735221640268961, 'val_loss': 1.6416854095458984, 'test_acc': 0.48}
{'fold': 9, 'epoch': 36, 'train_loss': 0.770075794061025, 'val_loss': 1.6941502888997395, 'test_acc': 0.5}
{'fold': 9, 'epoch': 37, 'train_loss': 0.7643183962504069, 'val_loss': 1.8084446716308593, 'test_acc': 0.5133333333333333}
{'fold': 9, 'epoch': 38, 'train_loss': 0.7617719944318135, 'val_loss': 2.015964101155599, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 39, 'train_loss': 0.7669226320584616, 'val_loss': 1.6303291320800781, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 40, 'train_loss': 0.769290014108022, 'val_loss': 1.7878733825683595, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 41, 'train_loss': 0.7898270479838053, 'val_loss': 1.6961759185791017, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 42, 'train_loss': 0.7629345615704854, 'val_loss': 1.816281255086263, 'test_acc': 0.48}
{'fold': 9, 'epoch': 43, 'train_loss': 0.7540324298540751, 'val_loss': 1.8914737447102865, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 44, 'train_loss': 0.7576048270861307, 'val_loss': 1.8320975494384766, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 45, 'train_loss': 0.7622683310508728, 'val_loss': 1.9958670552571613, 'test_acc': 0.5}
{'fold': 9, 'epoch': 46, 'train_loss': 0.7633795062700908, 'val_loss': 1.8413274383544922, 'test_acc': 0.5}
{'fold': 9, 'epoch': 47, 'train_loss': 0.75452312707901, 'val_loss': 2.029399668375651, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 48, 'train_loss': 0.750004551410675, 'val_loss': 2.0162913767496744, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 49, 'train_loss': 0.76427565574646, 'val_loss': 2.034836171468099, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 50, 'train_loss': 0.7419466121991476, 'val_loss': 2.050218098958333, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 51, 'train_loss': 0.7465365171432495, 'val_loss': 1.9743853505452473, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 52, 'train_loss': 0.7507435059547425, 'val_loss': 2.062691090901693, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 53, 'train_loss': 0.7444903826713563, 'val_loss': 2.203611119588216, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 54, 'train_loss': 0.7460908230145772, 'val_loss': 2.049738108317057, 'test_acc': 0.48}
{'fold': 9, 'epoch': 55, 'train_loss': 0.7453917813301086, 'val_loss': 2.198115259806315, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 56, 'train_loss': 0.7396402192115784, 'val_loss': 2.3432120768229168, 'test_acc': 0.5}
{'fold': 9, 'epoch': 57, 'train_loss': 0.7405295022328695, 'val_loss': 2.1292362721761067, 'test_acc': 0.48}
{'fold': 9, 'epoch': 58, 'train_loss': 0.7393794218699138, 'val_loss': 2.2042383575439453, 'test_acc': 0.4666666666666667}
{'fold': 9, 'epoch': 59, 'train_loss': 0.7362543924649556, 'val_loss': 2.232835693359375, 'test_acc': 0.5}
{'fold': 9, 'epoch': 60, 'train_loss': 0.7300616145133972, 'val_loss': 2.351733678181966, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 61, 'train_loss': 0.7408471934000651, 'val_loss': 2.309918975830078, 'test_acc': 0.48}
{'fold': 9, 'epoch': 62, 'train_loss': 0.7432579874992371, 'val_loss': 2.258195546468099, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 63, 'train_loss': 0.747141444683075, 'val_loss': 2.4395379130045574, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 64, 'train_loss': 0.7454607446988424, 'val_loss': 2.2016363525390625, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 65, 'train_loss': 0.7430332628885905, 'val_loss': 2.3737230428059894, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 66, 'train_loss': 0.7375367514292399, 'val_loss': 2.362848917643229, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 67, 'train_loss': 0.7396187901496887, 'val_loss': 2.133974304199219, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 68, 'train_loss': 0.7296677716573079, 'val_loss': 2.2940858205159507, 'test_acc': 0.48}
{'fold': 9, 'epoch': 69, 'train_loss': 0.7336436931292216, 'val_loss': 2.355360666910807, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 70, 'train_loss': 0.7366464678446452, 'val_loss': 2.3691238657633464, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 71, 'train_loss': 0.728873430887858, 'val_loss': 2.580067240397135, 'test_acc': 0.48}
{'fold': 9, 'epoch': 72, 'train_loss': 0.7327215075492859, 'val_loss': 2.6913579813639323, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 73, 'train_loss': 0.728418341477712, 'val_loss': 2.764665069580078, 'test_acc': 0.48}
{'fold': 9, 'epoch': 74, 'train_loss': 0.7346615688006083, 'val_loss': 2.8289276123046876, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 75, 'train_loss': 0.7273061347007751, 'val_loss': 2.6023699951171877, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 76, 'train_loss': 0.7290256031354269, 'val_loss': 2.631974894205729, 'test_acc': 0.48}
{'fold': 9, 'epoch': 77, 'train_loss': 0.7237493459383647, 'val_loss': 2.663097890218099, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 78, 'train_loss': 0.7136791507403056, 'val_loss': 2.8721691385904946, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 79, 'train_loss': 0.7185402099291484, 'val_loss': 2.732857971191406, 'test_acc': 0.46}
{'fold': 9, 'epoch': 80, 'train_loss': 0.716754359404246, 'val_loss': 2.8223787943522134, 'test_acc': 0.46}
{'fold': 9, 'epoch': 81, 'train_loss': 0.7375345126787821, 'val_loss': 2.6399532063802083, 'test_acc': 0.4666666666666667}
{'fold': 9, 'epoch': 82, 'train_loss': 0.7269734851519267, 'val_loss': 2.894068044026693, 'test_acc': 0.48}
{'fold': 9, 'epoch': 83, 'train_loss': 0.7258316270510355, 'val_loss': 2.927287292480469, 'test_acc': 0.48}
{'fold': 9, 'epoch': 84, 'train_loss': 0.7293380697568258, 'val_loss': 2.9231166585286457, 'test_acc': 0.48}
{'fold': 9, 'epoch': 85, 'train_loss': 0.7307847841580709, 'val_loss': 3.0715569559733074, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 86, 'train_loss': 0.7285299499829611, 'val_loss': 3.03233159383138, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 87, 'train_loss': 0.723056743144989, 'val_loss': 2.9086175537109376, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 88, 'train_loss': 0.7346002666155497, 'val_loss': 2.8756666056315106, 'test_acc': 0.5}
{'fold': 9, 'epoch': 89, 'train_loss': 0.7712243898709615, 'val_loss': 2.191847381591797, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 90, 'train_loss': 0.7551764829953511, 'val_loss': 1.6529821014404298, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 91, 'train_loss': 0.7859123508135478, 'val_loss': 1.7126869328816732, 'test_acc': 0.4866666666666667}
{'fold': 9, 'epoch': 92, 'train_loss': 0.7697394506136577, 'val_loss': 1.785063273111979, 'test_acc': 0.49333333333333335}
{'fold': 9, 'epoch': 93, 'train_loss': 0.7700719507535299, 'val_loss': 1.8435183715820314, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 94, 'train_loss': 0.7479861958821614, 'val_loss': 2.119897486368815, 'test_acc': 0.5133333333333333}
{'fold': 9, 'epoch': 95, 'train_loss': 0.7565412966410319, 'val_loss': 1.9907519022623699, 'test_acc': 0.5066666666666667}
{'fold': 9, 'epoch': 96, 'train_loss': 0.7531328320503234, 'val_loss': 1.9039036051432292, 'test_acc': 0.4666666666666667}
{'fold': 9, 'epoch': 97, 'train_loss': 0.7344450052579244, 'val_loss': 2.040600954691569, 'test_acc': 0.48}
{'fold': 9, 'epoch': 98, 'train_loss': 0.7280101958910624, 'val_loss': 2.2347932561238606, 'test_acc': 0.4666666666666667}
{'fold': 9, 'epoch': 99, 'train_loss': 0.7270209797223409, 'val_loss': 2.1168338521321615, 'test_acc': 0.47333333333333333}
{'fold': 9, 'epoch': 100, 'train_loss': 0.7324949256579081, 'val_loss': 2.3096755599975585, 'test_acc': 0.47333333333333333}
Val Loss: 0.9517, Test Accuracy: 0.492 ± 0.035, Duration: 82.751
Best result - 0.492 ± 0.035
--
MUTAG - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.7895923476470145, 'val_loss': 0.6747323671976725, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 2, 'train_loss': 0.675524934342033, 'val_loss': 0.5808238983154297, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6570026749058774, 'val_loss': 0.5187607871161567, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5729025225890311, 'val_loss': 0.6259625752766927, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6486611836834958, 'val_loss': 0.6090921825832791, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6166882860033136, 'val_loss': 0.46099445554945206, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5179409306300314, 'val_loss': 0.29906217257181805, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 8, 'train_loss': 0.5381279572060234, 'val_loss': 0.3292587333255344, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 9, 'train_loss': 0.4689665468115556, 'val_loss': 0.2917991744147407, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 10, 'train_loss': 0.4438927518694024, 'val_loss': 0.3843850824568007, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 11, 'train_loss': 0.4652775212338096, 'val_loss': 0.3829986784193251, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4620974330525649, 'val_loss': 0.30764150619506836, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 13, 'train_loss': 0.41460198477694865, 'val_loss': 0.2599625587463379, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 14, 'train_loss': 0.41336309282403244, 'val_loss': 0.2627212206522624, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 15, 'train_loss': 0.4015804089997944, 'val_loss': 0.277652104695638, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 16, 'train_loss': 0.44238106200569555, 'val_loss': 0.3352835443284776, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 17, 'train_loss': 0.4249767896376158, 'val_loss': 0.3555171489715576, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 18, 'train_loss': 0.4317012884114918, 'val_loss': 0.3228721883561876, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 19, 'train_loss': 0.41110852988142715, 'val_loss': 0.2818012237548828, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 20, 'train_loss': 0.40903955384304647, 'val_loss': 0.2755858898162842, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 21, 'train_loss': 0.3927613167386306, 'val_loss': 0.27573773596021867, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 22, 'train_loss': 0.3687810772343686, 'val_loss': 0.30364439222547746, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 23, 'train_loss': 0.3818366151106985, 'val_loss': 0.33646874957614475, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 24, 'train_loss': 0.3797168770903035, 'val_loss': 0.3018498685624864, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 25, 'train_loss': 0.3801658875063846, 'val_loss': 0.3052213986714681, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 26, 'train_loss': 0.3831012594072442, 'val_loss': 0.2861597802903917, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 27, 'train_loss': 0.35072324307341324, 'val_loss': 0.3122650782267253, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 28, 'train_loss': 0.3603481760150508, 'val_loss': 0.3228029939863417, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 29, 'train_loss': 0.36998362447086136, 'val_loss': 0.3079799281226264, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 30, 'train_loss': 0.3541817131795381, 'val_loss': 0.3043190903133816, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 31, 'train_loss': 0.34921388720211227, 'val_loss': 0.326026095284356, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 32, 'train_loss': 0.32533619435209976, 'val_loss': 0.35074424743652344, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 33, 'train_loss': 0.35959237657095255, 'val_loss': 0.33807627360026044, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 34, 'train_loss': 0.33996512544782537, 'val_loss': 0.31973634825812447, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 35, 'train_loss': 0.3476786535037191, 'val_loss': 0.3077545960744222, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 36, 'train_loss': 0.3270879356484664, 'val_loss': 0.30648189120822483, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 37, 'train_loss': 0.3494341200903842, 'val_loss': 0.3173171944088406, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 38, 'train_loss': 0.32758453174641256, 'val_loss': 0.3304157257080078, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 39, 'train_loss': 0.3393710388949043, 'val_loss': 0.36331190003289116, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 40, 'train_loss': 0.33327368529219376, 'val_loss': 0.3969258732265896, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 41, 'train_loss': 0.29930470491710465, 'val_loss': 0.3984273009830051, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 42, 'train_loss': 0.3095806366518924, 'val_loss': 0.3735072612762451, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 43, 'train_loss': 0.31045442976449666, 'val_loss': 0.35721198717753094, 'test_acc': 1.0}
{'fold': 9, 'epoch': 44, 'train_loss': 0.33895842025154516, 'val_loss': 0.36006871859232586, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 45, 'train_loss': 0.3279903217365867, 'val_loss': 0.39699750476413304, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 46, 'train_loss': 0.32099872200112595, 'val_loss': 0.4537004364861382, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 47, 'train_loss': 0.3024249500349948, 'val_loss': 0.4486691686842177, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 48, 'train_loss': 0.30154086806272207, 'val_loss': 0.4325666692521837, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 49, 'train_loss': 0.3107198900298068, 'val_loss': 0.41117172771030003, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 50, 'train_loss': 0.3105898507331547, 'val_loss': 0.41245166460673016, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 51, 'train_loss': 0.3221129508394944, 'val_loss': 0.4665578206380208, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 52, 'train_loss': 0.30300597219090714, 'val_loss': 0.524261368645562, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 53, 'train_loss': 0.3017815445598803, 'val_loss': 0.5274559656778971, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 54, 'train_loss': 0.29979324183965983, 'val_loss': 0.4377257294125027, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 55, 'train_loss': 0.30306005948468256, 'val_loss': 0.3769826359219021, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 56, 'train_loss': 0.3635013056428809, 'val_loss': 0.4300752745734321, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 57, 'train_loss': 0.2942699303752498, 'val_loss': 0.5820590125189887, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 58, 'train_loss': 0.349745722193467, 'val_loss': 0.5998688803778754, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 59, 'train_loss': 0.2773825604664652, 'val_loss': 0.56058136622111, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 60, 'train_loss': 0.33630844950675964, 'val_loss': 0.49282217025756836, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 61, 'train_loss': 0.2746589889651851, 'val_loss': 0.4809612168206109, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 62, 'train_loss': 0.30600391406761973, 'val_loss': 0.4809695349799262, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 63, 'train_loss': 0.30783700629284505, 'val_loss': 0.48821836047702366, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 64, 'train_loss': 0.28747469892627314, 'val_loss': 0.527400811513265, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 65, 'train_loss': 0.28936887257977534, 'val_loss': 0.5604033999972873, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 66, 'train_loss': 0.2629817975194831, 'val_loss': 0.5928800370958116, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 67, 'train_loss': 0.2982970585948543, 'val_loss': 0.5570120281643338, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 68, 'train_loss': 0.2995184534474423, 'val_loss': 0.5356315506829156, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 69, 'train_loss': 0.2804855555295944, 'val_loss': 0.4178430239359538, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 70, 'train_loss': 0.29220309461417954, 'val_loss': 0.44999392827351886, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 71, 'train_loss': 0.27324180226576955, 'val_loss': 0.44287027253044975, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 72, 'train_loss': 0.26450608984420176, 'val_loss': 0.07219062911139594, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 73, 'train_loss': 0.2951176864536185, 'val_loss': 0.05737349722120497, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 74, 'train_loss': 0.27787469092168304, 'val_loss': 1.0608303281995985, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 75, 'train_loss': 0.3085359021237022, 'val_loss': 1.2308375040690105, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 76, 'train_loss': 0.296841688846287, 'val_loss': 0.7957130008273654, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 77, 'train_loss': 0.3062496608809421, 'val_loss': 0.5863793161180284, 'test_acc': 0.9444444444444444}
{'fold': 9, 'epoch': 78, 'train_loss': 0.2739963641292171, 'val_loss': 0.5025408532884386, 'test_acc': 0.9444444444444444}
{'fold': 9, 'epoch': 79, 'train_loss': 0.3080325252131412, 'val_loss': 0.49230464299519855, 'test_acc': 0.9444444444444444}
{'fold': 9, 'epoch': 80, 'train_loss': 0.30062762530226456, 'val_loss': 0.5582373407151964, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 81, 'train_loss': 0.2779440519056822, 'val_loss': 0.6076725853814019, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 82, 'train_loss': 0.2663433700799942, 'val_loss': 0.6141217549641927, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 83, 'train_loss': 0.2587764153355046, 'val_loss': 0.6650271415710449, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 84, 'train_loss': 0.2893920776091124, 'val_loss': 0.6914802657233344, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 85, 'train_loss': 0.27633614445987503, 'val_loss': 0.6707077556186252, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 86, 'train_loss': 0.2691672491399865, 'val_loss': 0.6068299611409506, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 87, 'train_loss': 0.2608265100341094, 'val_loss': 0.49196285671657985, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 88, 'train_loss': 0.2637846297339389, 'val_loss': 0.25627607769436306, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 89, 'train_loss': 0.2556800222710559, 'val_loss': 0.11314260959625244, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 90, 'train_loss': 0.26450093012107045, 'val_loss': 0.09058352973726061, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 91, 'train_loss': 0.2423564187790218, 'val_loss': 0.08049852318233913, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 92, 'train_loss': 0.278901449159572, 'val_loss': 0.31265105141533744, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 93, 'train_loss': 0.2775784244662837, 'val_loss': 0.40599849489000106, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 94, 'train_loss': 0.2546183223787107, 'val_loss': 0.44774696562025285, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 95, 'train_loss': 0.2760431358688756, 'val_loss': 0.5480536884731717, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 96, 'train_loss': 0.2656374432538685, 'val_loss': 0.7050109969245063, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 97, 'train_loss': 0.27666840741508886, 'val_loss': 0.5689958996242948, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 98, 'train_loss': 0.27527744911218943, 'val_loss': 0.5088661511739095, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 99, 'train_loss': 0.25030049524809184, 'val_loss': 0.5295085377163358, 'test_acc': 0.7777777777777778}
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 100, 'train_loss': 0.25151439011096954, 'val_loss': 0.468712223900689, 'test_acc': 0.7777777777777778}
Val Loss: 0.2755, Test Accuracy: 0.809 ± 0.103, Duration: 1.657
Best result - 0.809 ± 0.103
--
MUTAG - ASAP
{'fold': 9, 'epoch': 1, 'train_loss': 0.7108004344137091, 'val_loss': 0.6380897098117404, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6543565706202858, 'val_loss': 0.6296021673414443, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6390674114227295, 'val_loss': 0.6297452714708116, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 4, 'train_loss': 0.644873990824348, 'val_loss': 0.6249393886990018, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 5, 'train_loss': 0.647339331476312, 'val_loss': 0.6213019159105089, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6143087807454561, 'val_loss': 0.6134049627516005, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 7, 'train_loss': 0.6090607517643979, 'val_loss': 0.6078947914971246, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 8, 'train_loss': 0.6203795891059073, 'val_loss': 0.5995135307312012, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 9, 'train_loss': 0.60293258805024, 'val_loss': 0.5917273627387153, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5945313635625338, 'val_loss': 0.5811900562710233, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 11, 'train_loss': 0.581496348506526, 'val_loss': 0.5809396107991537, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5761827701016476, 'val_loss': 0.562103377448188, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5677780007061205, 'val_loss': 0.5471033520168729, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5587545915653831, 'val_loss': 0.5429989496866862, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5340882445636549, 'val_loss': 0.5723990334404839, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5497865425912958, 'val_loss': 0.537977483537462, 'test_acc': 0.5}
{'fold': 9, 'epoch': 17, 'train_loss': 0.5285371322380868, 'val_loss': 0.5375527805752225, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5294871079294305, 'val_loss': 0.532623291015625, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5165147373550817, 'val_loss': 0.5427663061353896, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5494515064515566, 'val_loss': 0.5288808080885146, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 21, 'train_loss': 0.525766804030067, 'val_loss': 0.5290043089124892, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 22, 'train_loss': 0.5061576162513933, 'val_loss': 0.5291824870639377, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 23, 'train_loss': 0.5193175187236384, 'val_loss': 0.5224181811014811, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 24, 'train_loss': 0.522477355442549, 'val_loss': 0.5303758515252007, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 25, 'train_loss': 0.5108271601953005, 'val_loss': 0.5804086261325412, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 26, 'train_loss': 0.6023854581933272, 'val_loss': 0.5408948262532552, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 27, 'train_loss': 0.5318970303786429, 'val_loss': 0.5479261610243056, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 28, 'train_loss': 0.5566581738622565, 'val_loss': 0.5899751981099447, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 29, 'train_loss': 0.5863324011626997, 'val_loss': 0.5390612284342448, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 30, 'train_loss': 0.5290705185187491, 'val_loss': 0.5074215994940864, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 31, 'train_loss': 0.5317601404691997, 'val_loss': 0.5235228538513184, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 32, 'train_loss': 0.520655914356834, 'val_loss': 0.5189492437574599, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 33, 'train_loss': 0.5475927403098658, 'val_loss': 0.5005836486816406, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 34, 'train_loss': 0.4993033644400145, 'val_loss': 0.4955296516418457, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 35, 'train_loss': 0.5226243546134547, 'val_loss': 0.4939291212293837, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 36, 'train_loss': 0.5148325876185769, 'val_loss': 0.49177201588948566, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 37, 'train_loss': 0.511252412670537, 'val_loss': 0.49422454833984375, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 38, 'train_loss': 0.5010658220240944, 'val_loss': 0.49638859430948895, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 39, 'train_loss': 0.49446302024941696, 'val_loss': 0.49798944261338973, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 40, 'train_loss': 0.5054589133513602, 'val_loss': 0.49926169713338214, 'test_acc': 0.5}
{'fold': 9, 'epoch': 41, 'train_loss': 0.47137134012423065, 'val_loss': 0.5058988995022244, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 42, 'train_loss': 0.4942694635767686, 'val_loss': 0.5041479534573026, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 43, 'train_loss': 0.5205227513062326, 'val_loss': 0.49488740497165257, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 44, 'train_loss': 0.48654981663352564, 'val_loss': 0.5221367412143283, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 45, 'train_loss': 0.4919103258534482, 'val_loss': 0.5151507589552138, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 46, 'train_loss': 0.5250686187493173, 'val_loss': 0.48227882385253906, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 47, 'train_loss': 0.49269907568630417, 'val_loss': 0.46065441767374676, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 48, 'train_loss': 0.4727698595900285, 'val_loss': 0.4594331847296821, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 49, 'train_loss': 0.47886092411844355, 'val_loss': 0.4642856915791829, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 50, 'train_loss': 0.49980274783937556, 'val_loss': 0.4843286938137478, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 51, 'train_loss': 0.49895846686865153, 'val_loss': 0.5090618133544922, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 52, 'train_loss': 0.5090029365138004, 'val_loss': 0.4720387988620334, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 53, 'train_loss': 0.4898098785626261, 'val_loss': 0.45433976915147567, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 54, 'train_loss': 0.49060896039009094, 'val_loss': 0.45510551664564347, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 55, 'train_loss': 0.47418465426093653, 'val_loss': 0.4733657307094998, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 56, 'train_loss': 0.45607703139907435, 'val_loss': 0.484179072909885, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 57, 'train_loss': 0.46678227656765986, 'val_loss': 0.4649960199991862, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 58, 'train_loss': 0.4763658627083427, 'val_loss': 0.45990493562486434, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 59, 'train_loss': 0.4563356355616921, 'val_loss': 0.48588249418470597, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 60, 'train_loss': 0.48330429353212057, 'val_loss': 0.5003844896952311, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 61, 'train_loss': 0.47497432169161347, 'val_loss': 0.4949763086107042, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 62, 'train_loss': 0.4505977128681384, 'val_loss': 0.4819307327270508, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 63, 'train_loss': 0.4564643549291711, 'val_loss': 0.4633297920227051, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 64, 'train_loss': 0.4641185434241044, 'val_loss': 0.4948672718471951, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 65, 'train_loss': 0.47551930891840083, 'val_loss': 0.48401223288642037, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 66, 'train_loss': 0.4600735196941777, 'val_loss': 0.4226747618781196, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 67, 'train_loss': 0.4815107270290977, 'val_loss': 0.405637264251709, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 68, 'train_loss': 0.41955289872069107, 'val_loss': 0.48120805952284074, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 69, 'train_loss': 0.4785901559026618, 'val_loss': 0.45467498567369247, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 70, 'train_loss': 0.4940416561929803, 'val_loss': 0.3905472490522597, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 71, 'train_loss': 0.46548307569403397, 'val_loss': 0.4126509295569526, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 72, 'train_loss': 0.4502506365901546, 'val_loss': 0.4631614685058594, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 73, 'train_loss': 0.46687165686958715, 'val_loss': 0.4580409791734483, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 74, 'train_loss': 0.4492591995941965, 'val_loss': 0.43354158931308323, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 75, 'train_loss': 0.45479434571768107, 'val_loss': 0.4270212650299072, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 76, 'train_loss': 0.4520578368713981, 'val_loss': 0.4223041269514296, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 77, 'train_loss': 0.4313045238193713, 'val_loss': 0.43930233849419487, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 78, 'train_loss': 0.44523680053259196, 'val_loss': 0.45900747511121964, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 79, 'train_loss': 0.46711824755919606, 'val_loss': 0.4147183100382487, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 80, 'train_loss': 0.41029339558199834, 'val_loss': 0.37748683823479545, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 81, 'train_loss': 0.5095897285561812, 'val_loss': 0.3997970157199436, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 82, 'train_loss': 0.43444713165885523, 'val_loss': 0.42252418729994035, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 83, 'train_loss': 0.40595619145192596, 'val_loss': 0.4098716576894124, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 84, 'train_loss': 0.39399639556282445, 'val_loss': 0.4001448154449463, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 85, 'train_loss': 0.39420209589757416, 'val_loss': 0.4022224214341905, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 86, 'train_loss': 0.42390915908311544, 'val_loss': 0.3928348223368327, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 87, 'train_loss': 0.39784490905309977, 'val_loss': 0.42302974065144855, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 88, 'train_loss': 0.4072539853422265, 'val_loss': 0.41472824414571124, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 89, 'train_loss': 0.4040487596863194, 'val_loss': 0.441478172938029, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 90, 'train_loss': 0.3958516575788197, 'val_loss': 0.508941650390625, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 91, 'train_loss': 0.38963071609798233, 'val_loss': 0.45033815171983504, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 92, 'train_loss': 0.3702147838316466, 'val_loss': 0.4049577448103163, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 93, 'train_loss': 0.35374809252588374, 'val_loss': 0.4265761905246311, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 94, 'train_loss': 0.3979754196970086, 'val_loss': 0.4497627152336968, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 95, 'train_loss': 0.39592867932821574, 'val_loss': 0.40282408396402997, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 96, 'train_loss': 0.3812088260525151, 'val_loss': 0.3608526918623183, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 97, 'train_loss': 0.3864416119299437, 'val_loss': 0.3415360715654161, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 98, 'train_loss': 0.38704629634556015, 'val_loss': 0.37381137741936576, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 99, 'train_loss': 0.3626374194496556, 'val_loss': 0.43973517417907715, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 100, 'train_loss': 0.4010759897922215, 'val_loss': 0.4250406689114041, 'test_acc': 0.8333333333333334}
Val Loss: 0.3482, Test Accuracy: 0.792 ± 0.069, Duration: 20.131
Best result - 0.792 ± 0.069
--
IMDB-BINARY - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6968604755401612, 'val_loss': 0.6878384399414063, 'test_acc': 0.5}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6469675302505493, 'val_loss': 0.5454815673828125, 'test_acc': 0.68}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6319160461425781, 'val_loss': 0.7043892669677735, 'test_acc': 0.68}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5886640810966491, 'val_loss': 0.5678991317749024, 'test_acc': 0.71}
{'fold': 9, 'epoch': 5, 'train_loss': 0.5390993702411652, 'val_loss': 0.5341485214233398, 'test_acc': 0.7}
{'fold': 9, 'epoch': 6, 'train_loss': 0.4946508812904358, 'val_loss': 0.6204104614257813, 'test_acc': 0.72}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5411054635047913, 'val_loss': 0.4890179824829102, 'test_acc': 0.69}
{'fold': 9, 'epoch': 8, 'train_loss': 0.47937885761260984, 'val_loss': 0.6043574523925781, 'test_acc': 0.71}
{'fold': 9, 'epoch': 9, 'train_loss': 0.47692461729049684, 'val_loss': 0.4914437866210937, 'test_acc': 0.7}
{'fold': 9, 'epoch': 10, 'train_loss': 0.47294620513916014, 'val_loss': 0.4853177261352539, 'test_acc': 0.72}
{'fold': 9, 'epoch': 11, 'train_loss': 0.44544485688209534, 'val_loss': 0.6719155120849609, 'test_acc': 0.67}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4516515350341797, 'val_loss': 0.47667678833007815, 'test_acc': 0.66}
{'fold': 9, 'epoch': 13, 'train_loss': 0.42267284512519837, 'val_loss': 0.5589098358154296, 'test_acc': 0.79}
{'fold': 9, 'epoch': 14, 'train_loss': 0.3958883082866669, 'val_loss': 0.517598876953125, 'test_acc': 0.74}
{'fold': 9, 'epoch': 15, 'train_loss': 0.37657920360565184, 'val_loss': 0.881580581665039, 'test_acc': 0.72}
{'fold': 9, 'epoch': 16, 'train_loss': 0.4390147876739502, 'val_loss': 0.5268889236450195, 'test_acc': 0.75}
{'fold': 9, 'epoch': 17, 'train_loss': 0.4315506684780121, 'val_loss': 0.6707487487792969, 'test_acc': 0.65}
{'fold': 9, 'epoch': 18, 'train_loss': 0.4331038951873779, 'val_loss': 0.6098205947875976, 'test_acc': 0.66}
{'fold': 9, 'epoch': 19, 'train_loss': 0.4318882179260254, 'val_loss': 0.5423447036743164, 'test_acc': 0.7}
{'fold': 9, 'epoch': 20, 'train_loss': 0.4043916189670563, 'val_loss': 0.6762232971191406, 'test_acc': 0.7}
{'fold': 9, 'epoch': 21, 'train_loss': 0.36434054136276245, 'val_loss': 0.961382827758789, 'test_acc': 0.71}
{'fold': 9, 'epoch': 22, 'train_loss': 0.4132491147518158, 'val_loss': 0.7279006958007812, 'test_acc': 0.69}
{'fold': 9, 'epoch': 23, 'train_loss': 0.3944146180152893, 'val_loss': 1.2599501037597656, 'test_acc': 0.66}
{'fold': 9, 'epoch': 24, 'train_loss': 0.3621384584903717, 'val_loss': 0.5774358749389649, 'test_acc': 0.75}
{'fold': 9, 'epoch': 25, 'train_loss': 0.364679639339447, 'val_loss': 0.7697104644775391, 'test_acc': 0.7}
{'fold': 9, 'epoch': 26, 'train_loss': 0.3511127030849457, 'val_loss': 0.9615027618408203, 'test_acc': 0.7}
{'fold': 9, 'epoch': 27, 'train_loss': 0.33251344084739687, 'val_loss': 0.6911898803710937, 'test_acc': 0.73}
{'fold': 9, 'epoch': 28, 'train_loss': 0.32844919204711914, 'val_loss': 1.0175276184082032, 'test_acc': 0.72}
{'fold': 9, 'epoch': 29, 'train_loss': 0.33051748991012575, 'val_loss': 0.6284224319458008, 'test_acc': 0.75}
{'fold': 9, 'epoch': 30, 'train_loss': 0.3480805575847626, 'val_loss': 0.6522978210449218, 'test_acc': 0.7}
{'fold': 9, 'epoch': 31, 'train_loss': 0.3390693175792694, 'val_loss': 0.8061472320556641, 'test_acc': 0.71}
{'fold': 9, 'epoch': 32, 'train_loss': 0.31846527695655824, 'val_loss': 1.3105593872070314, 'test_acc': 0.71}
{'fold': 9, 'epoch': 33, 'train_loss': 0.3371131920814514, 'val_loss': 0.7064793395996094, 'test_acc': 0.68}
{'fold': 9, 'epoch': 34, 'train_loss': 0.32188225984573365, 'val_loss': 0.7314625549316406, 'test_acc': 0.74}
{'fold': 9, 'epoch': 35, 'train_loss': 0.36143860578536985, 'val_loss': 0.5083930969238282, 'test_acc': 0.73}
{'fold': 9, 'epoch': 36, 'train_loss': 0.32475742101669314, 'val_loss': 0.7851862335205078, 'test_acc': 0.71}
{'fold': 9, 'epoch': 37, 'train_loss': 0.3545380234718323, 'val_loss': 0.6528472900390625, 'test_acc': 0.71}
{'fold': 9, 'epoch': 38, 'train_loss': 0.3229814255237579, 'val_loss': 1.0072142791748047, 'test_acc': 0.71}
{'fold': 9, 'epoch': 39, 'train_loss': 0.3143220114707947, 'val_loss': 1.1309729766845704, 'test_acc': 0.73}
{'fold': 9, 'epoch': 40, 'train_loss': 0.31004195153713227, 'val_loss': 1.0935755920410157, 'test_acc': 0.73}
{'fold': 9, 'epoch': 41, 'train_loss': 0.29728275537490845, 'val_loss': 1.1933631134033202, 'test_acc': 0.75}
{'fold': 9, 'epoch': 42, 'train_loss': 0.2916861987113953, 'val_loss': 1.4670396423339844, 'test_acc': 0.7}
{'fold': 9, 'epoch': 43, 'train_loss': 0.2841592597961426, 'val_loss': 1.7611434936523438, 'test_acc': 0.71}
{'fold': 9, 'epoch': 44, 'train_loss': 0.29089908123016356, 'val_loss': 1.8168736267089844, 'test_acc': 0.71}
{'fold': 9, 'epoch': 45, 'train_loss': 0.3006765866279602, 'val_loss': 1.596278533935547, 'test_acc': 0.73}
{'fold': 9, 'epoch': 46, 'train_loss': 0.31178486466407773, 'val_loss': 1.5557350158691405, 'test_acc': 0.71}
{'fold': 9, 'epoch': 47, 'train_loss': 0.2917380499839783, 'val_loss': 1.9153485107421875, 'test_acc': 0.75}
{'fold': 9, 'epoch': 48, 'train_loss': 0.3023055177927017, 'val_loss': 1.503048858642578, 'test_acc': 0.71}
{'fold': 9, 'epoch': 49, 'train_loss': 0.2849573528766632, 'val_loss': 2.1772061157226563, 'test_acc': 0.73}
{'fold': 9, 'epoch': 50, 'train_loss': 0.28696372985839846, 'val_loss': 1.4330741882324218, 'test_acc': 0.73}
{'fold': 9, 'epoch': 51, 'train_loss': 0.2821132230758667, 'val_loss': 1.538558807373047, 'test_acc': 0.72}
{'fold': 9, 'epoch': 52, 'train_loss': 0.3645751202106476, 'val_loss': 0.746035385131836, 'test_acc': 0.74}
{'fold': 9, 'epoch': 53, 'train_loss': 0.49652372002601625, 'val_loss': 0.4230430221557617, 'test_acc': 0.72}
{'fold': 9, 'epoch': 54, 'train_loss': 0.40046111583709715, 'val_loss': 0.7751987457275391, 'test_acc': 0.7}
{'fold': 9, 'epoch': 55, 'train_loss': 0.407527060508728, 'val_loss': 0.5595530319213867, 'test_acc': 0.69}
{'fold': 9, 'epoch': 56, 'train_loss': 0.42202973246574405, 'val_loss': 0.5168640518188476, 'test_acc': 0.74}
{'fold': 9, 'epoch': 57, 'train_loss': 0.38390573382377624, 'val_loss': 0.9665753173828125, 'test_acc': 0.76}
{'fold': 9, 'epoch': 58, 'train_loss': 0.35712247490882876, 'val_loss': 0.6419335174560546, 'test_acc': 0.7}
{'fold': 9, 'epoch': 59, 'train_loss': 0.35293042182922363, 'val_loss': 1.0300661468505858, 'test_acc': 0.72}
{'fold': 9, 'epoch': 60, 'train_loss': 0.3704599595069885, 'val_loss': 0.4575560760498047, 'test_acc': 0.74}
{'fold': 9, 'epoch': 61, 'train_loss': 0.3552782654762268, 'val_loss': 0.5705776214599609, 'test_acc': 0.73}
{'fold': 9, 'epoch': 62, 'train_loss': 0.34997007727622986, 'val_loss': 0.7921685028076172, 'test_acc': 0.73}
{'fold': 9, 'epoch': 63, 'train_loss': 0.3353879749774933, 'val_loss': 0.4966647338867187, 'test_acc': 0.71}
{'fold': 9, 'epoch': 64, 'train_loss': 0.3142043077945709, 'val_loss': 0.6807781982421875, 'test_acc': 0.72}
{'fold': 9, 'epoch': 65, 'train_loss': 0.3177968466281891, 'val_loss': 1.3275033569335937, 'test_acc': 0.68}
{'fold': 9, 'epoch': 66, 'train_loss': 0.32433936834335325, 'val_loss': 0.6427749633789063, 'test_acc': 0.72}
{'fold': 9, 'epoch': 67, 'train_loss': 0.3552422142028809, 'val_loss': 0.6813401794433593, 'test_acc': 0.69}
{'fold': 9, 'epoch': 68, 'train_loss': 0.3317965495586395, 'val_loss': 0.6625371551513672, 'test_acc': 0.73}
{'fold': 9, 'epoch': 69, 'train_loss': 0.30005111813545227, 'val_loss': 1.14802490234375, 'test_acc': 0.72}
{'fold': 9, 'epoch': 70, 'train_loss': 0.28984879910945893, 'val_loss': 0.8979637145996093, 'test_acc': 0.75}
{'fold': 9, 'epoch': 71, 'train_loss': 0.2886719417572021, 'val_loss': 0.8139718627929687, 'test_acc': 0.73}
{'fold': 9, 'epoch': 72, 'train_loss': 0.28258844137191774, 'val_loss': 1.2248204040527344, 'test_acc': 0.74}
{'fold': 9, 'epoch': 73, 'train_loss': 0.26552428960800173, 'val_loss': 1.8922366333007812, 'test_acc': 0.74}
{'fold': 9, 'epoch': 74, 'train_loss': 0.26928569078445436, 'val_loss': 1.8332615661621094, 'test_acc': 0.75}
{'fold': 9, 'epoch': 75, 'train_loss': 0.2711713397502899, 'val_loss': 1.7510589599609374, 'test_acc': 0.71}
{'fold': 9, 'epoch': 76, 'train_loss': 0.2886026179790497, 'val_loss': 1.107803726196289, 'test_acc': 0.7}
{'fold': 9, 'epoch': 77, 'train_loss': 0.26903960466384885, 'val_loss': 1.4378631591796875, 'test_acc': 0.73}
{'fold': 9, 'epoch': 78, 'train_loss': 0.35154212832450865, 'val_loss': 0.8527603912353515, 'test_acc': 0.72}
{'fold': 9, 'epoch': 79, 'train_loss': 0.31361752450466157, 'val_loss': 0.9679066467285157, 'test_acc': 0.72}
{'fold': 9, 'epoch': 80, 'train_loss': 0.29408578157424925, 'val_loss': 3.8448443603515625, 'test_acc': 0.71}
{'fold': 9, 'epoch': 81, 'train_loss': 0.2972590470314026, 'val_loss': 2.292966156005859, 'test_acc': 0.73}
{'fold': 9, 'epoch': 82, 'train_loss': 0.2988339841365814, 'val_loss': 2.6168646240234374, 'test_acc': 0.7}
{'fold': 9, 'epoch': 83, 'train_loss': 0.3068981909751892, 'val_loss': 0.8426346588134765, 'test_acc': 0.71}
{'fold': 9, 'epoch': 84, 'train_loss': 0.2913931655883789, 'val_loss': 2.6605682373046875, 'test_acc': 0.73}
{'fold': 9, 'epoch': 85, 'train_loss': 0.2727503097057343, 'val_loss': 2.7216650390625, 'test_acc': 0.72}
{'fold': 9, 'epoch': 86, 'train_loss': 0.300146712064743, 'val_loss': 1.5380827331542968, 'test_acc': 0.72}
{'fold': 9, 'epoch': 87, 'train_loss': 0.2894792318344116, 'val_loss': 1.1884347534179687, 'test_acc': 0.71}
{'fold': 9, 'epoch': 88, 'train_loss': 0.2837786215543747, 'val_loss': 3.00606689453125, 'test_acc': 0.73}
{'fold': 9, 'epoch': 89, 'train_loss': 0.297290164232254, 'val_loss': 2.6196945190429686, 'test_acc': 0.7}
{'fold': 9, 'epoch': 90, 'train_loss': 0.2662660229206085, 'val_loss': 1.31482421875, 'test_acc': 0.74}
{'fold': 9, 'epoch': 91, 'train_loss': 0.2865944629907608, 'val_loss': 1.7681280517578124, 'test_acc': 0.73}
{'fold': 9, 'epoch': 92, 'train_loss': 0.2693334650993347, 'val_loss': 1.8484107971191406, 'test_acc': 0.71}
{'fold': 9, 'epoch': 93, 'train_loss': 0.2743480861186981, 'val_loss': 8.7240478515625, 'test_acc': 0.71}
{'fold': 9, 'epoch': 94, 'train_loss': 0.32447136640548707, 'val_loss': 2.1727774047851565, 'test_acc': 0.71}
{'fold': 9, 'epoch': 95, 'train_loss': 0.33773241996765135, 'val_loss': 2.111421661376953, 'test_acc': 0.7}
{'fold': 9, 'epoch': 96, 'train_loss': 0.28454605996608734, 'val_loss': 5.032823486328125, 'test_acc': 0.72}
{'fold': 9, 'epoch': 97, 'train_loss': 0.37909796595573425, 'val_loss': 0.9349212646484375, 'test_acc': 0.7}
{'fold': 9, 'epoch': 98, 'train_loss': 0.344548465013504, 'val_loss': 0.8071635437011718, 'test_acc': 0.71}
{'fold': 9, 'epoch': 99, 'train_loss': 0.3975224494934082, 'val_loss': 1.7601785278320312, 'test_acc': 0.72}
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
{'fold': 9, 'epoch': 100, 'train_loss': 0.8264873886108398, 'val_loss': 0.6297636413574219, 'test_acc': 0.63}
Val Loss: 0.4816, Test Accuracy: 0.729 ± 0.049, Duration: 7.910
Best result - 0.729 ± 0.049
--
IMDB-BINARY - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6741019892692566, 'val_loss': 0.48648643493652344, 'test_acc': 0.67}
{'fold': 9, 'epoch': 2, 'train_loss': 0.5721665835380554, 'val_loss': 0.513713493347168, 'test_acc': 0.69}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5118021476268768, 'val_loss': 0.5072010040283204, 'test_acc': 0.67}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5020504879951477, 'val_loss': 0.4568759536743164, 'test_acc': 0.62}
{'fold': 9, 'epoch': 5, 'train_loss': 0.4819009590148926, 'val_loss': 0.4481101226806641, 'test_acc': 0.64}
{'fold': 9, 'epoch': 6, 'train_loss': 0.46898369073867796, 'val_loss': 0.4443697738647461, 'test_acc': 0.66}
{'fold': 9, 'epoch': 7, 'train_loss': 0.456206237077713, 'val_loss': 0.4461445617675781, 'test_acc': 0.64}
{'fold': 9, 'epoch': 8, 'train_loss': 0.4489220702648163, 'val_loss': 0.46147445678710936, 'test_acc': 0.67}
{'fold': 9, 'epoch': 9, 'train_loss': 0.43936949491500854, 'val_loss': 0.49052833557128905, 'test_acc': 0.67}
{'fold': 9, 'epoch': 10, 'train_loss': 0.4175292062759399, 'val_loss': 0.5390533447265625, 'test_acc': 0.69}
{'fold': 9, 'epoch': 11, 'train_loss': 0.4560231912136078, 'val_loss': 0.45510498046875, 'test_acc': 0.67}
{'fold': 9, 'epoch': 12, 'train_loss': 0.42529464602470396, 'val_loss': 0.4223777770996094, 'test_acc': 0.68}
{'fold': 9, 'epoch': 13, 'train_loss': 0.4203156650066376, 'val_loss': 0.4347577285766602, 'test_acc': 0.68}
{'fold': 9, 'epoch': 14, 'train_loss': 0.3942240071296692, 'val_loss': 0.5565781784057617, 'test_acc': 0.66}
{'fold': 9, 'epoch': 15, 'train_loss': 0.39603561520576475, 'val_loss': 0.5696574783325196, 'test_acc': 0.67}
{'fold': 9, 'epoch': 16, 'train_loss': 0.38910905957221986, 'val_loss': 0.6384695816040039, 'test_acc': 0.74}
{'fold': 9, 'epoch': 17, 'train_loss': 0.3703657495975494, 'val_loss': 0.5096529006958008, 'test_acc': 0.71}
{'fold': 9, 'epoch': 18, 'train_loss': 0.3536213171482086, 'val_loss': 0.5094822692871094, 'test_acc': 0.7}
{'fold': 9, 'epoch': 19, 'train_loss': 0.35413397192955015, 'val_loss': 0.5440868759155273, 'test_acc': 0.64}
{'fold': 9, 'epoch': 20, 'train_loss': 0.34538041472435, 'val_loss': 0.6000787734985351, 'test_acc': 0.65}
{'fold': 9, 'epoch': 21, 'train_loss': 0.3734939694404602, 'val_loss': 0.45617420196533204, 'test_acc': 0.69}
{'fold': 9, 'epoch': 22, 'train_loss': 0.3426778417825699, 'val_loss': 0.49008731842041015, 'test_acc': 0.7}
{'fold': 9, 'epoch': 23, 'train_loss': 0.35835752010345456, 'val_loss': 0.4755459213256836, 'test_acc': 0.64}
{'fold': 9, 'epoch': 24, 'train_loss': 0.3554870593547821, 'val_loss': 0.6444290924072266, 'test_acc': 0.64}
{'fold': 9, 'epoch': 25, 'train_loss': 0.35651365041732785, 'val_loss': 0.4474129104614258, 'test_acc': 0.68}
{'fold': 9, 'epoch': 26, 'train_loss': 0.3197518491744995, 'val_loss': 0.4770986557006836, 'test_acc': 0.68}
{'fold': 9, 'epoch': 27, 'train_loss': 0.31606019496917725, 'val_loss': 0.5899000549316407, 'test_acc': 0.67}
{'fold': 9, 'epoch': 28, 'train_loss': 0.3085573279857636, 'val_loss': 0.8873601531982422, 'test_acc': 0.68}
{'fold': 9, 'epoch': 29, 'train_loss': 0.3434015870094299, 'val_loss': 0.5900905990600586, 'test_acc': 0.66}
{'fold': 9, 'epoch': 30, 'train_loss': 0.3395350337028503, 'val_loss': 0.4574596405029297, 'test_acc': 0.72}
{'fold': 9, 'epoch': 31, 'train_loss': 0.3303932958841324, 'val_loss': 0.5187609481811524, 'test_acc': 0.65}
{'fold': 9, 'epoch': 32, 'train_loss': 0.314871261715889, 'val_loss': 0.6427721405029296, 'test_acc': 0.65}
{'fold': 9, 'epoch': 33, 'train_loss': 0.31368093848228457, 'val_loss': 0.5799535751342774, 'test_acc': 0.63}
{'fold': 9, 'epoch': 34, 'train_loss': 0.2984717786312103, 'val_loss': 0.8710649108886719, 'test_acc': 0.65}
{'fold': 9, 'epoch': 35, 'train_loss': 0.29271533846855163, 'val_loss': 0.9070207214355469, 'test_acc': 0.63}
{'fold': 9, 'epoch': 36, 'train_loss': 0.29855712890625, 'val_loss': 1.1580347442626953, 'test_acc': 0.7}
{'fold': 9, 'epoch': 37, 'train_loss': 0.2906628143787384, 'val_loss': 0.7292415618896484, 'test_acc': 0.69}
{'fold': 9, 'epoch': 38, 'train_loss': 0.2783375859260559, 'val_loss': 1.4482321166992187, 'test_acc': 0.63}
{'fold': 9, 'epoch': 39, 'train_loss': 0.27645071625709533, 'val_loss': 0.896121826171875, 'test_acc': 0.73}
{'fold': 9, 'epoch': 40, 'train_loss': 0.2616329741477966, 'val_loss': 1.7167030334472657, 'test_acc': 0.66}
{'fold': 9, 'epoch': 41, 'train_loss': 0.25996469616889956, 'val_loss': 1.8065625, 'test_acc': 0.68}
{'fold': 9, 'epoch': 42, 'train_loss': 0.273429172039032, 'val_loss': 1.0924858856201172, 'test_acc': 0.71}
{'fold': 9, 'epoch': 43, 'train_loss': 0.30792162179946897, 'val_loss': 1.04880615234375, 'test_acc': 0.63}
{'fold': 9, 'epoch': 44, 'train_loss': 0.2894266355037689, 'val_loss': 1.0070784759521485, 'test_acc': 0.66}
{'fold': 9, 'epoch': 45, 'train_loss': 0.27832593142986295, 'val_loss': 1.372992706298828, 'test_acc': 0.68}
{'fold': 9, 'epoch': 46, 'train_loss': 0.2753578382730484, 'val_loss': 1.2039854431152344, 'test_acc': 0.69}
{'fold': 9, 'epoch': 47, 'train_loss': 0.6352962958812713, 'val_loss': 1.4822366333007813, 'test_acc': 0.66}
{'fold': 9, 'epoch': 48, 'train_loss': 0.5582789254188537, 'val_loss': 0.632189826965332, 'test_acc': 0.63}
{'fold': 9, 'epoch': 49, 'train_loss': 0.39635695397853854, 'val_loss': 0.7798868560791016, 'test_acc': 0.73}
{'fold': 9, 'epoch': 50, 'train_loss': 0.34669116139411926, 'val_loss': 0.7093465423583984, 'test_acc': 0.73}
{'fold': 9, 'epoch': 51, 'train_loss': 0.32015144944190976, 'val_loss': 0.7181804656982422, 'test_acc': 0.67}
{'fold': 9, 'epoch': 52, 'train_loss': 0.2923523712158203, 'val_loss': 1.4534934997558593, 'test_acc': 0.65}
{'fold': 9, 'epoch': 53, 'train_loss': 0.2895776700973511, 'val_loss': 1.7317242431640625, 'test_acc': 0.75}
{'fold': 9, 'epoch': 54, 'train_loss': 0.29642641186714175, 'val_loss': 1.231595458984375, 'test_acc': 0.72}
{'fold': 9, 'epoch': 55, 'train_loss': 0.2760725712776184, 'val_loss': 1.0438185119628907, 'test_acc': 0.72}
{'fold': 9, 'epoch': 56, 'train_loss': 0.2640482658147812, 'val_loss': 1.1551351928710938, 'test_acc': 0.73}
{'fold': 9, 'epoch': 57, 'train_loss': 0.2787790024280548, 'val_loss': 1.3676345825195313, 'test_acc': 0.68}
{'fold': 9, 'epoch': 58, 'train_loss': 0.254765065908432, 'val_loss': 1.0886949157714845, 'test_acc': 0.73}
{'fold': 9, 'epoch': 59, 'train_loss': 0.25622491657733915, 'val_loss': 1.5715972900390625, 'test_acc': 0.67}
{'fold': 9, 'epoch': 60, 'train_loss': 0.24967644095420838, 'val_loss': 2.5678948974609375, 'test_acc': 0.7}
{'fold': 9, 'epoch': 61, 'train_loss': 0.254452189207077, 'val_loss': 2.039392547607422, 'test_acc': 0.69}
{'fold': 9, 'epoch': 62, 'train_loss': 0.2441209626197815, 'val_loss': 1.6492912292480468, 'test_acc': 0.72}
{'fold': 9, 'epoch': 63, 'train_loss': 0.2639833664894104, 'val_loss': 2.289350891113281, 'test_acc': 0.71}
{'fold': 9, 'epoch': 64, 'train_loss': 0.27240645289421084, 'val_loss': 1.5366098022460937, 'test_acc': 0.73}
{'fold': 9, 'epoch': 65, 'train_loss': 0.28308655381202696, 'val_loss': 1.2257199859619141, 'test_acc': 0.64}
{'fold': 9, 'epoch': 66, 'train_loss': 0.2736907196044922, 'val_loss': 1.460064697265625, 'test_acc': 0.75}
{'fold': 9, 'epoch': 67, 'train_loss': 0.26069414377212524, 'val_loss': 1.9933956909179686, 'test_acc': 0.69}
{'fold': 9, 'epoch': 68, 'train_loss': 0.2427891170978546, 'val_loss': 2.3943809509277343, 'test_acc': 0.73}
{'fold': 9, 'epoch': 69, 'train_loss': 0.23512372016906738, 'val_loss': 3.156733093261719, 'test_acc': 0.71}
{'fold': 9, 'epoch': 70, 'train_loss': 0.3564075815677643, 'val_loss': 2.2100970458984377, 'test_acc': 0.7}
{'fold': 9, 'epoch': 71, 'train_loss': 0.2911145746707916, 'val_loss': 1.3056793212890625, 'test_acc': 0.69}
{'fold': 9, 'epoch': 72, 'train_loss': 0.2520581293106079, 'val_loss': 1.5598320007324218, 'test_acc': 0.69}
{'fold': 9, 'epoch': 73, 'train_loss': 0.23670106291770934, 'val_loss': 1.7436129760742187, 'test_acc': 0.73}
{'fold': 9, 'epoch': 74, 'train_loss': 0.2399003779888153, 'val_loss': 2.0495030212402345, 'test_acc': 0.74}
{'fold': 9, 'epoch': 75, 'train_loss': 0.23410162448883057, 'val_loss': 4.141531066894531, 'test_acc': 0.71}
{'fold': 9, 'epoch': 76, 'train_loss': 0.23467613875865936, 'val_loss': 5.299578857421875, 'test_acc': 0.73}
{'fold': 9, 'epoch': 77, 'train_loss': 0.279950749874115, 'val_loss': 2.878431701660156, 'test_acc': 0.74}
{'fold': 9, 'epoch': 78, 'train_loss': 0.2721714496612549, 'val_loss': 1.122809829711914, 'test_acc': 0.7}
{'fold': 9, 'epoch': 79, 'train_loss': 0.2918715953826904, 'val_loss': 0.7398772430419922, 'test_acc': 0.71}
{'fold': 9, 'epoch': 80, 'train_loss': 0.28946560978889463, 'val_loss': 0.8176010131835938, 'test_acc': 0.7}
{'fold': 9, 'epoch': 81, 'train_loss': 0.25261607885360715, 'val_loss': 0.9233016967773438, 'test_acc': 0.7}
{'fold': 9, 'epoch': 82, 'train_loss': 0.2384133219718933, 'val_loss': 1.0407122802734374, 'test_acc': 0.7}
{'fold': 9, 'epoch': 83, 'train_loss': 0.2433630758523941, 'val_loss': 1.1134342193603515, 'test_acc': 0.69}
{'fold': 9, 'epoch': 84, 'train_loss': 0.2375548678636551, 'val_loss': 1.1298277282714844, 'test_acc': 0.66}
{'fold': 9, 'epoch': 85, 'train_loss': 0.24335299611091613, 'val_loss': 1.3812942504882812, 'test_acc': 0.69}
{'fold': 9, 'epoch': 86, 'train_loss': 0.23112335324287414, 'val_loss': 1.4547625732421876, 'test_acc': 0.68}
{'fold': 9, 'epoch': 87, 'train_loss': 0.2447729241847992, 'val_loss': 1.172462158203125, 'test_acc': 0.68}
{'fold': 9, 'epoch': 88, 'train_loss': 0.22657657742500306, 'val_loss': 1.2308952331542968, 'test_acc': 0.68}
{'fold': 9, 'epoch': 89, 'train_loss': 0.2384237217903137, 'val_loss': 1.2729515838623047, 'test_acc': 0.75}
{'fold': 9, 'epoch': 90, 'train_loss': 0.2527363157272339, 'val_loss': 1.4118612670898438, 'test_acc': 0.71}
{'fold': 9, 'epoch': 91, 'train_loss': 0.2844418901205063, 'val_loss': 1.2697735595703126, 'test_acc': 0.74}
{'fold': 9, 'epoch': 92, 'train_loss': 0.24249462842941283, 'val_loss': 1.2890280151367188, 'test_acc': 0.73}
{'fold': 9, 'epoch': 93, 'train_loss': 0.23150672674179076, 'val_loss': 1.2090709686279297, 'test_acc': 0.75}
{'fold': 9, 'epoch': 94, 'train_loss': 0.22514135241508484, 'val_loss': 1.5235780334472657, 'test_acc': 0.75}
{'fold': 9, 'epoch': 95, 'train_loss': 0.2225557744503021, 'val_loss': 2.1508212280273438, 'test_acc': 0.74}
{'fold': 9, 'epoch': 96, 'train_loss': 0.22989091634750367, 'val_loss': 1.651493377685547, 'test_acc': 0.74}
{'fold': 9, 'epoch': 97, 'train_loss': 0.23062728404998778, 'val_loss': 1.439461669921875, 'test_acc': 0.71}
{'fold': 9, 'epoch': 98, 'train_loss': 0.22319608092308044, 'val_loss': 2.0429977416992187, 'test_acc': 0.73}
{'fold': 9, 'epoch': 99, 'train_loss': 0.24288348555564881, 'val_loss': 1.4290040588378907, 'test_acc': 0.71}
{'fold': 9, 'epoch': 100, 'train_loss': 0.24741659581661224, 'val_loss': 1.283065185546875, 'test_acc': 0.68}
Val Loss: 0.4855, Test Accuracy: 0.723 ± 0.045, Duration: 54.500
Best result - 0.723 ± 0.045
--
REDDIT-BINARY - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 1.7726496291160583, 'val_loss': 0.6405318069458008, 'test_acc': 0.69}
{'fold': 9, 'epoch': 2, 'train_loss': 0.5925919198989869, 'val_loss': 0.577636833190918, 'test_acc': 0.67}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5549300527572631, 'val_loss': 0.5550949096679687, 'test_acc': 0.675}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5458435583114624, 'val_loss': 0.5728238582611084, 'test_acc': 0.67}
{'fold': 9, 'epoch': 5, 'train_loss': 0.553780243396759, 'val_loss': 0.5809090900421142, 'test_acc': 0.67}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5387642908096314, 'val_loss': 0.5395007419586182, 'test_acc': 0.67}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5266620600223542, 'val_loss': 0.5219541549682617, 'test_acc': 0.73}
{'fold': 9, 'epoch': 8, 'train_loss': 0.49657147884368896, 'val_loss': 0.5252655982971192, 'test_acc': 0.69}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5012964224815368, 'val_loss': 0.5240103912353515, 'test_acc': 0.695}
{'fold': 9, 'epoch': 10, 'train_loss': 0.4988968479633331, 'val_loss': 0.5027952480316162, 'test_acc': 0.72}
{'fold': 9, 'epoch': 11, 'train_loss': 0.4881306171417236, 'val_loss': 0.5196145820617676, 'test_acc': 0.685}
{'fold': 9, 'epoch': 12, 'train_loss': 0.46504481315612795, 'val_loss': 0.523064775466919, 'test_acc': 0.675}
{'fold': 9, 'epoch': 13, 'train_loss': 0.48231693744659426, 'val_loss': 0.5022325992584229, 'test_acc': 0.685}
{'fold': 9, 'epoch': 14, 'train_loss': 0.4671273994445801, 'val_loss': 0.4503949165344238, 'test_acc': 0.75}
{'fold': 9, 'epoch': 15, 'train_loss': 0.4545056450366974, 'val_loss': 0.5116354560852051, 'test_acc': 0.7}
{'fold': 9, 'epoch': 16, 'train_loss': 0.4482870590686798, 'val_loss': 0.4492430019378662, 'test_acc': 0.745}
{'fold': 9, 'epoch': 17, 'train_loss': 0.48524134159088134, 'val_loss': 0.4925723075866699, 'test_acc': 0.695}
{'fold': 9, 'epoch': 18, 'train_loss': 0.47595402479171756, 'val_loss': 0.5260412120819091, 'test_acc': 0.705}
{'fold': 9, 'epoch': 19, 'train_loss': 0.49153603315353395, 'val_loss': 0.523352518081665, 'test_acc': 0.685}
{'fold': 9, 'epoch': 20, 'train_loss': 0.426797411441803, 'val_loss': 0.45942317008972167, 'test_acc': 0.74}
{'fold': 9, 'epoch': 21, 'train_loss': 0.4276820969581604, 'val_loss': 0.5049662685394287, 'test_acc': 0.71}
{'fold': 9, 'epoch': 22, 'train_loss': 0.4278547465801239, 'val_loss': 0.45588147163391113, 'test_acc': 0.735}
{'fold': 9, 'epoch': 23, 'train_loss': 0.4010960853099823, 'val_loss': 0.46434233665466307, 'test_acc': 0.71}
{'fold': 9, 'epoch': 24, 'train_loss': 0.3896015763282776, 'val_loss': 0.5058939647674561, 'test_acc': 0.68}
{'fold': 9, 'epoch': 25, 'train_loss': 0.36555482268333434, 'val_loss': 0.505974817276001, 'test_acc': 0.71}
{'fold': 9, 'epoch': 26, 'train_loss': 0.396963427066803, 'val_loss': 0.5744546127319335, 'test_acc': 0.63}
{'fold': 9, 'epoch': 27, 'train_loss': 0.39496795415878294, 'val_loss': 0.5114937686920166, 'test_acc': 0.705}
{'fold': 9, 'epoch': 28, 'train_loss': 0.342445011138916, 'val_loss': 0.5410464096069336, 'test_acc': 0.72}
{'fold': 9, 'epoch': 29, 'train_loss': 0.3433312726020813, 'val_loss': 0.5452098655700683, 'test_acc': 0.74}
{'fold': 9, 'epoch': 30, 'train_loss': 0.34736318826675416, 'val_loss': 0.4697544574737549, 'test_acc': 0.715}
{'fold': 9, 'epoch': 31, 'train_loss': 0.3103679549694061, 'val_loss': 0.5688298130035401, 'test_acc': 0.695}
{'fold': 9, 'epoch': 32, 'train_loss': 0.2879882049560547, 'val_loss': 0.5146666145324708, 'test_acc': 0.7}
{'fold': 9, 'epoch': 33, 'train_loss': 0.27897893071174623, 'val_loss': 0.5838301277160645, 'test_acc': 0.69}
{'fold': 9, 'epoch': 34, 'train_loss': 0.2723285186290741, 'val_loss': 0.5598082637786865, 'test_acc': 0.7}
{'fold': 9, 'epoch': 35, 'train_loss': 0.32450523018836974, 'val_loss': 0.5386130809783936, 'test_acc': 0.71}
{'fold': 9, 'epoch': 36, 'train_loss': 0.31606590390205386, 'val_loss': 0.5174708652496338, 'test_acc': 0.705}
{'fold': 9, 'epoch': 37, 'train_loss': 0.28046178102493285, 'val_loss': 0.47754945755004885, 'test_acc': 0.73}
{'fold': 9, 'epoch': 38, 'train_loss': 0.27469062447547915, 'val_loss': 0.5811857032775879, 'test_acc': 0.725}
{'fold': 9, 'epoch': 39, 'train_loss': 0.2633639335632324, 'val_loss': 0.6924811553955078, 'test_acc': 0.725}
{'fold': 9, 'epoch': 40, 'train_loss': 0.2443739104270935, 'val_loss': 0.6097706604003906, 'test_acc': 0.725}
{'fold': 9, 'epoch': 41, 'train_loss': 0.22430619716644287, 'val_loss': 0.6231548500061035, 'test_acc': 0.72}
{'fold': 9, 'epoch': 42, 'train_loss': 0.23829807817935944, 'val_loss': 0.6416079521179199, 'test_acc': 0.745}
{'fold': 9, 'epoch': 43, 'train_loss': 0.23266379594802855, 'val_loss': 0.6355525398254395, 'test_acc': 0.71}
{'fold': 9, 'epoch': 44, 'train_loss': 0.24269511938095092, 'val_loss': 0.5839398193359375, 'test_acc': 0.72}
{'fold': 9, 'epoch': 45, 'train_loss': 0.19700795650482178, 'val_loss': 0.6408414649963379, 'test_acc': 0.68}
{'fold': 9, 'epoch': 46, 'train_loss': 0.21000814735889434, 'val_loss': 0.5517917442321777, 'test_acc': 0.69}
{'fold': 9, 'epoch': 47, 'train_loss': 0.2744567227363586, 'val_loss': 0.5438185119628907, 'test_acc': 0.69}
{'fold': 9, 'epoch': 48, 'train_loss': 0.19493611693382262, 'val_loss': 0.6696989631652832, 'test_acc': 0.7}
{'fold': 9, 'epoch': 49, 'train_loss': 0.17473569095134736, 'val_loss': 0.7535653686523438, 'test_acc': 0.735}
{'fold': 9, 'epoch': 50, 'train_loss': 0.1671658045053482, 'val_loss': 0.94441743850708, 'test_acc': 0.715}
{'fold': 9, 'epoch': 51, 'train_loss': 0.15533668100833892, 'val_loss': 0.7342142677307129, 'test_acc': 0.7}
{'fold': 9, 'epoch': 52, 'train_loss': 0.14544779688119888, 'val_loss': 0.9189622879028321, 'test_acc': 0.73}
{'fold': 9, 'epoch': 53, 'train_loss': 0.19720679104328157, 'val_loss': 0.7708728981018066, 'test_acc': 0.735}
{'fold': 9, 'epoch': 54, 'train_loss': 0.2067198121547699, 'val_loss': 0.6155255317687989, 'test_acc': 0.685}
{'fold': 9, 'epoch': 55, 'train_loss': 0.22236859083175659, 'val_loss': 0.651279354095459, 'test_acc': 0.745}
{'fold': 9, 'epoch': 56, 'train_loss': 0.17035821139812468, 'val_loss': 0.7417917823791504, 'test_acc': 0.725}
{'fold': 9, 'epoch': 57, 'train_loss': 0.24227001309394836, 'val_loss': 0.5486400985717773, 'test_acc': 0.725}
{'fold': 9, 'epoch': 58, 'train_loss': 0.1535007405281067, 'val_loss': 0.618098258972168, 'test_acc': 0.725}
{'fold': 9, 'epoch': 59, 'train_loss': 0.1439259159564972, 'val_loss': 0.6868244552612305, 'test_acc': 0.735}
{'fold': 9, 'epoch': 60, 'train_loss': 0.20495130807161333, 'val_loss': 0.4611577606201172, 'test_acc': 0.735}
{'fold': 9, 'epoch': 61, 'train_loss': 0.1919906395673752, 'val_loss': 0.5420542526245117, 'test_acc': 0.72}
{'fold': 9, 'epoch': 62, 'train_loss': 0.1279129120707512, 'val_loss': 0.9422442626953125, 'test_acc': 0.71}
{'fold': 9, 'epoch': 63, 'train_loss': 0.12791187942028046, 'val_loss': 0.7662103271484375, 'test_acc': 0.73}
{'fold': 9, 'epoch': 64, 'train_loss': 0.15819664895534516, 'val_loss': 0.9162467956542969, 'test_acc': 0.71}
{'fold': 9, 'epoch': 65, 'train_loss': 0.14793295443058013, 'val_loss': 0.7814726638793945, 'test_acc': 0.695}
{'fold': 9, 'epoch': 66, 'train_loss': 0.1466110095381737, 'val_loss': 0.9670536422729492, 'test_acc': 0.715}
{'fold': 9, 'epoch': 67, 'train_loss': 0.12804688841104508, 'val_loss': 0.8440843963623047, 'test_acc': 0.705}
{'fold': 9, 'epoch': 68, 'train_loss': 0.29851153314113615, 'val_loss': 0.5124026679992676, 'test_acc': 0.69}
{'fold': 9, 'epoch': 69, 'train_loss': 0.23797913372516633, 'val_loss': 0.49993663787841797, 'test_acc': 0.73}
{'fold': 9, 'epoch': 70, 'train_loss': 0.1592431178689003, 'val_loss': 0.8112834358215332, 'test_acc': 0.735}
{'fold': 9, 'epoch': 71, 'train_loss': 0.12084513783454895, 'val_loss': 0.8500080871582031, 'test_acc': 0.745}
{'fold': 9, 'epoch': 72, 'train_loss': 0.10122421830892563, 'val_loss': 0.9371030807495118, 'test_acc': 0.715}
{'fold': 9, 'epoch': 73, 'train_loss': 0.10425191134214401, 'val_loss': 0.8836851119995117, 'test_acc': 0.7}
{'fold': 9, 'epoch': 74, 'train_loss': 0.11235352456569672, 'val_loss': 0.8087151527404786, 'test_acc': 0.71}
{'fold': 9, 'epoch': 75, 'train_loss': 0.11227493286132813, 'val_loss': 0.8164066696166992, 'test_acc': 0.71}
{'fold': 9, 'epoch': 76, 'train_loss': 0.10548830181360244, 'val_loss': 1.0392443466186523, 'test_acc': 0.715}
{'fold': 9, 'epoch': 77, 'train_loss': 0.15392169147729873, 'val_loss': 0.8198087310791016, 'test_acc': 0.74}
{'fold': 9, 'epoch': 78, 'train_loss': 0.12410275131464005, 'val_loss': 0.9870923614501953, 'test_acc': 0.725}
{'fold': 9, 'epoch': 79, 'train_loss': 0.1097450977563858, 'val_loss': 0.9174242401123047, 'test_acc': 0.725}
{'fold': 9, 'epoch': 80, 'train_loss': 0.20823494732379913, 'val_loss': 0.5314978313446045, 'test_acc': 0.75}
{'fold': 9, 'epoch': 81, 'train_loss': 0.10524997949600219, 'val_loss': 0.7647121047973633, 'test_acc': 0.76}
{'fold': 9, 'epoch': 82, 'train_loss': 0.27443634808063505, 'val_loss': 0.6090136528015136, 'test_acc': 0.715}
{'fold': 9, 'epoch': 83, 'train_loss': 0.19606530845165251, 'val_loss': 0.6103201675415039, 'test_acc': 0.73}
{'fold': 9, 'epoch': 84, 'train_loss': 0.2602117729187012, 'val_loss': 0.6234444046020508, 'test_acc': 0.715}
{'fold': 9, 'epoch': 85, 'train_loss': 0.203635618686676, 'val_loss': 0.7136217308044434, 'test_acc': 0.735}
{'fold': 9, 'epoch': 86, 'train_loss': 0.19707131624221802, 'val_loss': 0.7442687606811523, 'test_acc': 0.72}
{'fold': 9, 'epoch': 87, 'train_loss': 0.16815315425395966, 'val_loss': 0.6034220123291015, 'test_acc': 0.73}
{'fold': 9, 'epoch': 88, 'train_loss': 0.13206521958112716, 'val_loss': 0.7540567779541015, 'test_acc': 0.715}
{'fold': 9, 'epoch': 89, 'train_loss': 0.11317914247512817, 'val_loss': 0.7050122642517089, 'test_acc': 0.72}
{'fold': 9, 'epoch': 90, 'train_loss': 0.10717512309551239, 'val_loss': 0.8195684814453125, 'test_acc': 0.73}
{'fold': 9, 'epoch': 91, 'train_loss': 0.094971182346344, 'val_loss': 0.7890349006652833, 'test_acc': 0.72}
{'fold': 9, 'epoch': 92, 'train_loss': 0.2322611367702484, 'val_loss': 0.6121534156799316, 'test_acc': 0.725}
{'fold': 9, 'epoch': 93, 'train_loss': 0.13986836791038512, 'val_loss': 0.6006058883666993, 'test_acc': 0.735}
{'fold': 9, 'epoch': 94, 'train_loss': 0.10998458951711655, 'val_loss': 0.9479592704772949, 'test_acc': 0.73}
{'fold': 9, 'epoch': 95, 'train_loss': 0.15371079325675965, 'val_loss': 0.6886812400817871, 'test_acc': 0.745}
{'fold': 9, 'epoch': 96, 'train_loss': 0.09529301121830941, 'val_loss': 0.6855628967285157, 'test_acc': 0.73}
{'fold': 9, 'epoch': 97, 'train_loss': 0.10133137613534927, 'val_loss': 0.8108326911926269, 'test_acc': 0.73}
{'fold': 9, 'epoch': 98, 'train_loss': 0.07314215630292892, 'val_loss': 0.7876714706420899, 'test_acc': 0.745}
{'fold': 9, 'epoch': 99, 'train_loss': 0.07981971144676209, 'val_loss': 0.8803680801391601, 'test_acc': 0.74}
{'fold': 9, 'epoch': 100, 'train_loss': 0.1171815711259842, 'val_loss': 0.8294107055664063, 'test_acc': 0.755}
Val Loss: 0.4241, Test Accuracy: 0.786 ± 0.062, Duration: 28.849
Best result - 0.786 ± 0.062
--
REDDIT-BINARY - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Traceback (most recent call last):
  File "F:/Project/BernNet/GraphClassification/main.py", line 104, in <module>
    loss, acc, std = cross_validation_with_val_set(
  File "F:\Project\BernNet\GraphClassification\train_eval.py", line 46, in cross_validation_with_val_set
    train_loss = train(model, optimizer, train_loader)
  File "F:\Project\BernNet\GraphClassification\train_eval.py", line 125, in train
    out = model(data)
  File "D:\Program\Anaconda\envs\pyg\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "F:\Project\BernNet\GraphClassification\model\asap.py", line 51, in forward
    x, edge_index, edge_weight, batch, _ = pool(
  File "D:\Program\Anaconda\envs\pyg\lib\site-packages\torch\nn\modules\module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\nn\pool\asap.py", line 117, in forward
    x_q = self.lin(x_q)[edge_index[1]]
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

Process finished with exit code 1
D:\Program\Anaconda\envs\pyg\python.exe F:/Project/BernNet/GraphClassification/main.py
--
DD - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.742287866139816, 'val_loss': 0.6854720808501936, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6909203478845499, 'val_loss': 0.684963845799112, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 3, 'train_loss': 0.68279725111137, 'val_loss': 0.6797042585845686, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 4, 'train_loss': 0.679850795511472, 'val_loss': 0.6770025400015024, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6770094562385042, 'val_loss': 0.6769735874273838, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 6, 'train_loss': 0.678880526857861, 'val_loss': 0.676957285302317, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 7, 'train_loss': 0.6786389421608489, 'val_loss': 0.6769610021868323, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 8, 'train_loss': 0.6779403939085492, 'val_loss': 0.6769532423753005, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 9, 'train_loss': 0.6790637252694469, 'val_loss': 0.6769525902902979, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 10, 'train_loss': 0.6783106195724616, 'val_loss': 0.6769699357513689, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 11, 'train_loss': 0.6787641402018272, 'val_loss': 0.6769525902902979, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 12, 'train_loss': 0.6782636884915627, 'val_loss': 0.6770546415932158, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 13, 'train_loss': 0.6796656210543746, 'val_loss': 0.6771027654664129, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 14, 'train_loss': 0.6784625538324905, 'val_loss': 0.6775306636451656, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 15, 'train_loss': 0.6782194759886143, 'val_loss': 0.6773551875709468, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 16, 'train_loss': 0.6791005508374359, 'val_loss': 0.6772667648445847, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 17, 'train_loss': 0.6782813749070895, 'val_loss': 0.6770333836221287, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 18, 'train_loss': 0.6787915765228918, 'val_loss': 0.6769567636343149, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 19, 'train_loss': 0.6790491292032145, 'val_loss': 0.6769521338307959, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 20, 'train_loss': 0.6784916540323678, 'val_loss': 0.676968501164363, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 21, 'train_loss': 0.6785525760408175, 'val_loss': 0.6769959539429754, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 22, 'train_loss': 0.6783021072209892, 'val_loss': 0.6770788991553152, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 23, 'train_loss': 0.6782826862092746, 'val_loss': 0.6771292401175214, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 24, 'train_loss': 0.679221669496116, 'val_loss': 0.6772810455061432, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 25, 'train_loss': 0.6790611147880554, 'val_loss': 0.6774017464401375, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 26, 'train_loss': 0.6786204849259329, 'val_loss': 0.6770931146083734, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 27, 'train_loss': 0.6786460886567326, 'val_loss': 0.677109873192942, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 28, 'train_loss': 0.6781911152904316, 'val_loss': 0.6771353697165464, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 29, 'train_loss': 0.6788298548278162, 'val_loss': 0.6772029257228231, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 30, 'train_loss': 0.6782758569313307, 'val_loss': 0.6770653357872596, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 31, 'train_loss': 0.6785777231394234, 'val_loss': 0.6769603501018296, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 32, 'train_loss': 0.6779508055266688, 'val_loss': 0.6769583938468215, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 33, 'train_loss': 0.6789204761133356, 'val_loss': 0.6769706530448718, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 34, 'train_loss': 0.6782897741107617, 'val_loss': 0.6770297971546141, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 35, 'train_loss': 0.6787648898060039, 'val_loss': 0.6771448901575855, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 36, 'train_loss': 0.6785642949201293, 'val_loss': 0.6771180242554754, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 37, 'train_loss': 0.6783883894904185, 'val_loss': 0.6770825508313302, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 38, 'train_loss': 0.6785636766482208, 'val_loss': 0.6771590404021435, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 39, 'train_loss': 0.6786002334901842, 'val_loss': 0.6770091260600294, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 40, 'train_loss': 0.6786946199708066, 'val_loss': 0.6769518729967948, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 41, 'train_loss': 0.6785597083932262, 'val_loss': 0.6770635099492521, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 42, 'train_loss': 0.6786144294981229, 'val_loss': 0.6770840506268363, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 43, 'train_loss': 0.6785532771530798, 'val_loss': 0.676961654271835, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 44, 'train_loss': 0.6784326141163454, 'val_loss': 0.6770462296966814, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 45, 'train_loss': 0.678591969659773, 'val_loss': 0.6770404261401576, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 46, 'train_loss': 0.6781318258430998, 'val_loss': 0.6770772689428085, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 47, 'train_loss': 0.6784693821001861, 'val_loss': 0.6771732558552016, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 48, 'train_loss': 0.6783670682018086, 'val_loss': 0.6770901150173612, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 49, 'train_loss': 0.6785838250386513, 'val_loss': 0.6769797170264089, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 50, 'train_loss': 0.6783127471552057, 'val_loss': 0.6770030616695045, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 51, 'train_loss': 0.6783444244982832, 'val_loss': 0.6770604451497396, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 52, 'train_loss': 0.6783540642867654, 'val_loss': 0.6769883897569444, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 53, 'train_loss': 0.6789395566714012, 'val_loss': 0.6769544161283053, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 54, 'train_loss': 0.6786426972534697, 'val_loss': 0.6769520686222956, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 55, 'train_loss': 0.6783853254075778, 'val_loss': 0.677041273850661, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 56, 'train_loss': 0.6784350296198312, 'val_loss': 0.6770177987905649, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 57, 'train_loss': 0.6782315474445537, 'val_loss': 0.6769655015733507, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 58, 'train_loss': 0.6787032808287669, 'val_loss': 0.6769621759398371, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 59, 'train_loss': 0.678325821787624, 'val_loss': 0.6769644582373464, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 60, 'train_loss': 0.6792136683302411, 'val_loss': 0.676971435546875, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 61, 'train_loss': 0.6784098916134592, 'val_loss': 0.676951677371294, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 62, 'train_loss': 0.6783460671618834, 'val_loss': 0.6769622411483374, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 63, 'train_loss': 0.6787578120069989, 'val_loss': 0.6769526554987981, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 64, 'train_loss': 0.6785523820731599, 'val_loss': 0.6769732613848825, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 65, 'train_loss': 0.6783267572774725, 'val_loss': 0.6770505986661992, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 66, 'train_loss': 0.6786289902056678, 'val_loss': 0.6769806951539129, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 67, 'train_loss': 0.6786110936585119, 'val_loss': 0.6771819937942375, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 68, 'train_loss': 0.6784628841836574, 'val_loss': 0.6771634745801616, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 69, 'train_loss': 0.6783935660022801, 'val_loss': 0.676997323321481, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 70, 'train_loss': 0.6785164273391335, 'val_loss': 0.6769630888588408, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 71, 'train_loss': 0.6784356438507468, 'val_loss': 0.6769544161283053, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 72, 'train_loss': 0.6786381784131972, 'val_loss': 0.6769516121627938, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 73, 'train_loss': 0.6783859487307273, 'val_loss': 0.6769760001418937, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 74, 'train_loss': 0.6785184660200345, 'val_loss': 0.6769566332173144, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 75, 'train_loss': 0.678548048108311, 'val_loss': 0.6770644228682559, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 76, 'train_loss': 0.6784892112521802, 'val_loss': 0.6770207331730769, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 77, 'train_loss': 0.6782843975697533, 'val_loss': 0.6769518729967948, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 78, 'train_loss': 0.6784697023488707, 'val_loss': 0.6769595675998263, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 79, 'train_loss': 0.6783849425235037, 'val_loss': 0.6769526554987981, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 80, 'train_loss': 0.6783910201767743, 'val_loss': 0.6769610673953326, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 81, 'train_loss': 0.6783393560829809, 'val_loss': 0.677049816164196, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 82, 'train_loss': 0.6783751643310159, 'val_loss': 0.6770816379123263, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 83, 'train_loss': 0.6784136770135265, 'val_loss': 0.6769923022669605, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 84, 'train_loss': 0.6785320619405326, 'val_loss': 0.6770533374232105, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 85, 'train_loss': 0.6783799347230943, 'val_loss': 0.6769586546808226, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 86, 'train_loss': 0.6783879682169123, 'val_loss': 0.6770286886101096, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 87, 'train_loss': 0.6787804587412689, 'val_loss': 0.6771353045080462, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 88, 'train_loss': 0.6785726193654336, 'val_loss': 0.6769811516134148, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 89, 'train_loss': 0.6783356898922032, 'val_loss': 0.6769583938468215, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 90, 'train_loss': 0.6783714344946005, 'val_loss': 0.677032470703125, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 91, 'train_loss': 0.6783718830448086, 'val_loss': 0.6770029964610043, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 92, 'train_loss': 0.6784766073954307, 'val_loss': 0.6770546415932158, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 93, 'train_loss': 0.6790741176928504, 'val_loss': 0.6769591111403245, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 94, 'train_loss': 0.67850426193011, 'val_loss': 0.6770073654305222, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 95, 'train_loss': 0.6785545480453362, 'val_loss': 0.6771158723749666, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 96, 'train_loss': 0.6784271456427493, 'val_loss': 0.676999018742488, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 97, 'train_loss': 0.6786590137724149, 'val_loss': 0.6769559811323117, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 98, 'train_loss': 0.6784827032331693, 'val_loss': 0.6769520686222956, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 99, 'train_loss': 0.6786853852918593, 'val_loss': 0.6772499410515158, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 100, 'train_loss': 0.6785975037995031, 'val_loss': 0.6774047460311499, 'test_acc': 0.5897435897435898}
Val Loss: 0.6757, Test Accuracy: 0.587 ± 0.003, Duration: 17.162
Best result - 0.587 ± 0.003
--
DD - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6900513101432283, 'val_loss': 0.6763818854959602, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6789454443980072, 'val_loss': 0.6760509523571047, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6794886639562704, 'val_loss': 0.6767511612329727, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6730793522576154, 'val_loss': 0.6730296469142294, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6677652021585885, 'val_loss': 0.6663284301757812, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6473022194231971, 'val_loss': 0.6466451791616586, 'test_acc': 0.6495726495726496}
{'fold': 9, 'epoch': 7, 'train_loss': 0.6323276586451773, 'val_loss': 0.6396180503388755, 'test_acc': 0.5641025641025641}
{'fold': 9, 'epoch': 8, 'train_loss': 0.6014427269919443, 'val_loss': 0.6353809976170206, 'test_acc': 0.6239316239316239}
{'fold': 9, 'epoch': 9, 'train_loss': 0.6106489644212237, 'val_loss': 0.6317731417142428, 'test_acc': 0.6495726495726496}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5869075746859534, 'val_loss': 0.6323441073425815, 'test_acc': 0.6324786324786325}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5819701279624033, 'val_loss': 0.6318625425681089, 'test_acc': 0.6153846153846154}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5581555447335971, 'val_loss': 0.6480843307625535, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5441049711178925, 'val_loss': 0.6766774756276709, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5451144111358514, 'val_loss': 0.6354852008004473, 'test_acc': 0.6068376068376068}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5533041054919615, 'val_loss': 0.6308832413110977, 'test_acc': 0.6239316239316239}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5497487783432007, 'val_loss': 0.6246375059470152, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 17, 'train_loss': 0.530153405868401, 'val_loss': 0.6249066866361178, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5321599174354036, 'val_loss': 0.6352448422684629, 'test_acc': 0.6324786324786325}
{'fold': 9, 'epoch': 19, 'train_loss': 0.49626327318660285, 'val_loss': 0.619272215753539, 'test_acc': 0.6324786324786325}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5115818209567312, 'val_loss': 0.6321895631969484, 'test_acc': 0.6581196581196581}
{'fold': 9, 'epoch': 21, 'train_loss': 0.49226332670551237, 'val_loss': 0.6684848100711138, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 22, 'train_loss': 0.494141441280559, 'val_loss': 0.5760252210828993, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 23, 'train_loss': 0.4590940869460672, 'val_loss': 0.6505775125617654, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 24, 'train_loss': 0.4798903702679327, 'val_loss': 0.5722508878789396, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 25, 'train_loss': 0.4593780424635289, 'val_loss': 0.5372495732755742, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 26, 'train_loss': 0.4356114914861776, 'val_loss': 0.5219025245079627, 'test_acc': 0.7606837606837606}
{'fold': 9, 'epoch': 27, 'train_loss': 0.4150501429024389, 'val_loss': 0.5856049366486378, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 28, 'train_loss': 0.38926111287989856, 'val_loss': 0.6154658651759481, 'test_acc': 0.7521367521367521}
{'fold': 9, 'epoch': 29, 'train_loss': 0.37038086379988716, 'val_loss': 0.6473457140800281, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 30, 'train_loss': 0.3622548393273758, 'val_loss': 0.8613366021050347, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 31, 'train_loss': 0.3557473476660454, 'val_loss': 0.7049346662994124, 'test_acc': 0.6495726495726496}
{'fold': 9, 'epoch': 32, 'train_loss': 0.38995902669631827, 'val_loss': 0.6161300137511685, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 33, 'train_loss': 0.3345157089879957, 'val_loss': 0.9003462832198184, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 34, 'train_loss': 0.3400074182930639, 'val_loss': 0.6157428056765826, 'test_acc': 0.7521367521367521}
{'fold': 9, 'epoch': 35, 'train_loss': 0.30046221714908794, 'val_loss': 1.0358001839401376, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 36, 'train_loss': 0.27106855986482004, 'val_loss': 0.9850506252712674, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 37, 'train_loss': 0.24226916695045211, 'val_loss': 1.2540611853966346, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 38, 'train_loss': 0.25300935668460395, 'val_loss': 1.2236358120910122, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 39, 'train_loss': 0.2271946244320627, 'val_loss': 1.309596917568109, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 40, 'train_loss': 0.298831745477046, 'val_loss': 1.632592486520099, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 41, 'train_loss': 0.25197834584672574, 'val_loss': 1.5232365274021769, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 42, 'train_loss': 0.2487233281135559, 'val_loss': 1.4246333195612981, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 43, 'train_loss': 0.27610306062940826, 'val_loss': 1.5520721174712875, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 44, 'train_loss': 0.23435849532232447, 'val_loss': 1.8872841076973157, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 45, 'train_loss': 0.22036247182700594, 'val_loss': 1.2407602163461537, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 46, 'train_loss': 0.18404931069935782, 'val_loss': 1.9275327503171742, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 47, 'train_loss': 0.20550332407830124, 'val_loss': 1.666195078792735, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 48, 'train_loss': 0.15484212364180613, 'val_loss': 1.9457124889406383, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 49, 'train_loss': 0.133777053300607, 'val_loss': 2.8229440542367787, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 50, 'train_loss': 0.11514200156523009, 'val_loss': 3.0406045506143164, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 51, 'train_loss': 0.16039326786994934, 'val_loss': 2.448627178485577, 'test_acc': 0.6239316239316239}
{'fold': 9, 'epoch': 52, 'train_loss': 0.19522486197746405, 'val_loss': 1.6662224663628473, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 53, 'train_loss': 0.16600887628935151, 'val_loss': 2.9184852013221154, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 54, 'train_loss': 0.1061426617078862, 'val_loss': 3.9326231866820245, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 55, 'train_loss': 0.09457632874028157, 'val_loss': 3.7700706547142095, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 56, 'train_loss': 0.1244015279462782, 'val_loss': 3.073916965060764, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 57, 'train_loss': 0.14412982736603688, 'val_loss': 2.574344732822516, 'test_acc': 0.7521367521367521}
{'fold': 9, 'epoch': 58, 'train_loss': 0.12671306148423986, 'val_loss': 2.4678965511485043, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 59, 'train_loss': 0.09258884620868553, 'val_loss': 2.6855100974058495, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 60, 'train_loss': 0.07614478342613931, 'val_loss': 3.790963197365785, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 61, 'train_loss': 0.058581036026194945, 'val_loss': 3.9925868368556356, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 62, 'train_loss': 0.06380249667218175, 'val_loss': 4.013039613381411, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 63, 'train_loss': 0.06474111336520162, 'val_loss': 5.118212577624199, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 64, 'train_loss': 0.08338481910779315, 'val_loss': 4.097470536191239, 'test_acc': 0.5641025641025641}
{'fold': 9, 'epoch': 65, 'train_loss': 0.402427403856132, 'val_loss': 1.2805409227680957, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 66, 'train_loss': 0.2686473823199838, 'val_loss': 1.8072295881744125, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 67, 'train_loss': 0.15567969114093458, 'val_loss': 3.5907133705595617, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 68, 'train_loss': 0.11510321106445992, 'val_loss': 3.8717069707365117, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 69, 'train_loss': 0.0741131787772401, 'val_loss': 3.3231036846454325, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 70, 'train_loss': 0.04712465247612888, 'val_loss': 3.7463355431189904, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 71, 'train_loss': 0.033163305435140254, 'val_loss': 4.800909058660523, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 72, 'train_loss': 0.0726615206922515, 'val_loss': 4.604454627403846, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 73, 'train_loss': 0.05252592783358138, 'val_loss': 3.4013471032819176, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 74, 'train_loss': 0.04518694145668108, 'val_loss': 3.4547575600126867, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 75, 'train_loss': 0.032437361397985684, 'val_loss': 4.37751704811031, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 76, 'train_loss': 0.02952203211390366, 'val_loss': 5.582630124866453, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 77, 'train_loss': 0.025584119861408815, 'val_loss': 6.3178570087139425, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 78, 'train_loss': 0.02944287256894128, 'val_loss': 5.804815308660523, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 79, 'train_loss': 0.018025315982305397, 'val_loss': 5.668632311698718, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 80, 'train_loss': 0.023331230727292724, 'val_loss': 6.420780540531517, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 81, 'train_loss': 0.08417348803605064, 'val_loss': 5.370322203024839, 'test_acc': 0.6581196581196581}
{'fold': 9, 'epoch': 82, 'train_loss': 0.12028053289247771, 'val_loss': 3.608661619007078, 'test_acc': 0.6068376068376068}
{'fold': 9, 'epoch': 83, 'train_loss': 0.14762497301829064, 'val_loss': 2.581616002270299, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 84, 'train_loss': 0.0909189863477723, 'val_loss': 3.3814796382545405, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 85, 'train_loss': 0.06740262182587284, 'val_loss': 4.50155039730235, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 86, 'train_loss': 0.05391426112944797, 'val_loss': 6.245723365718483, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 87, 'train_loss': 0.03185045346617699, 'val_loss': 6.686528654180021, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 88, 'train_loss': 0.026439163139310932, 'val_loss': 6.3415303026509084, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 89, 'train_loss': 0.02878288615305545, 'val_loss': 7.288288344684829, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 90, 'train_loss': 0.02002909939767698, 'val_loss': 7.961482121394231, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 91, 'train_loss': 0.011039922290118569, 'val_loss': 7.019775912293002, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 92, 'train_loss': 0.00749443024712599, 'val_loss': 9.606512086004274, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 93, 'train_loss': 0.008986944518194108, 'val_loss': 8.798637194511217, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 94, 'train_loss': 0.0062337183120751245, 'val_loss': 10.169259356637287, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 95, 'train_loss': 0.00412109440539853, 'val_loss': 10.366093040531517, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 96, 'train_loss': 0.0071907577073296244, 'val_loss': 11.05678460536859, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 97, 'train_loss': 0.002288818233093973, 'val_loss': 10.678219526241987, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 98, 'train_loss': 0.0018352186899253371, 'val_loss': 10.468766693376068, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 99, 'train_loss': 0.0030468122311473147, 'val_loss': 10.839165581597221, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 100, 'train_loss': 0.0018409733913081177, 'val_loss': 11.305382361778847, 'test_acc': 0.6923076923076923}
Val Loss: 0.5024, Test Accuracy: 0.754 ± 0.040, Duration: 262.108
Best result - 0.754 ± 0.040
--
NCI1 - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6856926676710736, 'val_loss': 0.640893265568717, 'test_acc': 0.5790754257907542}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6542028177103567, 'val_loss': 0.6155389219594988, 'test_acc': 0.6058394160583942}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6415886842711418, 'val_loss': 0.6045057500945971, 'test_acc': 0.6155717761557178}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6313986620183699, 'val_loss': 0.599576112417699, 'test_acc': 0.6447688564476886}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6230061632873368, 'val_loss': 0.6004722530244331, 'test_acc': 0.6447688564476886}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6117740786858719, 'val_loss': 0.5835123642227655, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 7, 'train_loss': 0.6020967361410748, 'val_loss': 0.5762027638381995, 'test_acc': 0.6642335766423357}
{'fold': 9, 'epoch': 8, 'train_loss': 0.6026433227706129, 'val_loss': 0.5732043027297714, 'test_acc': 0.6545012165450121}
{'fold': 9, 'epoch': 9, 'train_loss': 0.6009676846564541, 'val_loss': 0.5729841431852095, 'test_acc': 0.6520681265206812}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5890544454256693, 'val_loss': 0.5859265710315565, 'test_acc': 0.656934306569343}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5825050610985489, 'val_loss': 0.5848095399619889, 'test_acc': 0.6690997566909975}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5868661090405318, 'val_loss': 0.5804076647236399, 'test_acc': 0.6593673965936739}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5696816297923272, 'val_loss': 0.5757189887573539, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 14, 'train_loss': 0.570267766206514, 'val_loss': 0.5867615952688993, 'test_acc': 0.6690997566909975}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5623542369137136, 'val_loss': 0.5774693755917886, 'test_acc': 0.6690997566909975}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5548086099671041, 'val_loss': 0.5688908082725358, 'test_acc': 0.6885644768856448}
{'fold': 9, 'epoch': 17, 'train_loss': 0.541221702620931, 'val_loss': 0.5734865067939109, 'test_acc': 0.683698296836983}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5437071124712626, 'val_loss': 0.5770989253283128, 'test_acc': 0.6739659367396593}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5956606524124053, 'val_loss': 0.5464825131307263, 'test_acc': 0.6690997566909975}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5446194414674801, 'val_loss': 0.5673560980172633, 'test_acc': 0.6618004866180048}
{'fold': 9, 'epoch': 21, 'train_loss': 0.5411218378665673, 'val_loss': 0.5806152547943041, 'test_acc': 0.6885644768856448}
{'fold': 9, 'epoch': 22, 'train_loss': 0.5210373420494896, 'val_loss': 0.6069145434674266, 'test_acc': 0.6739659367396593}
{'fold': 9, 'epoch': 23, 'train_loss': 0.5188256858909217, 'val_loss': 0.6381708525683178, 'test_acc': 0.6885644768856448}
{'fold': 9, 'epoch': 24, 'train_loss': 0.49897286636695953, 'val_loss': 0.5957062250216222, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 25, 'train_loss': 0.5042714164784935, 'val_loss': 0.6024352024941548, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 26, 'train_loss': 0.4886895574792458, 'val_loss': 0.6190563141574534, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 27, 'train_loss': 0.4839818518237186, 'val_loss': 0.6901254329078099, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 28, 'train_loss': 0.48679946228825555, 'val_loss': 0.6850734627159842, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 29, 'train_loss': 0.4976849365988497, 'val_loss': 0.6383950310031863, 'test_acc': 0.6958637469586375}
{'fold': 9, 'epoch': 30, 'train_loss': 0.4703377177100402, 'val_loss': 0.6396678752852762, 'test_acc': 0.6958637469586375}
{'fold': 9, 'epoch': 31, 'train_loss': 0.4810037582460111, 'val_loss': 0.599127725383081, 'test_acc': 0.6788321167883211}
{'fold': 9, 'epoch': 32, 'train_loss': 0.4745680446172283, 'val_loss': 0.5833376654743279, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 33, 'train_loss': 0.4803737319298904, 'val_loss': 0.667660729438429, 'test_acc': 0.681265206812652}
{'fold': 9, 'epoch': 34, 'train_loss': 0.45706873044480373, 'val_loss': 0.6670044251602062, 'test_acc': 0.6642335766423357}
{'fold': 9, 'epoch': 35, 'train_loss': 0.4647079608301177, 'val_loss': 0.596293317140454, 'test_acc': 0.6763990267639902}
{'fold': 9, 'epoch': 36, 'train_loss': 0.4305604392449641, 'val_loss': 0.615974305610007, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 37, 'train_loss': 0.42510071259055404, 'val_loss': 0.607556431252881, 'test_acc': 0.6788321167883211}
{'fold': 9, 'epoch': 38, 'train_loss': 0.416844795945207, 'val_loss': 0.6118897602795974, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 39, 'train_loss': 0.40324570691788575, 'val_loss': 0.6190406377008072, 'test_acc': 0.6958637469586375}
{'fold': 9, 'epoch': 40, 'train_loss': 0.386625285348753, 'val_loss': 0.6743140185836458, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 41, 'train_loss': 0.39722982447802874, 'val_loss': 0.7068146847170345, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 42, 'train_loss': 0.39788532097554263, 'val_loss': 0.6834677188065801, 'test_acc': 0.6885644768856448}
{'fold': 9, 'epoch': 43, 'train_loss': 0.3705650411383079, 'val_loss': 0.7012500043623059, 'test_acc': 0.683698296836983}
{'fold': 9, 'epoch': 44, 'train_loss': 0.37062572649795644, 'val_loss': 0.750875463740959, 'test_acc': 0.708029197080292}
{'fold': 9, 'epoch': 45, 'train_loss': 0.36662707965449404, 'val_loss': 0.6715099074834745, 'test_acc': 0.6885644768856448}
{'fold': 9, 'epoch': 46, 'train_loss': 0.3433309254710112, 'val_loss': 0.7462491942728233, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 47, 'train_loss': 0.3380564635397454, 'val_loss': 0.8536322122652745, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 48, 'train_loss': 0.33308371393930014, 'val_loss': 0.7815969007728744, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 49, 'train_loss': 0.3390230471200317, 'val_loss': 0.7595387131628328, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 50, 'train_loss': 0.3296179076760935, 'val_loss': 0.747328324329534, 'test_acc': 0.7055961070559611}
{'fold': 9, 'epoch': 51, 'train_loss': 0.3086069040997475, 'val_loss': 0.8215213506471212, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 52, 'train_loss': 0.3029023984632933, 'val_loss': 0.8395449761346598, 'test_acc': 0.7007299270072993}
{'fold': 9, 'epoch': 53, 'train_loss': 0.2935254280149502, 'val_loss': 0.7900796365853934, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 54, 'train_loss': 0.29246309087804345, 'val_loss': 0.8977153898735696, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 55, 'train_loss': 0.29225615986652326, 'val_loss': 1.023861966283942, 'test_acc': 0.6958637469586375}
{'fold': 9, 'epoch': 56, 'train_loss': 0.3091724378276625, 'val_loss': 0.9287953249149369, 'test_acc': 0.6788321167883211}
{'fold': 9, 'epoch': 57, 'train_loss': 0.3149216827684945, 'val_loss': 0.8299575350870472, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 58, 'train_loss': 0.2939917351642664, 'val_loss': 0.8850848552954458, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 59, 'train_loss': 0.3112507695989307, 'val_loss': 0.9180381431486775, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 60, 'train_loss': 0.279588236135868, 'val_loss': 0.8711322357474742, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 61, 'train_loss': 0.27749296507986215, 'val_loss': 1.0661619423079665, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 62, 'train_loss': 0.27286900337015046, 'val_loss': 0.9725173778487528, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 63, 'train_loss': 0.2550685422046341, 'val_loss': 0.9545585627683468, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 64, 'train_loss': 0.24911350026327908, 'val_loss': 1.0271744739690256, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 65, 'train_loss': 0.23780610388792925, 'val_loss': 1.0295408346357138, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 66, 'train_loss': 0.24873471071540293, 'val_loss': 1.1332949199815736, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 67, 'train_loss': 0.26318813965558424, 'val_loss': 1.0879043801857606, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 68, 'train_loss': 0.2521115930846138, 'val_loss': 1.1578252124090265, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 69, 'train_loss': 0.2765373604286036, 'val_loss': 1.0362858760675955, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 70, 'train_loss': 0.24889531945515142, 'val_loss': 1.114676433758144, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 71, 'train_loss': 0.22789009441134414, 'val_loss': 1.0429922994906016, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 72, 'train_loss': 0.22000507528184393, 'val_loss': 1.251183050392318, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 73, 'train_loss': 0.2042300842901796, 'val_loss': 1.2870560597329244, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 74, 'train_loss': 0.23090183129420827, 'val_loss': 1.0192221597453393, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 75, 'train_loss': 0.25710634419517797, 'val_loss': 1.1933756037060073, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 76, 'train_loss': 0.25404578483597784, 'val_loss': 1.1645449153408227, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 77, 'train_loss': 0.26864246284439613, 'val_loss': 1.1127906483745342, 'test_acc': 0.7007299270072993}
{'fold': 9, 'epoch': 78, 'train_loss': 0.22901699541549034, 'val_loss': 1.3128614750511745, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 79, 'train_loss': 0.23343343521556714, 'val_loss': 1.2340606912209169, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 80, 'train_loss': 0.2577025106498505, 'val_loss': 1.1096414881611103, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 81, 'train_loss': 0.2458505341171348, 'val_loss': 1.3790099429388116, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 82, 'train_loss': 0.22893038044010636, 'val_loss': 1.1770604196256094, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 83, 'train_loss': 0.22096187673926063, 'val_loss': 1.3110907478054075, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 84, 'train_loss': 0.22483466199424726, 'val_loss': 1.541276864471807, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 85, 'train_loss': 0.23143761429183385, 'val_loss': 1.2130973518909909, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 86, 'train_loss': 0.21360549149432032, 'val_loss': 1.2134674262536413, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 87, 'train_loss': 0.1911894549636075, 'val_loss': 1.4377114186901825, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 88, 'train_loss': 0.21392490517193963, 'val_loss': 1.3001253448263572, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 89, 'train_loss': 0.2081900920238518, 'val_loss': 1.121596385092631, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 90, 'train_loss': 0.1894133481811143, 'val_loss': 1.2334831520878775, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 91, 'train_loss': 0.1740784865697515, 'val_loss': 1.3951172004941026, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 92, 'train_loss': 0.17574297210740927, 'val_loss': 1.360278514759964, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 93, 'train_loss': 0.19051876475631174, 'val_loss': 1.2797133870368456, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 94, 'train_loss': 0.18375290872261762, 'val_loss': 1.509253418358573, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 95, 'train_loss': 0.18044869612603293, 'val_loss': 1.4836585237452002, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 96, 'train_loss': 0.16215959651771833, 'val_loss': 1.5475234938944054, 'test_acc': 0.7420924574209246}
{'fold': 9, 'epoch': 97, 'train_loss': 0.17425792919893335, 'val_loss': 1.46495445975422, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 98, 'train_loss': 0.16725861906570239, 'val_loss': 1.6692016861444552, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 99, 'train_loss': 0.15708142540750714, 'val_loss': 1.6906002443782315, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 100, 'train_loss': 0.1687981011006084, 'val_loss': 1.5614422614847077, 'test_acc': 0.7153284671532847}
Val Loss: 0.5914, Test Accuracy: 0.673 ± 0.047, Duration: 43.499
Best result - 0.673 ± 0.047
--
NCI1 - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6947956611640262, 'val_loss': 0.6865703044436564, 'test_acc': 0.5450121654501217}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6861972130128067, 'val_loss': 0.6595524975853245, 'test_acc': 0.5936739659367397}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6663847301708231, 'val_loss': 0.6575311535466326, 'test_acc': 0.6228710462287105}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6596312015306051, 'val_loss': 0.6333797668308527, 'test_acc': 0.6301703163017032}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6473692217592485, 'val_loss': 0.6191783937514553, 'test_acc': 0.6180048661800487}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6325406314682787, 'val_loss': 0.6340120969897639, 'test_acc': 0.6034063260340633}
{'fold': 9, 'epoch': 7, 'train_loss': 0.6265980594059557, 'val_loss': 0.6013450668966103, 'test_acc': 0.6593673965936739}
{'fold': 9, 'epoch': 8, 'train_loss': 0.6154526112433477, 'val_loss': 0.5934687751343071, 'test_acc': 0.6423357664233577}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5841935104407243, 'val_loss': 0.6062710801470309, 'test_acc': 0.6204379562043796}
{'fold': 9, 'epoch': 10, 'train_loss': 0.617688403657463, 'val_loss': 0.5638177853141098, 'test_acc': 0.6861313868613139}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5823506311198511, 'val_loss': 0.5532357106823701, 'test_acc': 0.6690997566909975}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5877358131745146, 'val_loss': 0.5429531385138667, 'test_acc': 0.681265206812652}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5762660849993536, 'val_loss': 0.5464986211772093, 'test_acc': 0.6739659367396593}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5873582838805632, 'val_loss': 0.5570297798101049, 'test_acc': 0.6642335766423357}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5639011893249196, 'val_loss': 0.5312283857025369, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5620440315736182, 'val_loss': 0.5406334916460543, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 17, 'train_loss': 0.549198996121576, 'val_loss': 0.5879509872473649, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5561431676801973, 'val_loss': 0.5405200554506622, 'test_acc': 0.7007299270072993}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5430304806887959, 'val_loss': 0.623740896981418, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5481977008845105, 'val_loss': 0.5988839420959027, 'test_acc': 0.7007299270072993}
{'fold': 9, 'epoch': 21, 'train_loss': 0.5565123179533186, 'val_loss': 0.6010271568948045, 'test_acc': 0.6958637469586375}
{'fold': 9, 'epoch': 22, 'train_loss': 0.534943067969487, 'val_loss': 0.6692446130905708, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 23, 'train_loss': 0.538360617746692, 'val_loss': 0.5760841949722773, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 24, 'train_loss': 0.5440496661657255, 'val_loss': 0.6189746624651906, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 25, 'train_loss': 0.5205132008469018, 'val_loss': 0.6588190176191121, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 26, 'train_loss': 0.5262532656210183, 'val_loss': 0.6542969144463829, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 27, 'train_loss': 0.5164885009002221, 'val_loss': 0.7079025718707528, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 28, 'train_loss': 0.5193321653381171, 'val_loss': 1.2312398641358906, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 29, 'train_loss': 0.4998060028338374, 'val_loss': 0.8099303164331292, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 30, 'train_loss': 0.5079687120270555, 'val_loss': 0.7507808179460883, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 31, 'train_loss': 0.5009548675694895, 'val_loss': 0.6917458717550384, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 32, 'train_loss': 0.5004618978558376, 'val_loss': 0.7009759540975529, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 33, 'train_loss': 0.49194200587098613, 'val_loss': 0.7098534275435473, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 34, 'train_loss': 0.5143681281674517, 'val_loss': 0.7014092586916438, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 35, 'train_loss': 0.49335151555474366, 'val_loss': 0.5811386827714832, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 36, 'train_loss': 0.4862313382236917, 'val_loss': 0.6655088874835458, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 37, 'train_loss': 0.4927439862939273, 'val_loss': 0.6670171199343791, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 38, 'train_loss': 0.4774257549404228, 'val_loss': 0.5727897931769527, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 39, 'train_loss': 0.5064099818250559, 'val_loss': 0.5744669257579349, 'test_acc': 0.7007299270072993}
{'fold': 9, 'epoch': 40, 'train_loss': 0.49347683475545434, 'val_loss': 0.6812768736604936, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 41, 'train_loss': 0.46485819636760256, 'val_loss': 0.6344997935051465, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 42, 'train_loss': 0.4780713173304741, 'val_loss': 0.5124459440690757, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 43, 'train_loss': 0.4720876638906716, 'val_loss': 0.5133131658363806, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4591484906609621, 'val_loss': 0.569459831627616, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 45, 'train_loss': 0.47669857551871714, 'val_loss': 0.6664088822339282, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 46, 'train_loss': 0.47693532067203753, 'val_loss': 0.5212011963781649, 'test_acc': 0.7420924574209246}
{'fold': 9, 'epoch': 47, 'train_loss': 0.4645847982733789, 'val_loss': 0.5090425844030079, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 48, 'train_loss': 0.4673346186061265, 'val_loss': 0.49175292848090474, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 49, 'train_loss': 0.4568868202594655, 'val_loss': 0.5271933687864429, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 50, 'train_loss': 0.4535768085732657, 'val_loss': 0.5004814918313873, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 51, 'train_loss': 0.45581230086131685, 'val_loss': 0.5207505098514603, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 52, 'train_loss': 0.44949054928301607, 'val_loss': 0.538679881687582, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 53, 'train_loss': 0.4393918064678963, 'val_loss': 0.5406321852746672, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 54, 'train_loss': 0.449918065918043, 'val_loss': 0.5672831883395675, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 55, 'train_loss': 0.4502261353525222, 'val_loss': 0.59618724581679, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 56, 'train_loss': 0.4629462528837858, 'val_loss': 0.6260779165003422, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 57, 'train_loss': 0.4576022011520219, 'val_loss': 0.6196848760266084, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 58, 'train_loss': 0.43886149712722666, 'val_loss': 0.6159901212891813, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 59, 'train_loss': 0.4349490566555311, 'val_loss': 0.6055286388907699, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 60, 'train_loss': 0.431510907817641, 'val_loss': 0.6140427879463437, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 61, 'train_loss': 0.4633150064452141, 'val_loss': 0.5756189898562838, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 62, 'train_loss': 0.4448824336348949, 'val_loss': 0.6534456034936464, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 63, 'train_loss': 0.43294996755546605, 'val_loss': 0.6115290463115757, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 64, 'train_loss': 0.42664114046850926, 'val_loss': 0.5064688564217004, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 65, 'train_loss': 0.42871613608369574, 'val_loss': 0.5406436873758507, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 66, 'train_loss': 0.42128134324892885, 'val_loss': 0.5601897993807085, 'test_acc': 0.7420924574209246}
{'fold': 9, 'epoch': 67, 'train_loss': 0.44134152682448247, 'val_loss': 0.5993880715103335, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 68, 'train_loss': 0.4283862865899311, 'val_loss': 0.5964109021671787, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 69, 'train_loss': 0.4180968704014799, 'val_loss': 0.5841573009815819, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 70, 'train_loss': 0.4276213619830835, 'val_loss': 0.6262678833193442, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 71, 'train_loss': 0.4263309826961109, 'val_loss': 0.7239157938899204, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 72, 'train_loss': 0.40182613463587424, 'val_loss': 0.6765903092358814, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 73, 'train_loss': 0.39482104771038623, 'val_loss': 0.7297754380534746, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 74, 'train_loss': 0.3980639064979089, 'val_loss': 0.7083377745319747, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 75, 'train_loss': 0.4217632705857864, 'val_loss': 0.6443787897300256, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 76, 'train_loss': 0.4276593608142686, 'val_loss': 0.5786745925309305, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 77, 'train_loss': 0.43685991413111813, 'val_loss': 0.5125753096420399, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 78, 'train_loss': 0.4166270799735457, 'val_loss': 0.5555186236861849, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 79, 'train_loss': 0.4158296105780451, 'val_loss': 0.6067946427059869, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 80, 'train_loss': 0.41181750570190506, 'val_loss': 0.6171582122208719, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 81, 'train_loss': 0.383604447772033, 'val_loss': 0.733266078642685, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 82, 'train_loss': 0.3849639442280261, 'val_loss': 0.7057305403289423, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 83, 'train_loss': 0.38997512323432887, 'val_loss': 0.8402202773268206, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 84, 'train_loss': 0.39688730856217896, 'val_loss': 0.6336876388883939, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 85, 'train_loss': 0.4049474568697658, 'val_loss': 0.721392000388635, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 86, 'train_loss': 0.3828506098466488, 'val_loss': 0.7962294019341759, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 87, 'train_loss': 0.3790626713684295, 'val_loss': 0.9115544943333832, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 88, 'train_loss': 0.3882688214309024, 'val_loss': 0.6874793857843626, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 89, 'train_loss': 0.40030982300022805, 'val_loss': 0.6197101890025638, 'test_acc': 0.7420924574209246}
{'fold': 9, 'epoch': 90, 'train_loss': 0.3886014506619632, 'val_loss': 0.5783237749642699, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 91, 'train_loss': 0.3695972745290928, 'val_loss': 0.5749506915572786, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 92, 'train_loss': 0.38930679556807174, 'val_loss': 0.5888446427319751, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 93, 'train_loss': 0.4069126278814608, 'val_loss': 0.5534898149996198, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 94, 'train_loss': 0.4251281115115414, 'val_loss': 0.6859211283588641, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 95, 'train_loss': 0.38870814573155704, 'val_loss': 0.5050991991140547, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 96, 'train_loss': 0.3798899372853792, 'val_loss': 0.6176476791827348, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 97, 'train_loss': 0.38076728024041856, 'val_loss': 0.6948982758533636, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 98, 'train_loss': 0.37423833175007154, 'val_loss': 0.6850242753968622, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 99, 'train_loss': 0.3730364422293475, 'val_loss': 0.718504204947293, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 100, 'train_loss': 0.3726425882238541, 'val_loss': 0.6368219278154582, 'test_acc': 0.7785888077858881}
Val Loss: 0.5483, Test Accuracy: 0.745 ± 0.025, Duration: 455.259
Best result - 0.745 ± 0.025
--
PROTEINS - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.8999555387213308, 'val_loss': 0.670504870715442, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6376626769835433, 'val_loss': 0.594272922825169, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5873938136630588, 'val_loss': 0.5764403472075591, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5776972547257121, 'val_loss': 0.5588750065983953, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 5, 'train_loss': 0.5985158711972863, 'val_loss': 0.5798227808496974, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 6, 'train_loss': 0.580177096873689, 'val_loss': 0.5700186652106207, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5592095735833033, 'val_loss': 0.5370534948400549, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 8, 'train_loss': 0.5442920804826499, 'val_loss': 0.5341682434082031, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5641712452693687, 'val_loss': 0.5353188042168144, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 10, 'train_loss': 0.548981595253436, 'val_loss': 0.5436658773336325, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5489137273043494, 'val_loss': 0.5428009033203125, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5350922680596845, 'val_loss': 0.5349099099099099, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5428499149955082, 'val_loss': 0.5407053973223712, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 14, 'train_loss': 0.534624318014224, 'val_loss': 0.5399382651389182, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5236662402832682, 'val_loss': 0.5392434060036599, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5309755185370226, 'val_loss': 0.53570979350322, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 17, 'train_loss': 0.5294337267961299, 'val_loss': 0.5370227366954357, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5304256775965193, 'val_loss': 0.5345408809077632, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5258899066852266, 'val_loss': 0.5515627130731806, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5192242243340789, 'val_loss': 0.5307412705979906, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 21, 'train_loss': 0.5286671780844195, 'val_loss': 0.5485550820290506, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 22, 'train_loss': 0.5234419668028518, 'val_loss': 0.5540900359282622, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 23, 'train_loss': 0.522502098688239, 'val_loss': 0.5388613520441828, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 24, 'train_loss': 0.5275898018536209, 'val_loss': 0.5358089068988422, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 25, 'train_loss': 0.5161386485787487, 'val_loss': 0.5473726289766329, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 26, 'train_loss': 0.517650334789563, 'val_loss': 0.5390935330777555, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 27, 'train_loss': 0.5137116309635834, 'val_loss': 0.5468059574161563, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 28, 'train_loss': 0.5461233837569737, 'val_loss': 0.5354949332572319, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 29, 'train_loss': 0.555121157573396, 'val_loss': 0.5413635528839387, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 30, 'train_loss': 0.5484648422493812, 'val_loss': 0.542965553902291, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 31, 'train_loss': 0.5398341582546599, 'val_loss': 0.5392400037060987, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 32, 'train_loss': 0.536121974689792, 'val_loss': 0.5382429930540893, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 33, 'train_loss': 0.5389986863708924, 'val_loss': 0.5280605694195172, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 34, 'train_loss': 0.5111090416458721, 'val_loss': 0.5356812004570488, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 35, 'train_loss': 0.5161296344632921, 'val_loss': 0.5398783297152132, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 36, 'train_loss': 0.5073270563592279, 'val_loss': 0.5443135167027379, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 37, 'train_loss': 0.502145992853291, 'val_loss': 0.5777469669376407, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 38, 'train_loss': 0.4855838791957474, 'val_loss': 0.5541968818183418, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 39, 'train_loss': 0.49141597433389905, 'val_loss': 0.5697753837516716, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 40, 'train_loss': 0.47845868336498804, 'val_loss': 0.5916139757311022, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 41, 'train_loss': 0.4650697039835381, 'val_loss': 0.5552249255481067, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 42, 'train_loss': 0.48611632902614194, 'val_loss': 0.5917795542124156, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 43, 'train_loss': 0.49206347858865657, 'val_loss': 0.5753863566630596, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4644864530006777, 'val_loss': 0.6648575378967835, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 45, 'train_loss': 0.46292542464671577, 'val_loss': 0.5632460654318869, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 46, 'train_loss': 0.4367502316013062, 'val_loss': 0.7476508338172156, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 47, 'train_loss': 0.43531976855965177, 'val_loss': 0.5601846162263338, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 48, 'train_loss': 0.4202169860252226, 'val_loss': 0.818393054309192, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 49, 'train_loss': 0.44029726163305416, 'val_loss': 0.6109539410015484, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 50, 'train_loss': 0.4317087066829138, 'val_loss': 0.6641756349855715, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 51, 'train_loss': 0.4188870912047749, 'val_loss': 0.655376537426098, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 52, 'train_loss': 0.43819905618492067, 'val_loss': 0.7055534843925957, 'test_acc': 0.6756756756756757}
{'fold': 9, 'epoch': 53, 'train_loss': 0.4109315845768326, 'val_loss': 0.6671759115683066, 'test_acc': 0.6576576576576577}
{'fold': 9, 'epoch': 54, 'train_loss': 0.4345756270802516, 'val_loss': 0.6270892856357334, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 55, 'train_loss': 0.41514308866025623, 'val_loss': 0.7266917873073269, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 56, 'train_loss': 0.4311076987003772, 'val_loss': 0.6835863268053209, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 57, 'train_loss': 0.36592399776316387, 'val_loss': 1.0376069352433488, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 58, 'train_loss': 0.5263022856011268, 'val_loss': 0.5671515593657622, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 59, 'train_loss': 0.4607163661658162, 'val_loss': 0.6164379635372678, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 60, 'train_loss': 0.4239795169511896, 'val_loss': 0.6985102473078547, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 61, 'train_loss': 0.38970748911267844, 'val_loss': 0.7148666381835938, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 62, 'train_loss': 0.3914160355507458, 'val_loss': 0.770100567791913, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 63, 'train_loss': 0.3821759041645444, 'val_loss': 0.6754382709125141, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 64, 'train_loss': 0.4052766199598944, 'val_loss': 0.7133160152950803, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 65, 'train_loss': 0.3369503172760459, 'val_loss': 0.9757307585295256, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 66, 'train_loss': 0.3277756465739005, 'val_loss': 0.8199741260425465, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 67, 'train_loss': 0.311810266790968, 'val_loss': 0.8300660966752885, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 68, 'train_loss': 0.3117064421894039, 'val_loss': 0.9188352017789274, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 69, 'train_loss': 0.29548372253023014, 'val_loss': 1.0477387024475648, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 70, 'train_loss': 0.3059170126379807, 'val_loss': 1.0075763324359517, 'test_acc': 0.6576576576576577}
{'fold': 9, 'epoch': 71, 'train_loss': 0.30112706069593076, 'val_loss': 1.0397709339588612, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 72, 'train_loss': 0.29125680716752206, 'val_loss': 0.8580048019821579, 'test_acc': 0.6756756756756757}
{'fold': 9, 'epoch': 73, 'train_loss': 0.2823085039017845, 'val_loss': 1.0577725247219876, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 74, 'train_loss': 0.2753011712555666, 'val_loss': 1.0317271464579814, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 75, 'train_loss': 0.2608769481163367, 'val_loss': 0.9839274775874507, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 76, 'train_loss': 0.24619030199869715, 'val_loss': 1.1819750811602618, 'test_acc': 0.6756756756756757}
{'fold': 9, 'epoch': 77, 'train_loss': 0.24434089747609783, 'val_loss': 1.1682256750158362, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 78, 'train_loss': 0.24909901013545582, 'val_loss': 0.9195880374392947, 'test_acc': 0.6576576576576577}
{'fold': 9, 'epoch': 79, 'train_loss': 0.23021422435271352, 'val_loss': 1.459896740612683, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 80, 'train_loss': 0.38842239066138945, 'val_loss': 0.9179125949069187, 'test_acc': 0.6756756756756757}
{'fold': 9, 'epoch': 81, 'train_loss': 0.28339691602287337, 'val_loss': 1.0861193682696368, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 82, 'train_loss': 0.2577575892042766, 'val_loss': 1.0884289440807995, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 83, 'train_loss': 0.24649995154001897, 'val_loss': 1.0617269567541174, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 84, 'train_loss': 0.2129708052718412, 'val_loss': 1.3219157725841075, 'test_acc': 0.6486486486486487}
{'fold': 9, 'epoch': 85, 'train_loss': 0.22054679156620763, 'val_loss': 1.1442873843081363, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 86, 'train_loss': 0.19743925731056333, 'val_loss': 1.2924197085268863, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 87, 'train_loss': 0.19437601811155325, 'val_loss': 1.3826594997096706, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 88, 'train_loss': 0.1915820716941932, 'val_loss': 1.5989338642842061, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 89, 'train_loss': 0.1822784157045495, 'val_loss': 1.6392679300394144, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 90, 'train_loss': 0.19211587120855161, 'val_loss': 1.7159610782657657, 'test_acc': 0.6756756756756757}
{'fold': 9, 'epoch': 91, 'train_loss': 0.17030347070910715, 'val_loss': 1.5836176141962275, 'test_acc': 0.6486486486486487}
{'fold': 9, 'epoch': 92, 'train_loss': 0.14775053611575018, 'val_loss': 2.020662462389147, 'test_acc': 0.6576576576576577}
{'fold': 9, 'epoch': 93, 'train_loss': 0.16416354184734033, 'val_loss': 1.9188640697582349, 'test_acc': 0.6576576576576577}
{'fold': 9, 'epoch': 94, 'train_loss': 0.17522086552631708, 'val_loss': 1.9428381017736487, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 95, 'train_loss': 0.16832562630492295, 'val_loss': 1.5618029070330095, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 96, 'train_loss': 0.18120059399425514, 'val_loss': 1.893912650443412, 'test_acc': 0.6486486486486487}
{'fold': 9, 'epoch': 97, 'train_loss': 0.16309644860517564, 'val_loss': 1.893492552611205, 'test_acc': 0.6756756756756757}
{'fold': 9, 'epoch': 98, 'train_loss': 0.1480254720056097, 'val_loss': 1.8536025038710586, 'test_acc': 0.6396396396396397}
{'fold': 9, 'epoch': 99, 'train_loss': 0.1717215276058809, 'val_loss': 1.7252667401288007, 'test_acc': 0.6756756756756757}
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
{'fold': 9, 'epoch': 100, 'train_loss': 0.1797450016577771, 'val_loss': 1.7446977770006336, 'test_acc': 0.7297297297297297}
Val Loss: 0.5407, Test Accuracy: 0.731 ± 0.040, Duration: 6.424
Best result - 0.731 ± 0.040
--
PROTEINS - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.750790148204157, 'val_loss': 0.6762366767402168, 'test_acc': 0.5945945945945946}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6687012731560686, 'val_loss': 0.653670714782165, 'test_acc': 0.6216216216216216}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6528920823609923, 'val_loss': 0.6214699959969735, 'test_acc': 0.6126126126126126}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6241377980754566, 'val_loss': 0.5697939761050113, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 5, 'train_loss': 0.573659105600599, 'val_loss': 0.5624820262462169, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5829914651066901, 'val_loss': 0.58219964654596, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5752118862705466, 'val_loss': 0.5268626857448269, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 8, 'train_loss': 0.56099382113126, 'val_loss': 0.5398996713999156, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5434579871987922, 'val_loss': 0.5462705938665716, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 10, 'train_loss': 0.561882141864661, 'val_loss': 0.5336917499164203, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5595149272753869, 'val_loss': 0.5691407349732545, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5586401844265485, 'val_loss': 0.5246401950045749, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5664631016594258, 'val_loss': 0.5343139236037796, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5542110846901582, 'val_loss': 0.5151182638632285, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5433417344601735, 'val_loss': 0.522390417150549, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5472882210071106, 'val_loss': 0.5234576388522312, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 17, 'train_loss': 0.5366397543855254, 'val_loss': 0.5269307660626935, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5418722356610025, 'val_loss': 0.5158925786748663, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5397727814253198, 'val_loss': 0.5091703603933523, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5245564836176692, 'val_loss': 0.534048166360941, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 21, 'train_loss': 0.5406119811414468, 'val_loss': 0.5108466277251372, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 22, 'train_loss': 0.5258410015424628, 'val_loss': 0.5100099374582102, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 23, 'train_loss': 0.5186256637209327, 'val_loss': 0.5185070209675007, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 24, 'train_loss': 0.5311378029861835, 'val_loss': 0.5170517655106278, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 25, 'train_loss': 0.5215298865245516, 'val_loss': 0.5638995471301379, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 26, 'train_loss': 0.5291869768122364, 'val_loss': 0.506199192356419, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 27, 'train_loss': 0.5428782575727178, 'val_loss': 0.5095536515519425, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 28, 'train_loss': 0.5375201288029535, 'val_loss': 0.5462443377520587, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 29, 'train_loss': 0.5428882413841658, 'val_loss': 0.5303518278104765, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 30, 'train_loss': 0.5460188360727998, 'val_loss': 0.5165624188947248, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 31, 'train_loss': 0.532182690337317, 'val_loss': 0.5229796675948409, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 32, 'train_loss': 0.5241882888021159, 'val_loss': 0.5438304248156848, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 33, 'train_loss': 0.5126904944913289, 'val_loss': 0.5357249148257144, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 34, 'train_loss': 0.5154948581750144, 'val_loss': 0.5319996051960163, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 35, 'train_loss': 0.5319243763833736, 'val_loss': 0.542943456151464, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 36, 'train_loss': 0.5442434395470068, 'val_loss': 0.603189416833826, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 37, 'train_loss': 0.5550712384492593, 'val_loss': 0.5298480472049197, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 38, 'train_loss': 0.5301534241498119, 'val_loss': 0.5261959728893933, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 39, 'train_loss': 0.5264484827499732, 'val_loss': 0.5304491197740709, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 40, 'train_loss': 0.5221066406680277, 'val_loss': 0.5304478138416737, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 41, 'train_loss': 0.5148857766396536, 'val_loss': 0.5403511459762985, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 42, 'train_loss': 0.5106279718220301, 'val_loss': 0.5207610946517807, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 43, 'train_loss': 0.5040700660410152, 'val_loss': 0.5472937231665259, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 44, 'train_loss': 0.5273583774882401, 'val_loss': 0.5221660029780757, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 45, 'train_loss': 0.52698710938763, 'val_loss': 0.5301305066357862, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 46, 'train_loss': 0.5295207938628833, 'val_loss': 0.5351149412962768, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 47, 'train_loss': 0.5072444830211057, 'val_loss': 0.5238983566696579, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 48, 'train_loss': 0.5119893116715514, 'val_loss': 0.5625893189026429, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 49, 'train_loss': 0.5179905881384005, 'val_loss': 0.5096569576778928, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 50, 'train_loss': 0.5077438327733649, 'val_loss': 0.5371062820022171, 'test_acc': 0.8018018018018018}
{'fold': 9, 'epoch': 51, 'train_loss': 0.5241096455909052, 'val_loss': 0.535389874432538, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 52, 'train_loss': 0.5229587956978683, 'val_loss': 0.527681574091181, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 53, 'train_loss': 0.5127092185111426, 'val_loss': 0.5662930462811444, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 54, 'train_loss': 0.5173715620931952, 'val_loss': 0.5375788920634502, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 55, 'train_loss': 0.5110037686715089, 'val_loss': 0.5240767539084494, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 56, 'train_loss': 0.5176609902068822, 'val_loss': 0.5515563552444046, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 57, 'train_loss': 0.5216443719285907, 'val_loss': 0.5309828337248381, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 58, 'train_loss': 0.4989456496923727, 'val_loss': 0.5344876813458966, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 59, 'train_loss': 0.5236148708797865, 'val_loss': 0.5242348060951577, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 60, 'train_loss': 0.5099763981287193, 'val_loss': 0.5759288684741871, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 61, 'train_loss': 0.49513663102346905, 'val_loss': 0.5332942309680285, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 62, 'train_loss': 0.513339178329365, 'val_loss': 0.5331961142050253, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 63, 'train_loss': 0.5071723367496238, 'val_loss': 0.5622392602868982, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 64, 'train_loss': 0.4959247512314322, 'val_loss': 0.5433393942343222, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 65, 'train_loss': 0.4918885362803869, 'val_loss': 0.5427766061043954, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 66, 'train_loss': 0.4971436079503711, 'val_loss': 0.5274550635535438, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 67, 'train_loss': 0.5031538186584391, 'val_loss': 0.5512076025610572, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 68, 'train_loss': 0.5007891707147412, 'val_loss': 0.5445601317259643, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 69, 'train_loss': 0.5070231323424146, 'val_loss': 0.554796992121516, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 70, 'train_loss': 0.4946701970833587, 'val_loss': 0.5483990574742222, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 71, 'train_loss': 0.5053584349543425, 'val_loss': 0.528294022018845, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 72, 'train_loss': 0.5000577837262223, 'val_loss': 0.6268551113369228, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 73, 'train_loss': 0.5163584809779586, 'val_loss': 0.5123726440979554, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 74, 'train_loss': 0.5084231857766474, 'val_loss': 0.5536787015897734, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 75, 'train_loss': 0.5023040417602702, 'val_loss': 0.5447959556235923, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 76, 'train_loss': 0.4967163959068615, 'val_loss': 0.5230533153087169, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 77, 'train_loss': 0.5164319776525401, 'val_loss': 0.5413526242917722, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 78, 'train_loss': 0.5156295838045619, 'val_loss': 0.5254859236983566, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 79, 'train_loss': 0.5223564993251454, 'val_loss': 0.5159381832088437, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 80, 'train_loss': 0.503238273255619, 'val_loss': 0.5737686844559403, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 81, 'train_loss': 0.5026056894950995, 'val_loss': 0.5151713259585269, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 82, 'train_loss': 0.4980262800900623, 'val_loss': 0.5838412035692919, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 83, 'train_loss': 0.5122772982024183, 'val_loss': 0.5225023836702913, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 84, 'train_loss': 0.5045719393241552, 'val_loss': 0.5668215365023226, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 85, 'train_loss': 0.5045400647201923, 'val_loss': 0.492813625851193, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 86, 'train_loss': 0.5076179937882856, 'val_loss': 0.5242963911176802, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 87, 'train_loss': 0.5029612596990284, 'val_loss': 0.5615744032301344, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 88, 'train_loss': 0.5082423082907191, 'val_loss': 0.5320194347484691, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 89, 'train_loss': 0.49802356068667875, 'val_loss': 0.526698688129047, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 90, 'train_loss': 0.4930998880319991, 'val_loss': 0.5718784847775021, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 91, 'train_loss': 0.5056693652931153, 'val_loss': 0.5355841490599487, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 92, 'train_loss': 0.4970708090381323, 'val_loss': 0.5297968752749331, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 93, 'train_loss': 0.5030237860751875, 'val_loss': 0.566372501957524, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 94, 'train_loss': 0.49520914739898825, 'val_loss': 0.5459203290509748, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 95, 'train_loss': 0.5025142686134235, 'val_loss': 0.5741712896673529, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 96, 'train_loss': 0.5075262441779628, 'val_loss': 0.5702276143941793, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 97, 'train_loss': 0.4956825914741499, 'val_loss': 0.5722904720821896, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 98, 'train_loss': 0.48694961748005433, 'val_loss': 0.5308484601544904, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 99, 'train_loss': 0.4878229056678369, 'val_loss': 0.5419496759638056, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 100, 'train_loss': 0.4990279225655544, 'val_loss': 0.586134214658995, 'test_acc': 0.7477477477477478}
Val Loss: 0.5129, Test Accuracy: 0.737 ± 0.036, Duration: 149.130
Best result - 0.737 ± 0.036
--
COLLAB - SortPool
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 1.0262779502868653, 'val_loss': 0.8840948333740234, 'test_acc': 0.554}
{'fold': 9, 'epoch': 2, 'train_loss': 0.8073308610916138, 'val_loss': 1.0871349639892578, 'test_acc': 0.592}
{'fold': 9, 'epoch': 3, 'train_loss': 0.7263370294570923, 'val_loss': 0.6131354370117188, 'test_acc': 0.694}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6230608119964599, 'val_loss': 0.5731312255859375, 'test_acc': 0.724}
{'fold': 9, 'epoch': 5, 'train_loss': 0.5718858139514923, 'val_loss': 0.5358066558837891, 'test_acc': 0.756}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5679786005020142, 'val_loss': 0.4961856002807617, 'test_acc': 0.726}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5625291152000427, 'val_loss': 0.5074056396484375, 'test_acc': 0.748}
{'fold': 9, 'epoch': 8, 'train_loss': 0.5999309248924255, 'val_loss': 0.5258290023803711, 'test_acc': 0.74}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5167651381492615, 'val_loss': 0.5137244110107422, 'test_acc': 0.732}
{'fold': 9, 'epoch': 10, 'train_loss': 0.49104115438461304, 'val_loss': 0.4842590484619141, 'test_acc': 0.77}
{'fold': 9, 'epoch': 11, 'train_loss': 0.4780600643157959, 'val_loss': 0.5727699699401856, 'test_acc': 0.78}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4771783490180969, 'val_loss': 0.4936154098510742, 'test_acc': 0.75}
{'fold': 9, 'epoch': 13, 'train_loss': 0.49721079206466673, 'val_loss': 0.5149004211425782, 'test_acc': 0.73}
{'fold': 9, 'epoch': 14, 'train_loss': 0.6242237355709076, 'val_loss': 0.5684129791259765, 'test_acc': 0.772}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5145333285331726, 'val_loss': 0.5245990524291992, 'test_acc': 0.78}
{'fold': 9, 'epoch': 16, 'train_loss': 0.496790602684021, 'val_loss': 0.5043775100708008, 'test_acc': 0.762}
{'fold': 9, 'epoch': 17, 'train_loss': 0.5106003994941711, 'val_loss': 0.5211492462158203, 'test_acc': 0.768}
{'fold': 9, 'epoch': 18, 'train_loss': 0.4635458407402039, 'val_loss': 0.5182218856811523, 'test_acc': 0.778}
{'fold': 9, 'epoch': 19, 'train_loss': 0.48967688512802127, 'val_loss': 0.5442758331298828, 'test_acc': 0.776}
{'fold': 9, 'epoch': 20, 'train_loss': 0.45551495838165285, 'val_loss': 0.5736453323364258, 'test_acc': 0.756}
{'fold': 9, 'epoch': 21, 'train_loss': 0.4703047623634338, 'val_loss': 0.5654781417846679, 'test_acc': 0.762}
{'fold': 9, 'epoch': 22, 'train_loss': 0.44798566699028014, 'val_loss': 0.5309780349731446, 'test_acc': 0.766}
{'fold': 9, 'epoch': 23, 'train_loss': 0.4246150217056274, 'val_loss': 0.5398856582641601, 'test_acc': 0.764}
{'fold': 9, 'epoch': 24, 'train_loss': 0.41835392582416536, 'val_loss': 0.5864287109375, 'test_acc': 0.764}
{'fold': 9, 'epoch': 25, 'train_loss': 0.43046031188964845, 'val_loss': 0.5782780380249023, 'test_acc': 0.764}
{'fold': 9, 'epoch': 26, 'train_loss': 0.4204060680866241, 'val_loss': 0.5836877136230468, 'test_acc': 0.69}
{'fold': 9, 'epoch': 27, 'train_loss': 0.4285479655265808, 'val_loss': 0.5945552444458008, 'test_acc': 0.742}
{'fold': 9, 'epoch': 28, 'train_loss': 0.4350249786376953, 'val_loss': 0.6281164703369141, 'test_acc': 0.734}
{'fold': 9, 'epoch': 29, 'train_loss': 0.463372456073761, 'val_loss': 0.6515584869384765, 'test_acc': 0.714}
{'fold': 9, 'epoch': 30, 'train_loss': 0.45513697385787966, 'val_loss': 0.6542282104492188, 'test_acc': 0.758}
{'fold': 9, 'epoch': 31, 'train_loss': 0.38449964952468874, 'val_loss': 0.7287255935668945, 'test_acc': 0.772}
{'fold': 9, 'epoch': 32, 'train_loss': 0.38965610337257384, 'val_loss': 0.7900818176269532, 'test_acc': 0.766}
{'fold': 9, 'epoch': 33, 'train_loss': 0.3775105996131897, 'val_loss': 0.8424357986450195, 'test_acc': 0.764}
{'fold': 9, 'epoch': 34, 'train_loss': 0.3861531732082367, 'val_loss': 1.0154710235595703, 'test_acc': 0.774}
{'fold': 9, 'epoch': 35, 'train_loss': 0.38841422426700595, 'val_loss': 0.6613578567504883, 'test_acc': 0.78}
{'fold': 9, 'epoch': 36, 'train_loss': 0.41174159502983093, 'val_loss': 0.6926559371948242, 'test_acc': 0.782}
{'fold': 9, 'epoch': 37, 'train_loss': 0.4213551126718521, 'val_loss': 0.6830694961547852, 'test_acc': 0.762}
{'fold': 9, 'epoch': 38, 'train_loss': 0.42530805873870847, 'val_loss': 0.7271417236328125, 'test_acc': 0.772}
{'fold': 9, 'epoch': 39, 'train_loss': 0.4078943567276001, 'val_loss': 0.7187774810791016, 'test_acc': 0.786}
{'fold': 9, 'epoch': 40, 'train_loss': 0.37714720034599303, 'val_loss': 0.8130408554077149, 'test_acc': 0.762}
{'fold': 9, 'epoch': 41, 'train_loss': 0.366675936460495, 'val_loss': 0.6102990188598633, 'test_acc': 0.784}
{'fold': 9, 'epoch': 42, 'train_loss': 0.37966158127784727, 'val_loss': 0.7136869277954102, 'test_acc': 0.748}
{'fold': 9, 'epoch': 43, 'train_loss': 0.3177929584980011, 'val_loss': 0.7682964248657227, 'test_acc': 0.774}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4679474192857742, 'val_loss': 0.6771234283447266, 'test_acc': 0.77}
{'fold': 9, 'epoch': 45, 'train_loss': 0.4040900797843933, 'val_loss': 0.9340884819030761, 'test_acc': 0.772}
{'fold': 9, 'epoch': 46, 'train_loss': 0.4164166100025177, 'val_loss': 0.5555340423583984, 'test_acc': 0.764}
{'fold': 9, 'epoch': 47, 'train_loss': 0.3765781230926514, 'val_loss': 0.676089714050293, 'test_acc': 0.778}
{'fold': 9, 'epoch': 48, 'train_loss': 0.3826762907505035, 'val_loss': 0.6993650588989258, 'test_acc': 0.76}
{'fold': 9, 'epoch': 49, 'train_loss': 0.38505212497711183, 'val_loss': 0.6124527587890625, 'test_acc': 0.748}
{'fold': 9, 'epoch': 50, 'train_loss': 0.3713350620269775, 'val_loss': 0.8032906112670899, 'test_acc': 0.78}
{'fold': 9, 'epoch': 51, 'train_loss': 0.3541922434568405, 'val_loss': 0.8201412734985352, 'test_acc': 0.77}
{'fold': 9, 'epoch': 52, 'train_loss': 0.33480376362800596, 'val_loss': 0.8912130813598633, 'test_acc': 0.772}
{'fold': 9, 'epoch': 53, 'train_loss': 0.346028648853302, 'val_loss': 0.6630587463378906, 'test_acc': 0.746}
{'fold': 9, 'epoch': 54, 'train_loss': 0.36735128927230837, 'val_loss': 0.7872006988525391, 'test_acc': 0.778}
{'fold': 9, 'epoch': 55, 'train_loss': 0.341632337808609, 'val_loss': 0.6924047698974609, 'test_acc': 0.778}
{'fold': 9, 'epoch': 56, 'train_loss': 0.3144956376552582, 'val_loss': 0.8887503433227539, 'test_acc': 0.774}
{'fold': 9, 'epoch': 57, 'train_loss': 0.29767582619190214, 'val_loss': 0.7671693420410156, 'test_acc': 0.778}
{'fold': 9, 'epoch': 58, 'train_loss': 0.3015985646247864, 'val_loss': 0.8413977203369141, 'test_acc': 0.772}
{'fold': 9, 'epoch': 59, 'train_loss': 0.3092207591533661, 'val_loss': 0.7216152496337891, 'test_acc': 0.732}
{'fold': 9, 'epoch': 60, 'train_loss': 0.36873633313179016, 'val_loss': 0.8680769119262696, 'test_acc': 0.766}
{'fold': 9, 'epoch': 61, 'train_loss': 0.7886273794174194, 'val_loss': 0.9346931686401367, 'test_acc': 0.676}
{'fold': 9, 'epoch': 62, 'train_loss': 0.7919410152435302, 'val_loss': 0.8910357971191406, 'test_acc': 0.65}
{'fold': 9, 'epoch': 63, 'train_loss': 0.8448465003967285, 'val_loss': 1.6336532440185547, 'test_acc': 0.686}
{'fold': 9, 'epoch': 64, 'train_loss': 0.9053168144226075, 'val_loss': 0.7781522979736328, 'test_acc': 0.688}
{'fold': 9, 'epoch': 65, 'train_loss': 2.8063696937561033, 'val_loss': 0.7645379486083984, 'test_acc': 0.64}
{'fold': 9, 'epoch': 66, 'train_loss': 0.915953182220459, 'val_loss': 0.7903240051269531, 'test_acc': 0.632}
{'fold': 9, 'epoch': 67, 'train_loss': 0.9358369331359864, 'val_loss': 1.0329134216308593, 'test_acc': 0.52}
{'fold': 9, 'epoch': 68, 'train_loss': 1.0016254811286927, 'val_loss': 0.9943433532714844, 'test_acc': 0.52}
{'fold': 9, 'epoch': 69, 'train_loss': 0.9948141412734985, 'val_loss': 0.9935889587402343, 'test_acc': 0.52}
{'fold': 9, 'epoch': 70, 'train_loss': 0.9949733285903931, 'val_loss': 0.9936499633789062, 'test_acc': 0.52}
{'fold': 9, 'epoch': 71, 'train_loss': 0.9947706146240234, 'val_loss': 0.993573974609375, 'test_acc': 0.52}
{'fold': 9, 'epoch': 72, 'train_loss': 0.9952552099227905, 'val_loss': 0.9936727294921875, 'test_acc': 0.52}
{'fold': 9, 'epoch': 73, 'train_loss': 0.994865966796875, 'val_loss': 0.9935833129882813, 'test_acc': 0.52}
{'fold': 9, 'epoch': 74, 'train_loss': 0.9946724352836609, 'val_loss': 0.9936273193359375, 'test_acc': 0.52}
{'fold': 9, 'epoch': 75, 'train_loss': 0.9950451474189759, 'val_loss': 0.9935766906738281, 'test_acc': 0.52}
{'fold': 9, 'epoch': 76, 'train_loss': 0.9946496977806091, 'val_loss': 0.9937245788574218, 'test_acc': 0.52}
{'fold': 9, 'epoch': 77, 'train_loss': 0.9952336750030517, 'val_loss': 0.9936517944335937, 'test_acc': 0.52}
{'fold': 9, 'epoch': 78, 'train_loss': 0.9947998304367065, 'val_loss': 0.9935550842285156, 'test_acc': 0.52}
{'fold': 9, 'epoch': 79, 'train_loss': 0.9947683124542236, 'val_loss': 0.9936476440429688, 'test_acc': 0.52}
{'fold': 9, 'epoch': 80, 'train_loss': 0.9945941123962402, 'val_loss': 0.9936004333496093, 'test_acc': 0.52}
{'fold': 9, 'epoch': 81, 'train_loss': 0.99479527759552, 'val_loss': 0.9935502014160156, 'test_acc': 0.52}
{'fold': 9, 'epoch': 82, 'train_loss': 0.994839750289917, 'val_loss': 0.993552734375, 'test_acc': 0.52}
{'fold': 9, 'epoch': 83, 'train_loss': 0.9946844682693482, 'val_loss': 0.9935846862792969, 'test_acc': 0.52}
{'fold': 9, 'epoch': 84, 'train_loss': 0.9946868906021118, 'val_loss': 0.9935540771484375, 'test_acc': 0.52}
{'fold': 9, 'epoch': 85, 'train_loss': 0.9946795053482056, 'val_loss': 0.9935668029785156, 'test_acc': 0.52}
{'fold': 9, 'epoch': 86, 'train_loss': 0.9945931253433228, 'val_loss': 0.9935886535644531, 'test_acc': 0.52}
{'fold': 9, 'epoch': 87, 'train_loss': 0.9950131340026855, 'val_loss': 0.9935662536621094, 'test_acc': 0.52}
{'fold': 9, 'epoch': 88, 'train_loss': 0.9947103266716003, 'val_loss': 0.9935710144042968, 'test_acc': 0.52}
{'fold': 9, 'epoch': 89, 'train_loss': 0.9949015455245972, 'val_loss': 0.9936583862304688, 'test_acc': 0.52}
{'fold': 9, 'epoch': 90, 'train_loss': 0.995686462879181, 'val_loss': 0.9935699768066406, 'test_acc': 0.52}
{'fold': 9, 'epoch': 91, 'train_loss': 0.9948462562561036, 'val_loss': 0.9936072692871094, 'test_acc': 0.52}
{'fold': 9, 'epoch': 92, 'train_loss': 0.9948651351928711, 'val_loss': 0.99355810546875, 'test_acc': 0.52}
{'fold': 9, 'epoch': 93, 'train_loss': 0.99459845495224, 'val_loss': 0.9936305847167969, 'test_acc': 0.52}
{'fold': 9, 'epoch': 94, 'train_loss': 0.9948055863380432, 'val_loss': 0.9935497131347656, 'test_acc': 0.52}
{'fold': 9, 'epoch': 95, 'train_loss': 0.9948826160430908, 'val_loss': 0.9935838012695313, 'test_acc': 0.52}
{'fold': 9, 'epoch': 96, 'train_loss': 0.9948296432495117, 'val_loss': 0.9935809326171875, 'test_acc': 0.52}
{'fold': 9, 'epoch': 97, 'train_loss': 0.9945873174667358, 'val_loss': 0.9935783386230469, 'test_acc': 0.52}
{'fold': 9, 'epoch': 98, 'train_loss': 0.9946823563575745, 'val_loss': 0.9935635070800781, 'test_acc': 0.52}
{'fold': 9, 'epoch': 99, 'train_loss': 0.9946827836036682, 'val_loss': 0.9935532531738281, 'test_acc': 0.52}
{'fold': 9, 'epoch': 100, 'train_loss': 0.9947718753814697, 'val_loss': 0.9935708312988282, 'test_acc': 0.52}
Val Loss: 0.5559, Test Accuracy: 0.733 ± 0.043, Duration: 126.362
Best result - 0.733 ± 0.043
--
COLLAB - ASAP
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.7398838458061219, 'val_loss': 0.5246758270263672, 'test_acc': 0.734}
{'fold': 9, 'epoch': 2, 'train_loss': 0.5572769827842713, 'val_loss': 0.5248349609375, 'test_acc': 0.756}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5175120151042938, 'val_loss': 0.4891613845825195, 'test_acc': 0.778}
{'fold': 9, 'epoch': 4, 'train_loss': 0.491577624797821, 'val_loss': 0.4497041015625, 'test_acc': 0.776}
{'fold': 9, 'epoch': 5, 'train_loss': 0.47830718421936036, 'val_loss': 0.4615985641479492, 'test_acc': 0.788}
{'fold': 9, 'epoch': 6, 'train_loss': 0.4415057237148285, 'val_loss': 0.45439466857910155, 'test_acc': 0.782}
{'fold': 9, 'epoch': 7, 'train_loss': 0.4336597599983215, 'val_loss': 0.4209668884277344, 'test_acc': 0.79}
{'fold': 9, 'epoch': 8, 'train_loss': 0.4088742437362671, 'val_loss': 0.4405408363342285, 'test_acc': 0.794}
{'fold': 9, 'epoch': 9, 'train_loss': 0.3834929974079132, 'val_loss': 0.4590717239379883, 'test_acc': 0.782}
{'fold': 9, 'epoch': 10, 'train_loss': 0.36888173484802245, 'val_loss': 0.4575062942504883, 'test_acc': 0.794}
{'fold': 9, 'epoch': 11, 'train_loss': 0.3455171592235565, 'val_loss': 0.44373689270019534, 'test_acc': 0.8}
{'fold': 9, 'epoch': 12, 'train_loss': 0.3230999960899353, 'val_loss': 0.43506497955322265, 'test_acc': 0.796}
{'fold': 9, 'epoch': 13, 'train_loss': 0.3184180853366852, 'val_loss': 0.4535312614440918, 'test_acc': 0.808}
{'fold': 9, 'epoch': 14, 'train_loss': 0.3029572138786316, 'val_loss': 0.5890430068969726, 'test_acc': 0.768}
{'fold': 9, 'epoch': 15, 'train_loss': 0.29624172043800356, 'val_loss': 0.49991268920898435, 'test_acc': 0.812}
{'fold': 9, 'epoch': 16, 'train_loss': 0.27755250489711764, 'val_loss': 0.45360574340820314, 'test_acc': 0.804}
{'fold': 9, 'epoch': 17, 'train_loss': 0.25713627445697784, 'val_loss': 0.5426661911010742, 'test_acc': 0.798}
{'fold': 9, 'epoch': 18, 'train_loss': 0.25981621783971787, 'val_loss': 0.5150735321044921, 'test_acc': 0.796}
{'fold': 9, 'epoch': 19, 'train_loss': 0.24937246239185334, 'val_loss': 0.5217694549560546, 'test_acc': 0.798}
{'fold': 9, 'epoch': 20, 'train_loss': 0.22907716608047485, 'val_loss': 0.6098038177490235, 'test_acc': 0.792}
{'fold': 9, 'epoch': 21, 'train_loss': 0.21864380264282227, 'val_loss': 0.6554756126403809, 'test_acc': 0.796}
{'fold': 9, 'epoch': 22, 'train_loss': 0.22793498528003692, 'val_loss': 0.5605555572509766, 'test_acc': 0.808}
{'fold': 9, 'epoch': 23, 'train_loss': 0.22045831549167633, 'val_loss': 0.6431002960205078, 'test_acc': 0.8}
{'fold': 9, 'epoch': 24, 'train_loss': 0.21413736486434937, 'val_loss': 0.5352030029296875, 'test_acc': 0.792}
{'fold': 9, 'epoch': 25, 'train_loss': 0.17163374412059784, 'val_loss': 0.7528259315490723, 'test_acc': 0.792}
{'fold': 9, 'epoch': 26, 'train_loss': 0.18309228372573852, 'val_loss': 0.6690862274169922, 'test_acc': 0.814}
{'fold': 9, 'epoch': 27, 'train_loss': 0.18208652555942537, 'val_loss': 0.5335741271972656, 'test_acc': 0.8}
{'fold': 9, 'epoch': 28, 'train_loss': 0.1739211744070053, 'val_loss': 0.7404082489013671, 'test_acc': 0.788}
{'fold': 9, 'epoch': 29, 'train_loss': 0.18038839703798293, 'val_loss': 0.6390409622192382, 'test_acc': 0.802}
{'fold': 9, 'epoch': 30, 'train_loss': 0.18485988163948058, 'val_loss': 0.6829909057617187, 'test_acc': 0.798}
{'fold': 9, 'epoch': 31, 'train_loss': 0.19304669946432113, 'val_loss': 0.6428836517333985, 'test_acc': 0.82}
{'fold': 9, 'epoch': 32, 'train_loss': 0.13588685536384582, 'val_loss': 0.7370132675170898, 'test_acc': 0.812}
{'fold': 9, 'epoch': 33, 'train_loss': 0.12887231063842775, 'val_loss': 0.7318411560058594, 'test_acc': 0.798}
{'fold': 9, 'epoch': 34, 'train_loss': 0.1309831705093384, 'val_loss': 0.7233470916748047, 'test_acc': 0.81}
{'fold': 9, 'epoch': 35, 'train_loss': 0.12585663479566575, 'val_loss': 0.8212938537597656, 'test_acc': 0.806}
{'fold': 9, 'epoch': 36, 'train_loss': 0.1211318756826222, 'val_loss': 0.8561990051269531, 'test_acc': 0.824}
{'fold': 9, 'epoch': 37, 'train_loss': 0.12125269210338592, 'val_loss': 0.8887489318847657, 'test_acc': 0.804}
{'fold': 9, 'epoch': 38, 'train_loss': 0.1077601499557495, 'val_loss': 0.9319192657470703, 'test_acc': 0.81}
{'fold': 9, 'epoch': 39, 'train_loss': 0.09753453290462494, 'val_loss': 1.1485477294921875, 'test_acc': 0.802}
{'fold': 9, 'epoch': 40, 'train_loss': 0.1213448748588562, 'val_loss': 1.0331261520385742, 'test_acc': 0.814}
{'fold': 9, 'epoch': 41, 'train_loss': 0.11245063829421997, 'val_loss': 1.0076074066162108, 'test_acc': 0.814}
{'fold': 9, 'epoch': 42, 'train_loss': 0.10528541910648347, 'val_loss': 0.9625740051269531, 'test_acc': 0.812}
{'fold': 9, 'epoch': 43, 'train_loss': 0.10248265624046325, 'val_loss': 1.07225244140625, 'test_acc': 0.798}
{'fold': 9, 'epoch': 44, 'train_loss': 0.16757044059038162, 'val_loss': 0.7422319488525391, 'test_acc': 0.81}
{'fold': 9, 'epoch': 45, 'train_loss': 0.11277854567766189, 'val_loss': 1.1453080902099608, 'test_acc': 0.808}
{'fold': 9, 'epoch': 46, 'train_loss': 0.10140827000141144, 'val_loss': 1.0535533447265626, 'test_acc': 0.82}
{'fold': 9, 'epoch': 47, 'train_loss': 0.10215933442115784, 'val_loss': 1.0282437744140625, 'test_acc': 0.81}
{'fold': 9, 'epoch': 48, 'train_loss': 0.09497079664468765, 'val_loss': 1.1135836334228515, 'test_acc': 0.822}
{'fold': 9, 'epoch': 49, 'train_loss': 0.08560066086053848, 'val_loss': 1.1719097595214845, 'test_acc': 0.82}
{'fold': 9, 'epoch': 50, 'train_loss': 0.07279341191332787, 'val_loss': 1.3491216583251953, 'test_acc': 0.808}
{'fold': 9, 'epoch': 51, 'train_loss': 0.07792805260419845, 'val_loss': 1.4369552001953125, 'test_acc': 0.812}
{'fold': 9, 'epoch': 52, 'train_loss': 0.12320491474866867, 'val_loss': 0.9458569869995117, 'test_acc': 0.812}
{'fold': 9, 'epoch': 53, 'train_loss': 0.11496233803033828, 'val_loss': 1.0483478240966797, 'test_acc': 0.806}
{'fold': 9, 'epoch': 54, 'train_loss': 0.08694423711299896, 'val_loss': 1.138279815673828, 'test_acc': 0.8}
{'fold': 9, 'epoch': 55, 'train_loss': 0.07587421128153801, 'val_loss': 1.224127487182617, 'test_acc': 0.82}
{'fold': 9, 'epoch': 56, 'train_loss': 0.07078631542623043, 'val_loss': 1.4847532653808593, 'test_acc': 0.808}
{'fold': 9, 'epoch': 57, 'train_loss': 0.09334316998720169, 'val_loss': 1.1780584411621093, 'test_acc': 0.8}
{'fold': 9, 'epoch': 58, 'train_loss': 0.11041263318061828, 'val_loss': 1.116209991455078, 'test_acc': 0.804}
{'fold': 9, 'epoch': 59, 'train_loss': 0.11256867533922195, 'val_loss': 1.049807632446289, 'test_acc': 0.81}
{'fold': 9, 'epoch': 60, 'train_loss': 0.09416828960180283, 'val_loss': 1.1190409698486328, 'test_acc': 0.82}
{'fold': 9, 'epoch': 61, 'train_loss': 0.0835453017950058, 'val_loss': 1.2123937683105468, 'test_acc': 0.796}
{'fold': 9, 'epoch': 62, 'train_loss': 0.07223637020587921, 'val_loss': 1.4458702468872071, 'test_acc': 0.804}
{'fold': 9, 'epoch': 63, 'train_loss': 0.06811614188551902, 'val_loss': 1.4625282440185547, 'test_acc': 0.816}
{'fold': 9, 'epoch': 64, 'train_loss': 0.07067223197221756, 'val_loss': 1.5530812377929688, 'test_acc': 0.798}
{'fold': 9, 'epoch': 65, 'train_loss': 0.08298020398616791, 'val_loss': 1.4001088256835938, 'test_acc': 0.8}
{'fold': 9, 'epoch': 66, 'train_loss': 0.07974754655361176, 'val_loss': 1.3433963165283203, 'test_acc': 0.804}
{'fold': 9, 'epoch': 67, 'train_loss': 0.18231494510173799, 'val_loss': 1.072858139038086, 'test_acc': 0.808}
{'fold': 9, 'epoch': 68, 'train_loss': 0.1222089859843254, 'val_loss': 1.2128607025146485, 'test_acc': 0.818}
{'fold': 9, 'epoch': 69, 'train_loss': 0.12248574876785279, 'val_loss': 1.1223011474609375, 'test_acc': 0.804}
{'fold': 9, 'epoch': 70, 'train_loss': 0.11518760696053505, 'val_loss': 1.1573328857421874, 'test_acc': 0.798}
{'fold': 9, 'epoch': 71, 'train_loss': 0.08978695333003998, 'val_loss': 1.3015760345458984, 'test_acc': 0.8}
{'fold': 9, 'epoch': 72, 'train_loss': 0.08210519206523895, 'val_loss': 1.3952974090576171, 'test_acc': 0.814}
{'fold': 9, 'epoch': 73, 'train_loss': 0.084514857172966, 'val_loss': 1.50351416015625, 'test_acc': 0.81}
{'fold': 9, 'epoch': 74, 'train_loss': 0.09320974153280258, 'val_loss': 1.1806922149658203, 'test_acc': 0.812}
{'fold': 9, 'epoch': 75, 'train_loss': 0.06603059267997742, 'val_loss': 1.4032608489990235, 'test_acc': 0.82}
{'fold': 9, 'epoch': 76, 'train_loss': 0.05714647729694843, 'val_loss': 1.5846443939208985, 'test_acc': 0.828}
{'fold': 9, 'epoch': 77, 'train_loss': 0.058785833835601806, 'val_loss': 1.8069795837402345, 'test_acc': 0.822}
{'fold': 9, 'epoch': 78, 'train_loss': 0.06033835244178772, 'val_loss': 1.6917595825195313, 'test_acc': 0.816}
{'fold': 9, 'epoch': 79, 'train_loss': 0.056775809559971094, 'val_loss': 1.8125149383544923, 'test_acc': 0.808}
{'fold': 9, 'epoch': 80, 'train_loss': 0.0572633039355278, 'val_loss': 1.7785284729003907, 'test_acc': 0.81}
{'fold': 9, 'epoch': 81, 'train_loss': 0.05337795308232308, 'val_loss': 1.9160187072753907, 'test_acc': 0.818}
{'fold': 9, 'epoch': 82, 'train_loss': 0.06060495735704899, 'val_loss': 1.9174457702636718, 'test_acc': 0.816}
{'fold': 9, 'epoch': 83, 'train_loss': 0.06677723062038422, 'val_loss': 1.6995747375488282, 'test_acc': 0.832}
{'fold': 9, 'epoch': 84, 'train_loss': 0.06159050187468529, 'val_loss': 1.8282429351806642, 'test_acc': 0.824}
{'fold': 9, 'epoch': 85, 'train_loss': 0.0568005550801754, 'val_loss': 1.8165313720703125, 'test_acc': 0.808}
{'fold': 9, 'epoch': 86, 'train_loss': 0.05273776561021805, 'val_loss': 1.9981927642822266, 'test_acc': 0.828}
{'fold': 9, 'epoch': 87, 'train_loss': 0.05283937256038189, 'val_loss': 2.072513671875, 'test_acc': 0.816}
{'fold': 9, 'epoch': 88, 'train_loss': 0.05916533945035189, 'val_loss': 1.9540757904052735, 'test_acc': 0.818}
{'fold': 9, 'epoch': 89, 'train_loss': 0.06499573700129986, 'val_loss': 2.023202941894531, 'test_acc': 0.816}
{'fold': 9, 'epoch': 90, 'train_loss': 0.07979116922616959, 'val_loss': 1.7684671325683594, 'test_acc': 0.804}
{'fold': 9, 'epoch': 91, 'train_loss': 0.1533984296321869, 'val_loss': 1.1653321990966796, 'test_acc': 0.808}
{'fold': 9, 'epoch': 92, 'train_loss': 0.10640263479948044, 'val_loss': 1.1303933563232422, 'test_acc': 0.804}
{'fold': 9, 'epoch': 93, 'train_loss': 0.07435068279504777, 'val_loss': 1.2950240020751953, 'test_acc': 0.808}
{'fold': 9, 'epoch': 94, 'train_loss': 0.07783974316716194, 'val_loss': 1.4440851593017578, 'test_acc': 0.816}
{'fold': 9, 'epoch': 95, 'train_loss': 0.06463273549079895, 'val_loss': 1.398943389892578, 'test_acc': 0.812}
{'fold': 9, 'epoch': 96, 'train_loss': 0.08392792055010796, 'val_loss': 1.3082635803222655, 'test_acc': 0.812}
{'fold': 9, 'epoch': 97, 'train_loss': 0.07321599748730659, 'val_loss': 1.425591293334961, 'test_acc': 0.802}
{'fold': 9, 'epoch': 98, 'train_loss': 0.06998432171344757, 'val_loss': 1.2856568908691406, 'test_acc': 0.82}
{'fold': 9, 'epoch': 99, 'train_loss': 0.06347906562685966, 'val_loss': 1.4854250183105469, 'test_acc': 0.818}
{'fold': 9, 'epoch': 100, 'train_loss': 0.055001143842935564, 'val_loss': 1.5379808044433594, 'test_acc': 0.816}
Val Loss: 0.4575, Test Accuracy: 0.795 ± 0.020, Duration: 496.991
Best result - 0.795 ± 0.020
--
DD - SortPool: 0.587 ± 0.003
DD - ASAP: 0.754 ± 0.040
NCI1 - SortPool: 0.673 ± 0.047
NCI1 - ASAP: 0.745 ± 0.025
PROTEINS - SortPool: 0.731 ± 0.040
PROTEINS - ASAP: 0.737 ± 0.036
COLLAB - SortPool: 0.733 ± 0.043
COLLAB - ASAP: 0.795 ± 0.020

Process finished with exit code 0
