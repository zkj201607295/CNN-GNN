D:\Program\Anaconda\envs\pyg\python.exe F:/Project/BernNet/GraphClassification/main.py
--
IMDB-MULTI - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 1.118355745077133, 'val_loss': 1.0406972757975261, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 2, 'train_loss': 1.0376088112592696, 'val_loss': 1.0087836074829102, 'test_acc': 0.54}
{'fold': 9, 'epoch': 3, 'train_loss': 1.0376178741455078, 'val_loss': 0.9726015853881836, 'test_acc': 0.58}
{'fold': 9, 'epoch': 4, 'train_loss': 0.9362928152084351, 'val_loss': 0.9475351079305013, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 5, 'train_loss': 0.9044378310441971, 'val_loss': 0.9352698008219401, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 6, 'train_loss': 0.8785378932952881, 'val_loss': 0.9352211380004882, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 7, 'train_loss': 0.84479461312294, 'val_loss': 0.913333002726237, 'test_acc': 0.56}
{'fold': 9, 'epoch': 8, 'train_loss': 0.869506710767746, 'val_loss': 0.9129651133219401, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 9, 'train_loss': 0.8412544846534729, 'val_loss': 0.8835492960611979, 'test_acc': 0.6}
{'fold': 9, 'epoch': 10, 'train_loss': 0.8083220183849334, 'val_loss': 0.891332499186198, 'test_acc': 0.58}
{'fold': 9, 'epoch': 11, 'train_loss': 0.8284628927707672, 'val_loss': 0.9001621754964193, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 12, 'train_loss': 0.7994510680437088, 'val_loss': 0.8823974736531576, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 13, 'train_loss': 0.8029146283864975, 'val_loss': 0.8821073404947917, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 14, 'train_loss': 0.791173878312111, 'val_loss': 0.8831068929036459, 'test_acc': 0.6}
{'fold': 9, 'epoch': 15, 'train_loss': 0.8022730976343155, 'val_loss': 0.8859134038289388, 'test_acc': 0.58}
{'fold': 9, 'epoch': 16, 'train_loss': 0.7948985159397125, 'val_loss': 0.8599586359659831, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 17, 'train_loss': 0.7746809005737305, 'val_loss': 0.8713666915893554, 'test_acc': 0.6}
{'fold': 9, 'epoch': 18, 'train_loss': 0.7737642467021942, 'val_loss': 0.8715582402547201, 'test_acc': 0.58}
{'fold': 9, 'epoch': 19, 'train_loss': 0.7852589040994644, 'val_loss': 0.871685307820638, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 20, 'train_loss': 0.7769966334104538, 'val_loss': 0.8865164693196614, 'test_acc': 0.6}
{'fold': 9, 'epoch': 21, 'train_loss': 0.785198113322258, 'val_loss': 0.8828701655069987, 'test_acc': 0.6}
{'fold': 9, 'epoch': 22, 'train_loss': 0.7743954092264176, 'val_loss': 0.8791140492757161, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 23, 'train_loss': 0.7854067206382751, 'val_loss': 0.8914190419514973, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 24, 'train_loss': 0.766324308514595, 'val_loss': 0.8789298375447591, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 25, 'train_loss': 0.7715938538312912, 'val_loss': 0.876382916768392, 'test_acc': 0.6}
{'fold': 9, 'epoch': 26, 'train_loss': 0.7580866247415543, 'val_loss': 0.8989249038696289, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 27, 'train_loss': 0.765567809343338, 'val_loss': 0.9075477981567383, 'test_acc': 0.58}
{'fold': 9, 'epoch': 28, 'train_loss': 0.7604588538408279, 'val_loss': 0.8848391850789388, 'test_acc': 0.58}
{'fold': 9, 'epoch': 29, 'train_loss': 0.7642391741275787, 'val_loss': 0.89068177541097, 'test_acc': 0.58}
{'fold': 9, 'epoch': 30, 'train_loss': 0.7679925054311753, 'val_loss': 0.8912264760335287, 'test_acc': 0.58}
{'fold': 9, 'epoch': 31, 'train_loss': 0.7687356889247894, 'val_loss': 0.9017203776041667, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 32, 'train_loss': 0.7552062630653381, 'val_loss': 0.9183209864298503, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 33, 'train_loss': 0.768571537733078, 'val_loss': 0.8871836598714192, 'test_acc': 0.58}
{'fold': 9, 'epoch': 34, 'train_loss': 0.7597247213125229, 'val_loss': 0.8999680582682291, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 35, 'train_loss': 0.7597006171941757, 'val_loss': 0.8852279790242513, 'test_acc': 0.58}
{'fold': 9, 'epoch': 36, 'train_loss': 0.7502323091030121, 'val_loss': 0.9281752395629883, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 37, 'train_loss': 0.7596903860569, 'val_loss': 0.9077682622273763, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 38, 'train_loss': 0.7404689133167267, 'val_loss': 0.9072498957316081, 'test_acc': 0.58}
{'fold': 9, 'epoch': 39, 'train_loss': 0.759611377120018, 'val_loss': 0.8995791625976562, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 40, 'train_loss': 0.7501291662454606, 'val_loss': 0.9066953531901042, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 41, 'train_loss': 0.7559598296880722, 'val_loss': 0.9101516723632812, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 42, 'train_loss': 0.7492132544517517, 'val_loss': 0.8992089080810547, 'test_acc': 0.58}
{'fold': 9, 'epoch': 43, 'train_loss': 0.7514555633068085, 'val_loss': 0.9005241648356119, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 44, 'train_loss': 0.7466543316841125, 'val_loss': 0.9195565032958984, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 45, 'train_loss': 0.7565714985132217, 'val_loss': 0.8824661000569661, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 46, 'train_loss': 0.7551100552082062, 'val_loss': 0.9317135111490885, 'test_acc': 0.6066666666666667}
{'fold': 9, 'epoch': 47, 'train_loss': 0.7601989150047302, 'val_loss': 0.9559247716267903, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 48, 'train_loss': 0.7471483677625657, 'val_loss': 0.969064458211263, 'test_acc': 0.58}
{'fold': 9, 'epoch': 49, 'train_loss': 0.7590539336204529, 'val_loss': 0.946655985514323, 'test_acc': 0.56}
{'fold': 9, 'epoch': 50, 'train_loss': 0.734996497631073, 'val_loss': 0.9709121576944987, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 51, 'train_loss': 0.7624746084213256, 'val_loss': 0.9538470967610677, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 52, 'train_loss': 0.7474607944488525, 'val_loss': 0.9576735560099284, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 53, 'train_loss': 0.7650024890899658, 'val_loss': 0.9099809900919597, 'test_acc': 0.58}
{'fold': 9, 'epoch': 54, 'train_loss': 0.7537230342626572, 'val_loss': 0.9134681828816732, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 55, 'train_loss': 0.7375458717346192, 'val_loss': 0.9159025446573893, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 56, 'train_loss': 0.7528901129961014, 'val_loss': 0.9284573364257812, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 57, 'train_loss': 0.747189724445343, 'val_loss': 0.9264518102010091, 'test_acc': 0.58}
{'fold': 9, 'epoch': 58, 'train_loss': 0.7408978551626205, 'val_loss': 0.9386959584554037, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 59, 'train_loss': 0.7399683892726898, 'val_loss': 0.9260665766398112, 'test_acc': 0.58}
{'fold': 9, 'epoch': 60, 'train_loss': 0.7376062363386154, 'val_loss': 0.9272639973958333, 'test_acc': 0.58}
{'fold': 9, 'epoch': 61, 'train_loss': 0.7443055003881455, 'val_loss': 0.9459308497111003, 'test_acc': 0.5933333333333334}
{'fold': 9, 'epoch': 62, 'train_loss': 0.7403582066297532, 'val_loss': 0.9571294657389323, 'test_acc': 0.58}
{'fold': 9, 'epoch': 63, 'train_loss': 0.7435350865125656, 'val_loss': 0.9720131556193033, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 64, 'train_loss': 0.735909378528595, 'val_loss': 1.0324632517496746, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 65, 'train_loss': 0.7347489118576049, 'val_loss': 1.0253763834635417, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 66, 'train_loss': 0.7473684221506118, 'val_loss': 1.0000205357869467, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 67, 'train_loss': 0.7450287163257598, 'val_loss': 0.9469906234741211, 'test_acc': 0.58}
{'fold': 9, 'epoch': 68, 'train_loss': 0.7420920580625534, 'val_loss': 0.9418453852335612, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 69, 'train_loss': 0.7556999802589417, 'val_loss': 0.9231805928548177, 'test_acc': 0.58}
{'fold': 9, 'epoch': 70, 'train_loss': 0.743623036146164, 'val_loss': 0.9342354838053385, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 71, 'train_loss': 0.7431675583124161, 'val_loss': 0.9090205383300781, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 72, 'train_loss': 0.7406189560890197, 'val_loss': 0.9572389984130859, 'test_acc': 0.58}
{'fold': 9, 'epoch': 73, 'train_loss': 0.7497159838676453, 'val_loss': 0.9201952870686849, 'test_acc': 0.58}
{'fold': 9, 'epoch': 74, 'train_loss': 0.746988394856453, 'val_loss': 0.9381798299153646, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 75, 'train_loss': 0.7407958000898361, 'val_loss': 0.9771038055419922, 'test_acc': 0.58}
{'fold': 9, 'epoch': 76, 'train_loss': 0.747511026263237, 'val_loss': 1.0004180018107096, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 77, 'train_loss': 0.739462423324585, 'val_loss': 0.9748478062947591, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 78, 'train_loss': 0.7370573550462722, 'val_loss': 0.9143167877197266, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 79, 'train_loss': 0.7445578217506409, 'val_loss': 0.9063495000203451, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 80, 'train_loss': 0.7486003875732422, 'val_loss': 0.940624148050944, 'test_acc': 0.56}
{'fold': 9, 'epoch': 81, 'train_loss': 0.740191838145256, 'val_loss': 0.9100496292114257, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 82, 'train_loss': 0.7335135757923126, 'val_loss': 0.8969795227050781, 'test_acc': 0.58}
{'fold': 9, 'epoch': 83, 'train_loss': 0.743282949924469, 'val_loss': 0.9465842183430989, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 84, 'train_loss': 0.7399421453475952, 'val_loss': 0.9476399612426758, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 85, 'train_loss': 0.7357806354761124, 'val_loss': 0.9428145980834961, 'test_acc': 0.58}
{'fold': 9, 'epoch': 86, 'train_loss': 0.7383644640445709, 'val_loss': 0.9247245534261068, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 87, 'train_loss': 0.7373015522956848, 'val_loss': 0.9467363993326823, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 88, 'train_loss': 0.7363677769899368, 'val_loss': 0.9157762654622396, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 89, 'train_loss': 0.7442712545394897, 'val_loss': 0.8957287216186524, 'test_acc': 0.56}
{'fold': 9, 'epoch': 90, 'train_loss': 0.7482727468013763, 'val_loss': 0.9453563690185547, 'test_acc': 0.56}
{'fold': 9, 'epoch': 91, 'train_loss': 0.7418657600879669, 'val_loss': 0.8858685302734375, 'test_acc': 0.58}
{'fold': 9, 'epoch': 92, 'train_loss': 0.7339929968118668, 'val_loss': 0.9033379745483399, 'test_acc': 0.56}
{'fold': 9, 'epoch': 93, 'train_loss': 0.733163344860077, 'val_loss': 0.9290166982014973, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 94, 'train_loss': 0.7536968380212784, 'val_loss': 0.9407643763224284, 'test_acc': 0.5866666666666667}
{'fold': 9, 'epoch': 95, 'train_loss': 0.7483019560575486, 'val_loss': 0.9136510340372721, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 96, 'train_loss': 0.7421725213527679, 'val_loss': 0.9041899236043295, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 97, 'train_loss': 0.7428002774715423, 'val_loss': 0.9315994771321615, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 98, 'train_loss': 0.7423625230789185, 'val_loss': 0.9192225392659505, 'test_acc': 0.58}
{'fold': 9, 'epoch': 99, 'train_loss': 0.7356853276491165, 'val_loss': 0.9045017496744792, 'test_acc': 0.58}
{'fold': 9, 'epoch': 100, 'train_loss': 0.7374124705791474, 'val_loss': 0.9170376714070638, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 101, 'train_loss': 0.722999119758606, 'val_loss': 0.9710576883951823, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 102, 'train_loss': 0.7521141827106476, 'val_loss': 0.9954836400349935, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 103, 'train_loss': 0.7524624288082122, 'val_loss': 0.9614594777425131, 'test_acc': 0.54}
{'fold': 9, 'epoch': 104, 'train_loss': 0.7428581207990647, 'val_loss': 0.956219711303711, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 105, 'train_loss': 0.7371682494878768, 'val_loss': 0.9449661636352539, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 106, 'train_loss': 0.7315413355827332, 'val_loss': 0.9172392400105794, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 107, 'train_loss': 0.7389868915081024, 'val_loss': 0.9340554300944011, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 108, 'train_loss': 0.7377480834722518, 'val_loss': 0.9118448003133138, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 109, 'train_loss': 0.7358597576618194, 'val_loss': 0.9217925135294597, 'test_acc': 0.56}
{'fold': 9, 'epoch': 110, 'train_loss': 0.7313013911247254, 'val_loss': 0.9354408009847005, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 111, 'train_loss': 0.7396570712327957, 'val_loss': 0.9401810328165691, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 112, 'train_loss': 0.7307064443826675, 'val_loss': 0.9676390075683594, 'test_acc': 0.54}
{'fold': 9, 'epoch': 113, 'train_loss': 0.7362368673086166, 'val_loss': 0.9671570714314779, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 114, 'train_loss': 0.7334360659122467, 'val_loss': 0.950938606262207, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 115, 'train_loss': 0.7278363466262817, 'val_loss': 0.9332981236775716, 'test_acc': 0.56}
{'fold': 9, 'epoch': 116, 'train_loss': 0.7244476020336151, 'val_loss': 0.9644280242919921, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 117, 'train_loss': 0.7293863207101822, 'val_loss': 0.9998225911458334, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 118, 'train_loss': 0.7273445963859558, 'val_loss': 1.0260753377278646, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 119, 'train_loss': 0.7336325883865357, 'val_loss': 1.019563496907552, 'test_acc': 0.56}
{'fold': 9, 'epoch': 120, 'train_loss': 0.7396989911794662, 'val_loss': 0.9881554031372071, 'test_acc': 0.56}
{'fold': 9, 'epoch': 121, 'train_loss': 0.7349718928337097, 'val_loss': 0.9908029429117838, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 122, 'train_loss': 0.7411317676305771, 'val_loss': 0.972488530476888, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 123, 'train_loss': 0.739667996764183, 'val_loss': 0.997125015258789, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 124, 'train_loss': 0.7306714296340943, 'val_loss': 0.9795171483357747, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 125, 'train_loss': 0.7385488271713256, 'val_loss': 1.0095328013102214, 'test_acc': 0.56}
{'fold': 9, 'epoch': 126, 'train_loss': 0.7304428339004516, 'val_loss': 0.9613164520263672, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 127, 'train_loss': 0.7327818214893341, 'val_loss': 0.9676844024658203, 'test_acc': 0.5733333333333334}
{'fold': 9, 'epoch': 128, 'train_loss': 0.7282340943813324, 'val_loss': 0.995506960550944, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 129, 'train_loss': 0.7269673198461533, 'val_loss': 1.0014005025227866, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 130, 'train_loss': 0.7263839602470398, 'val_loss': 1.0010226694742839, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 131, 'train_loss': 0.7363525301218032, 'val_loss': 0.9873502349853516, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 132, 'train_loss': 0.7309769570827485, 'val_loss': 0.9717379252115885, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 133, 'train_loss': 0.7352408558130265, 'val_loss': 0.9489145278930664, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 134, 'train_loss': 0.7330926597118378, 'val_loss': 0.8929342778523763, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 135, 'train_loss': 0.7311943352222443, 'val_loss': 0.8873667526245117, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 136, 'train_loss': 0.7284553527832032, 'val_loss': 0.8967992146809896, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 137, 'train_loss': 0.7257027745246887, 'val_loss': 0.9149424997965495, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 138, 'train_loss': 0.7308057308197021, 'val_loss': 0.8669013595581054, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 139, 'train_loss': 0.7361630320549011, 'val_loss': 0.9145640182495117, 'test_acc': 0.56}
{'fold': 9, 'epoch': 140, 'train_loss': 0.7360636383295059, 'val_loss': 0.8798496119181315, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 141, 'train_loss': 0.7399996429681778, 'val_loss': 0.895382334391276, 'test_acc': 0.56}
{'fold': 9, 'epoch': 142, 'train_loss': 0.7239774852991104, 'val_loss': 0.9157352066040039, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 143, 'train_loss': 0.728868481516838, 'val_loss': 0.9208500671386719, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 144, 'train_loss': 0.7265785992145538, 'val_loss': 0.9131444040934245, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 145, 'train_loss': 0.7293308943510055, 'val_loss': 0.9365022150675456, 'test_acc': 0.56}
{'fold': 9, 'epoch': 146, 'train_loss': 0.7206907510757447, 'val_loss': 0.9731650797526041, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 147, 'train_loss': 0.7288575500249863, 'val_loss': 0.9659168752034505, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 148, 'train_loss': 0.7445371389389038, 'val_loss': 0.9111654408772787, 'test_acc': 0.54}
{'fold': 9, 'epoch': 149, 'train_loss': 0.7461449027061462, 'val_loss': 0.954437967936198, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 150, 'train_loss': 0.7307118445634841, 'val_loss': 0.9545953114827473, 'test_acc': 0.54}
{'fold': 9, 'epoch': 151, 'train_loss': 0.7494192987680435, 'val_loss': 0.9208700688680013, 'test_acc': 0.56}
{'fold': 9, 'epoch': 152, 'train_loss': 0.728642252087593, 'val_loss': 0.9418319829305013, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 153, 'train_loss': 0.7313819020986557, 'val_loss': 0.9574651845296224, 'test_acc': 0.56}
{'fold': 9, 'epoch': 154, 'train_loss': 0.744198027253151, 'val_loss': 0.9849942525227865, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 155, 'train_loss': 0.7297737389802933, 'val_loss': 1.0299365997314454, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 156, 'train_loss': 0.736556476354599, 'val_loss': 0.9572833887736003, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 157, 'train_loss': 0.7266305655241012, 'val_loss': 0.9512983703613281, 'test_acc': 0.54}
{'fold': 9, 'epoch': 158, 'train_loss': 0.7359420210123062, 'val_loss': 0.9506496302286784, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 159, 'train_loss': 0.7370379865169525, 'val_loss': 0.9606536610921224, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 160, 'train_loss': 0.7287129193544388, 'val_loss': 0.9968027750651042, 'test_acc': 0.54}
{'fold': 9, 'epoch': 161, 'train_loss': 0.7191471546888352, 'val_loss': 1.0304314931233725, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 162, 'train_loss': 0.7323939770460128, 'val_loss': 1.0192461522420246, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 163, 'train_loss': 0.7324480980634689, 'val_loss': 1.0597746022542318, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 164, 'train_loss': 0.7238280266523361, 'val_loss': 1.0509160868326823, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 165, 'train_loss': 0.7324221700429916, 'val_loss': 1.0115209706624348, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 166, 'train_loss': 0.7601396054029464, 'val_loss': 0.9848218154907227, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 167, 'train_loss': 0.7309385538101196, 'val_loss': 0.9667289352416992, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 168, 'train_loss': 0.7348954945802688, 'val_loss': 1.0063755544026693, 'test_acc': 0.54}
{'fold': 9, 'epoch': 169, 'train_loss': 0.72934131026268, 'val_loss': 1.0044525655110677, 'test_acc': 0.54}
{'fold': 9, 'epoch': 170, 'train_loss': 0.736757117509842, 'val_loss': 0.9898317972819011, 'test_acc': 0.54}
{'fold': 9, 'epoch': 171, 'train_loss': 0.7283623158931732, 'val_loss': 1.0180597432454428, 'test_acc': 0.54}
{'fold': 9, 'epoch': 172, 'train_loss': 0.7304567605257034, 'val_loss': 1.0579665756225587, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 173, 'train_loss': 0.7227848798036576, 'val_loss': 1.0423963165283203, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 174, 'train_loss': 0.7299243450164795, 'val_loss': 1.0290420786539713, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 175, 'train_loss': 0.7230754882097244, 'val_loss': 1.0428511555989584, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 176, 'train_loss': 0.7229281187057495, 'val_loss': 1.0691989771525066, 'test_acc': 0.5666666666666667}
{'fold': 9, 'epoch': 177, 'train_loss': 0.7283502101898194, 'val_loss': 1.0675729242960612, 'test_acc': 0.56}
{'fold': 9, 'epoch': 178, 'train_loss': 0.7349127918481827, 'val_loss': 1.1124332427978516, 'test_acc': 0.52}
{'fold': 9, 'epoch': 179, 'train_loss': 0.7410648494958878, 'val_loss': 1.0079203542073567, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 180, 'train_loss': 0.7289585679769516, 'val_loss': 1.024819081624349, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 181, 'train_loss': 0.7320715367794037, 'val_loss': 1.054540252685547, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 182, 'train_loss': 0.7407754868268966, 'val_loss': 1.065975824991862, 'test_acc': 0.54}
{'fold': 9, 'epoch': 183, 'train_loss': 0.7349625498056411, 'val_loss': 1.0782578659057618, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 184, 'train_loss': 0.731325876712799, 'val_loss': 1.0189850362141928, 'test_acc': 0.54}
{'fold': 9, 'epoch': 185, 'train_loss': 0.7224988669157029, 'val_loss': 1.0353567632039389, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 186, 'train_loss': 0.7225622862577439, 'val_loss': 1.008373006184896, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 187, 'train_loss': 0.72701356112957, 'val_loss': 1.0130773289998372, 'test_acc': 0.54}
{'fold': 9, 'epoch': 188, 'train_loss': 0.7281269788742065, 'val_loss': 1.0310658009847005, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 189, 'train_loss': 0.7222478300333023, 'val_loss': 1.0397191619873047, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 190, 'train_loss': 0.7337880969047547, 'val_loss': 1.0025530751546223, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 191, 'train_loss': 0.7213446170091629, 'val_loss': 1.0477891794840495, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 192, 'train_loss': 0.7397381603717804, 'val_loss': 1.0407350667317707, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 193, 'train_loss': 0.7389864444732666, 'val_loss': 1.0110155232747395, 'test_acc': 0.5466666666666666}
{'fold': 9, 'epoch': 194, 'train_loss': 0.7285750091075898, 'val_loss': 0.9925848134358723, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 195, 'train_loss': 0.7266673922538758, 'val_loss': 0.9776698303222656, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 196, 'train_loss': 0.7248129546642303, 'val_loss': 0.9876839447021485, 'test_acc': 0.5266666666666666}
{'fold': 9, 'epoch': 197, 'train_loss': 0.7221645563840866, 'val_loss': 0.9881665293375651, 'test_acc': 0.5333333333333333}
{'fold': 9, 'epoch': 198, 'train_loss': 0.7253655850887298, 'val_loss': 0.9783556111653646, 'test_acc': 0.5533333333333333}
{'fold': 9, 'epoch': 199, 'train_loss': 0.7243697881698609, 'val_loss': 0.9868771616617839, 'test_acc': 0.54}
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
{'fold': 9, 'epoch': 200, 'train_loss': 0.7192997604608535, 'val_loss': 0.9906668599446614, 'test_acc': 0.54}
Val Loss: 0.8888, Test Accuracy: 0.579 ± 0.039, Duration: 110.450
Best result - 0.579 ± 0.039
--
MUTAG - Classifier
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6729865827058491, 'val_loss': 0.5397289594014486, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6460289374778145, 'val_loss': 0.4815536075168186, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5680565269369828, 'val_loss': 0.42084715101453996, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6177467484223215, 'val_loss': 0.36910827954610187, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 5, 'train_loss': 0.49130325725204066, 'val_loss': 0.31650908788045246, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 6, 'train_loss': 0.4439486759273629, 'val_loss': 0.2625016106499566, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 7, 'train_loss': 0.38540295785979223, 'val_loss': 0.22010750240749782, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 8, 'train_loss': 0.43152773301852376, 'val_loss': 0.1591569052802192, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 9, 'train_loss': 0.42540271658646434, 'val_loss': 0.14992221196492514, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 10, 'train_loss': 0.39534168808083786, 'val_loss': 0.1572651465733846, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 11, 'train_loss': 0.38411823934630346, 'val_loss': 0.1715992291768392, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 12, 'train_loss': 0.35325061569088384, 'val_loss': 0.14757450421651205, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 13, 'train_loss': 0.38436496336209147, 'val_loss': 0.15089474784003365, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 14, 'train_loss': 0.36977856096468475, 'val_loss': 0.13900049527486166, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 15, 'train_loss': 0.36075710936596517, 'val_loss': 0.13915997081332737, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 16, 'train_loss': 0.36459713154717494, 'val_loss': 0.14133589797549778, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 17, 'train_loss': 0.3522907613139403, 'val_loss': 0.14663800928327772, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 18, 'train_loss': 0.3635441684409192, 'val_loss': 0.14684083726671007, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 19, 'train_loss': 0.3505529439763019, 'val_loss': 0.1428531805674235, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 20, 'train_loss': 0.3216312747252615, 'val_loss': 0.1211765209833781, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 21, 'train_loss': 0.35779251864081935, 'val_loss': 0.12030094199710423, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 22, 'train_loss': 0.3741397669440822, 'val_loss': 0.1355990833706326, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 23, 'train_loss': 0.34432469386803477, 'val_loss': 0.1252763271331787, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 24, 'train_loss': 0.33504799557359594, 'val_loss': 0.15637793805864122, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 25, 'train_loss': 0.34136636868903514, 'val_loss': 0.1664249234729343, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 26, 'train_loss': 0.33127611091262416, 'val_loss': 0.1564828422334459, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 27, 'train_loss': 0.3413910614816766, 'val_loss': 0.13999674055311415, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 28, 'train_loss': 0.3298399926800477, 'val_loss': 0.12261921829647487, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 29, 'train_loss': 0.3075657365353484, 'val_loss': 0.11352827813890246, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 30, 'train_loss': 0.3388333893135974, 'val_loss': 0.1166331238216824, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 31, 'train_loss': 0.3324736929253528, 'val_loss': 0.10813980632358128, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 32, 'train_loss': 0.35073240571900416, 'val_loss': 0.10863463083902995, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 33, 'train_loss': 0.33347251305454656, 'val_loss': 0.11297972997029622, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 34, 'train_loss': 0.3221324690078434, 'val_loss': 0.12993645668029785, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 35, 'train_loss': 0.3189792335033417, 'val_loss': 0.13941767480638292, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 36, 'train_loss': 0.34032731699316127, 'val_loss': 0.14667860666910806, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 37, 'train_loss': 0.3159814947529843, 'val_loss': 0.15107793278164333, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 38, 'train_loss': 0.30465375083057505, 'val_loss': 0.15351013342539468, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 39, 'train_loss': 0.3391702630017933, 'val_loss': 0.15510710080464682, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 40, 'train_loss': 0.3364250079581612, 'val_loss': 0.15127368768056235, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 41, 'train_loss': 0.3248775581780233, 'val_loss': 0.15754808319939506, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 42, 'train_loss': 0.32132523467666224, 'val_loss': 0.17112000783284506, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 43, 'train_loss': 0.3567202052003459, 'val_loss': 0.17406956354777017, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 44, 'train_loss': 0.30915886712701696, 'val_loss': 0.16777045196957058, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 45, 'train_loss': 0.2815850455509989, 'val_loss': 0.14921731419033474, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 46, 'train_loss': 0.3197221210912654, 'val_loss': 0.1381421618991428, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 47, 'train_loss': 0.30287381849790873, 'val_loss': 0.1347145504421658, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 48, 'train_loss': 0.3125151780090834, 'val_loss': 0.12789462672339547, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 49, 'train_loss': 0.29452778163709137, 'val_loss': 0.12495940261416966, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 50, 'train_loss': 0.31746206550221695, 'val_loss': 0.1327616638607449, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 51, 'train_loss': 0.293660743064002, 'val_loss': 0.12565737300448948, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 52, 'train_loss': 0.2958065334119295, 'val_loss': 0.12569312254587808, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 53, 'train_loss': 0.2736488984603631, 'val_loss': 0.12910731633504233, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 54, 'train_loss': 0.295534599768488, 'val_loss': 0.13619848092397055, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 55, 'train_loss': 0.2757809111162236, 'val_loss': 0.14003729820251465, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 56, 'train_loss': 0.3022430248950657, 'val_loss': 0.1380663447909885, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 57, 'train_loss': 0.3128231465816498, 'val_loss': 0.13127516375647652, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 58, 'train_loss': 0.32033644695031016, 'val_loss': 0.1354812913470798, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 59, 'train_loss': 0.31161073163936015, 'val_loss': 0.1580381923251682, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 60, 'train_loss': 0.2962921948025101, 'val_loss': 0.16650883356730142, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 61, 'train_loss': 0.3124772353391898, 'val_loss': 0.1710323757595486, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 62, 'train_loss': 0.2968883177167491, 'val_loss': 0.1734993060429891, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 63, 'train_loss': 0.28281970400559275, 'val_loss': 0.16894753774007162, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 64, 'train_loss': 0.2850946497760321, 'val_loss': 0.13735665215386283, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 65, 'train_loss': 0.31133069411704417, 'val_loss': 0.14039970768822563, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 66, 'train_loss': 0.2918910780235341, 'val_loss': 0.13655519485473633, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 67, 'train_loss': 0.27959483626641723, 'val_loss': 0.12189582983652751, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 68, 'train_loss': 0.26404858968759837, 'val_loss': 0.12002125051286486, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 69, 'train_loss': 0.3255070118527663, 'val_loss': 0.11772133244408502, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 70, 'train_loss': 0.2828752265164727, 'val_loss': 0.12940716743469238, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 71, 'train_loss': 0.3297010934666583, 'val_loss': 0.13728027873569065, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 72, 'train_loss': 0.31546734900851, 'val_loss': 0.1247934103012085, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 73, 'train_loss': 0.3030815171568017, 'val_loss': 0.11155516571468777, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 74, 'train_loss': 0.28295098715706873, 'val_loss': 0.10306975576612684, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 75, 'train_loss': 0.2910309908421416, 'val_loss': 0.11553553740183513, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 76, 'train_loss': 0.28518054046128927, 'val_loss': 0.1352293093999227, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 77, 'train_loss': 0.30304095619603205, 'val_loss': 0.1425391303168403, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 78, 'train_loss': 0.2842478399213992, 'val_loss': 0.13777526219685873, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 79, 'train_loss': 0.2614406582556273, 'val_loss': 0.13727552360958523, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 80, 'train_loss': 0.29675368494109106, 'val_loss': 0.1364748477935791, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 81, 'train_loss': 0.31683971732854843, 'val_loss': 0.1319727102915446, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 82, 'train_loss': 0.26894495518584, 'val_loss': 0.13229857550726998, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 83, 'train_loss': 0.29586795364555557, 'val_loss': 0.1325225697623359, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 84, 'train_loss': 0.2916395781855834, 'val_loss': 0.14103790124257407, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 85, 'train_loss': 0.27567409527929204, 'val_loss': 0.14943720234764946, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 86, 'train_loss': 0.27725086557237727, 'val_loss': 0.14913545714484322, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 87, 'train_loss': 0.2835582886871539, 'val_loss': 0.15127552880181205, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 88, 'train_loss': 0.2826427033073024, 'val_loss': 0.16472087966071236, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 89, 'train_loss': 0.29121241522462743, 'val_loss': 0.18320126003689235, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 90, 'train_loss': 0.29787523887659373, 'val_loss': 0.16970745722452799, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 91, 'train_loss': 0.30985473253225027, 'val_loss': 0.13749191496107313, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 92, 'train_loss': 0.3288173577503154, 'val_loss': 0.1327250666088528, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 93, 'train_loss': 0.26725031827625473, 'val_loss': 0.1667822864320543, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 94, 'train_loss': 0.2532036786800937, 'val_loss': 0.18713586860232884, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 95, 'train_loss': 0.27439186682826594, 'val_loss': 0.2005393107732137, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 96, 'train_loss': 0.2779844971863847, 'val_loss': 0.2039849493238661, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 97, 'train_loss': 0.29160664034517186, 'val_loss': 0.20262024137708876, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 98, 'train_loss': 0.31022343353221293, 'val_loss': 0.16470927662319607, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 99, 'train_loss': 0.26727384211201416, 'val_loss': 0.15020571814643013, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 100, 'train_loss': 0.27651170680397436, 'val_loss': 0.14420534504784477, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 101, 'train_loss': 0.25714786389940664, 'val_loss': 0.12876648373074, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 102, 'train_loss': 0.3009541371935292, 'val_loss': 0.10685362418492635, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 103, 'train_loss': 0.285173894151261, 'val_loss': 0.1100883616341485, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 104, 'train_loss': 0.2580249668344071, 'val_loss': 0.13266539573669434, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 105, 'train_loss': 0.2724951497818294, 'val_loss': 0.1425480710135566, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 106, 'train_loss': 0.2620664093839495, 'val_loss': 0.15374280346764457, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 107, 'train_loss': 0.2579057346049108, 'val_loss': 0.16219282150268555, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 108, 'train_loss': 0.27301032056933955, 'val_loss': 0.17159407668643528, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 109, 'train_loss': 0.28626524539370285, 'val_loss': 0.17826403511895073, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 110, 'train_loss': 0.30070147232005473, 'val_loss': 0.18054978052775064, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 111, 'train_loss': 0.265643962119755, 'val_loss': 0.18839983145395914, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 112, 'train_loss': 0.2923693664764103, 'val_loss': 0.19721064302656385, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 113, 'train_loss': 0.2535286933967942, 'val_loss': 0.20498132705688477, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 114, 'train_loss': 0.283962419158534, 'val_loss': 0.20556692282358804, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 115, 'train_loss': 0.2694842850691394, 'val_loss': 0.20119327969021267, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 116, 'train_loss': 0.24533350530423617, 'val_loss': 0.1940175692240397, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 117, 'train_loss': 0.2797138236070934, 'val_loss': 0.18033779991997612, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 118, 'train_loss': 0.24833486111540543, 'val_loss': 0.1560985114839342, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 119, 'train_loss': 0.2615786747712838, 'val_loss': 0.1390055153104994, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 120, 'train_loss': 0.26648803917985214, 'val_loss': 0.14104493459065756, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 121, 'train_loss': 0.26466652947036845, 'val_loss': 0.13313439157274035, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 122, 'train_loss': 0.2687097717272608, 'val_loss': 0.13443484571244982, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 123, 'train_loss': 0.2809803211375287, 'val_loss': 0.12830592526329887, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 124, 'train_loss': 0.24607398674676292, 'val_loss': 0.13149876064724392, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 125, 'train_loss': 0.2397756294200295, 'val_loss': 0.13070423073238796, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 126, 'train_loss': 0.2686047852039337, 'val_loss': 0.12426135275099012, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 127, 'train_loss': 0.24726691528370506, 'val_loss': 0.1236832406785753, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 128, 'train_loss': 0.2321213170101768, 'val_loss': 0.12951996591356066, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 129, 'train_loss': 0.26314137327043635, 'val_loss': 0.13523956139882407, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 130, 'train_loss': 0.2571179780520891, 'val_loss': 0.18425426218244764, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 131, 'train_loss': 0.2754133508393639, 'val_loss': 0.16616976261138916, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 132, 'train_loss': 0.2424748885004144, 'val_loss': 0.16450945536295572, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 133, 'train_loss': 0.25305819511413574, 'val_loss': 0.1622368892033895, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 134, 'train_loss': 0.2628118850682911, 'val_loss': 0.16073494487338597, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 135, 'train_loss': 0.2709312352694963, 'val_loss': 0.13988971710205078, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 136, 'train_loss': 0.24013778018323997, 'val_loss': 0.11767835087246364, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 137, 'train_loss': 0.271665862516353, 'val_loss': 0.11605630980597602, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 138, 'train_loss': 0.281268747621461, 'val_loss': 0.13176871670616996, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 139, 'train_loss': 0.3003668381195319, 'val_loss': 0.1566630999247233, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 140, 'train_loss': 0.24094204486984955, 'val_loss': 0.16936112774742973, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 141, 'train_loss': 0.25763875246047974, 'val_loss': 0.16480312082502577, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 142, 'train_loss': 0.2691841403904714, 'val_loss': 0.17062343491448295, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 143, 'train_loss': 0.2855924760040484, 'val_loss': 0.1822660896513197, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 144, 'train_loss': 0.2520052453404979, 'val_loss': 0.18968253665500218, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 145, 'train_loss': 0.2883976995944977, 'val_loss': 0.17988750669691297, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 146, 'train_loss': 0.2320913060715324, 'val_loss': 0.15029414494832358, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 147, 'train_loss': 0.27697937072891937, 'val_loss': 0.15914943483140734, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 148, 'train_loss': 0.25980315396660253, 'val_loss': 0.16887595918443468, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 149, 'train_loss': 0.2742095377884413, 'val_loss': 0.16063445144229466, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 150, 'train_loss': 0.2628163146345239, 'val_loss': 0.1577998267279731, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 151, 'train_loss': 0.2847365380117768, 'val_loss': 0.14311823580000135, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 152, 'train_loss': 0.255342141578072, 'val_loss': 0.14750899208916557, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 153, 'train_loss': 0.2578939332773811, 'val_loss': 0.1603338188595242, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 154, 'train_loss': 0.26438292900198385, 'val_loss': 0.17588961124420166, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 155, 'train_loss': 0.2326427415797585, 'val_loss': 0.17001965310838488, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 156, 'train_loss': 0.27264636323640223, 'val_loss': 0.17636144161224365, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 157, 'train_loss': 0.27502303139159556, 'val_loss': 0.1873889366785685, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 158, 'train_loss': 0.2535231701637569, 'val_loss': 0.19261407852172852, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 159, 'train_loss': 0.2735950060580906, 'val_loss': 0.1783886088265313, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 160, 'train_loss': 0.24811521250950663, 'val_loss': 0.15967892275916207, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 161, 'train_loss': 0.24635890518364154, 'val_loss': 0.13920489947001138, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 162, 'train_loss': 0.2674061675605021, 'val_loss': 0.12537407875061035, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 163, 'train_loss': 0.24846037085119047, 'val_loss': 0.11696108182271321, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 164, 'train_loss': 0.2361585631182319, 'val_loss': 0.12317458788553874, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 165, 'train_loss': 0.236094773599976, 'val_loss': 0.12891851531134713, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 166, 'train_loss': 0.2692287697603828, 'val_loss': 0.13339033391740587, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 167, 'train_loss': 0.2788770998779096, 'val_loss': 0.1285719076792399, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 168, 'train_loss': 0.23867365443392805, 'val_loss': 0.12771639559004042, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 169, 'train_loss': 0.254621564165542, 'val_loss': 0.1233443816502889, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 170, 'train_loss': 0.2624339504461539, 'val_loss': 0.11708360248141819, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 171, 'train_loss': 0.25051756673737574, 'val_loss': 0.11771701441870795, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 172, 'train_loss': 0.26607060040298264, 'val_loss': 0.12889058060116237, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 173, 'train_loss': 0.2523512146190593, 'val_loss': 0.2584267457326253, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 174, 'train_loss': 0.2347399800231582, 'val_loss': 0.2597154246436225, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 175, 'train_loss': 0.25575979132401316, 'val_loss': 0.18179836538102892, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 176, 'train_loss': 0.2616746660910155, 'val_loss': 0.20089210404290092, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 177, 'train_loss': 0.26677771461637395, 'val_loss': 0.129603902498881, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 178, 'train_loss': 0.22885315865278244, 'val_loss': 0.1114921702278985, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 179, 'train_loss': 0.2331653056960357, 'val_loss': 0.10118642780515882, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 180, 'train_loss': 0.2548044014133905, 'val_loss': 0.09478342533111572, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 181, 'train_loss': 0.22895402382863195, 'val_loss': 0.09719356563356188, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 182, 'train_loss': 0.28179888074335296, 'val_loss': 0.10006282064649794, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 183, 'train_loss': 0.23748362103575155, 'val_loss': 0.11323644055260552, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 184, 'train_loss': 0.2536322874458213, 'val_loss': 0.11986951033274333, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 185, 'train_loss': 0.2522956270136331, 'val_loss': 0.12883276409573025, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 186, 'train_loss': 0.25863823804416153, 'val_loss': 0.1341017352210151, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 187, 'train_loss': 0.2566879274029481, 'val_loss': 0.1569731765323215, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 188, 'train_loss': 0.24309081388147255, 'val_loss': 0.19673905107710096, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 189, 'train_loss': 0.22437052703217455, 'val_loss': 0.1948242055045234, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 190, 'train_loss': 0.2866087046108748, 'val_loss': 0.18648486667209202, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 191, 'train_loss': 0.2614132714898963, 'val_loss': 0.18324669202168783, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 192, 'train_loss': 0.20599424211602463, 'val_loss': 0.17042824957105848, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 193, 'train_loss': 0.21219610971839806, 'val_loss': 0.16013716326819527, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 194, 'train_loss': 0.25927149229928065, 'val_loss': 0.15329457653893364, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 195, 'train_loss': 0.23692331227817034, 'val_loss': 0.15325029691060385, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 196, 'train_loss': 0.27910704675473663, 'val_loss': 0.1413495275709364, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 197, 'train_loss': 0.2630082553154544, 'val_loss': 0.14465377065870497, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 198, 'train_loss': 0.24488209776188197, 'val_loss': 0.13541388511657715, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 199, 'train_loss': 0.24365844734405218, 'val_loss': 0.15099708239237467, 'test_acc': 0.8333333333333334}
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
{'fold': 9, 'epoch': 200, 'train_loss': 0.23286331248910805, 'val_loss': 0.1569459703233507, 'test_acc': 0.8333333333333334}
Val Loss: 0.3297, Test Accuracy: 0.830 ± 0.092, Duration: 16.948
Best result - 0.830 ± 0.092
--
IMDB-BINARY - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.5818560257554054, 'val_loss': 0.40735371589660646, 'test_acc': 0.75}
{'fold': 9, 'epoch': 2, 'train_loss': 0.3877997100353241, 'val_loss': 0.33760527610778807, 'test_acc': 0.78}
{'fold': 9, 'epoch': 3, 'train_loss': 0.3405390061438084, 'val_loss': 0.33962472915649417, 'test_acc': 0.84}
{'fold': 9, 'epoch': 4, 'train_loss': 0.31047797910869124, 'val_loss': 0.36123992919921877, 'test_acc': 0.83}
{'fold': 9, 'epoch': 5, 'train_loss': 0.27866632007062436, 'val_loss': 0.3547401189804077, 'test_acc': 0.88}
{'fold': 9, 'epoch': 6, 'train_loss': 0.25924438759684565, 'val_loss': 0.361029052734375, 'test_acc': 0.89}
{'fold': 9, 'epoch': 7, 'train_loss': 0.26225613057613373, 'val_loss': 0.39619030475616457, 'test_acc': 0.88}
{'fold': 9, 'epoch': 8, 'train_loss': 0.2941830582916737, 'val_loss': 0.3914609432220459, 'test_acc': 0.88}
{'fold': 9, 'epoch': 9, 'train_loss': 0.2510449979454279, 'val_loss': 0.38727464199066164, 'test_acc': 0.89}
{'fold': 9, 'epoch': 10, 'train_loss': 0.24886559434235095, 'val_loss': 0.3656530237197876, 'test_acc': 0.89}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5640374194830656, 'val_loss': 0.3901792287826538, 'test_acc': 0.86}
{'fold': 9, 'epoch': 12, 'train_loss': 0.26285041868686676, 'val_loss': 0.36011454582214353, 'test_acc': 0.87}
{'fold': 9, 'epoch': 13, 'train_loss': 0.2561003942042589, 'val_loss': 0.339501371383667, 'test_acc': 0.88}
{'fold': 9, 'epoch': 14, 'train_loss': 0.24264838248491288, 'val_loss': 0.4570767593383789, 'test_acc': 0.81}
{'fold': 9, 'epoch': 15, 'train_loss': 0.24663281254470348, 'val_loss': 0.4460983180999756, 'test_acc': 0.85}
{'fold': 9, 'epoch': 16, 'train_loss': 0.2548042792826891, 'val_loss': 0.4187435054779053, 'test_acc': 0.86}
{'fold': 9, 'epoch': 17, 'train_loss': 0.24757280964404343, 'val_loss': 0.4463031244277954, 'test_acc': 0.85}
{'fold': 9, 'epoch': 18, 'train_loss': 0.2451570328325033, 'val_loss': 0.40054203987121584, 'test_acc': 0.87}
{'fold': 9, 'epoch': 19, 'train_loss': 0.23039601556956768, 'val_loss': 0.45553529262542725, 'test_acc': 0.84}
{'fold': 9, 'epoch': 20, 'train_loss': 0.22773723900318146, 'val_loss': 0.4576411533355713, 'test_acc': 0.85}
{'fold': 9, 'epoch': 21, 'train_loss': 0.22996603399515153, 'val_loss': 0.4934982633590698, 'test_acc': 0.84}
{'fold': 9, 'epoch': 22, 'train_loss': 0.23064661137759684, 'val_loss': 0.49732959270477295, 'test_acc': 0.85}
{'fold': 9, 'epoch': 23, 'train_loss': 0.22061685472726822, 'val_loss': 0.5145588159561157, 'test_acc': 0.84}
{'fold': 9, 'epoch': 24, 'train_loss': 0.235538474842906, 'val_loss': 0.494856915473938, 'test_acc': 0.84}
{'fold': 9, 'epoch': 25, 'train_loss': 0.23354627303779124, 'val_loss': 0.4458457851409912, 'test_acc': 0.82}
{'fold': 9, 'epoch': 26, 'train_loss': 0.22224566247314215, 'val_loss': 0.4892088508605957, 'test_acc': 0.84}
{'fold': 9, 'epoch': 27, 'train_loss': 0.22652159743010997, 'val_loss': 0.515709981918335, 'test_acc': 0.87}
{'fold': 9, 'epoch': 28, 'train_loss': 0.2321755565702915, 'val_loss': 0.5079559087753296, 'test_acc': 0.82}
{'fold': 9, 'epoch': 29, 'train_loss': 0.23532665520906448, 'val_loss': 0.44865926265716555, 'test_acc': 0.87}
{'fold': 9, 'epoch': 30, 'train_loss': 0.23536231815814973, 'val_loss': 0.5194281530380249, 'test_acc': 0.82}
{'fold': 9, 'epoch': 31, 'train_loss': 0.21922211162745953, 'val_loss': 0.5068316078186035, 'test_acc': 0.82}
{'fold': 9, 'epoch': 32, 'train_loss': 0.22178545277565717, 'val_loss': 0.4891129732131958, 'test_acc': 0.86}
{'fold': 9, 'epoch': 33, 'train_loss': 0.23213520385324954, 'val_loss': 0.44904548168182373, 'test_acc': 0.83}
{'fold': 9, 'epoch': 34, 'train_loss': 0.222191346809268, 'val_loss': 0.49989920139312743, 'test_acc': 0.82}
{'fold': 9, 'epoch': 35, 'train_loss': 0.20850089862942695, 'val_loss': 0.5052238750457764, 'test_acc': 0.85}
{'fold': 9, 'epoch': 36, 'train_loss': 0.2243097398430109, 'val_loss': 0.5279627847671509, 'test_acc': 0.82}
{'fold': 9, 'epoch': 37, 'train_loss': 0.21879576556384564, 'val_loss': 0.4995261240005493, 'test_acc': 0.82}
{'fold': 9, 'epoch': 38, 'train_loss': 0.2171698311343789, 'val_loss': 0.5112082099914551, 'test_acc': 0.82}
{'fold': 9, 'epoch': 39, 'train_loss': 0.211276800557971, 'val_loss': 0.5045402860641479, 'test_acc': 0.81}
{'fold': 9, 'epoch': 40, 'train_loss': 0.22704267613589763, 'val_loss': 0.5828445959091186, 'test_acc': 0.82}
{'fold': 9, 'epoch': 41, 'train_loss': 0.21574385836720467, 'val_loss': 0.4919434404373169, 'test_acc': 0.82}
{'fold': 9, 'epoch': 42, 'train_loss': 0.2240513812750578, 'val_loss': 0.5017434072494507, 'test_acc': 0.82}
{'fold': 9, 'epoch': 43, 'train_loss': 0.2248439259827137, 'val_loss': 0.546913046836853, 'test_acc': 0.81}
{'fold': 9, 'epoch': 44, 'train_loss': 0.2151778370141983, 'val_loss': 0.4546350622177124, 'test_acc': 0.83}
{'fold': 9, 'epoch': 45, 'train_loss': 0.22011677958071232, 'val_loss': 0.6668402814865112, 'test_acc': 0.79}
{'fold': 9, 'epoch': 46, 'train_loss': 0.2162353176623583, 'val_loss': 0.4449660110473633, 'test_acc': 0.83}
{'fold': 9, 'epoch': 47, 'train_loss': 0.21523960456252098, 'val_loss': 0.5218833208084106, 'test_acc': 0.83}
{'fold': 9, 'epoch': 48, 'train_loss': 0.22491875290870667, 'val_loss': 0.6163581705093384, 'test_acc': 0.82}
{'fold': 9, 'epoch': 49, 'train_loss': 0.2188419844955206, 'val_loss': 0.3659529781341553, 'test_acc': 0.86}
{'fold': 9, 'epoch': 50, 'train_loss': 0.2337530914694071, 'val_loss': 0.5443830919265747, 'test_acc': 0.83}
{'fold': 9, 'epoch': 51, 'train_loss': 0.2091527283191681, 'val_loss': 0.5796102285385132, 'test_acc': 0.86}
{'fold': 9, 'epoch': 52, 'train_loss': 0.21955031119287013, 'val_loss': 0.4936669683456421, 'test_acc': 0.81}
{'fold': 9, 'epoch': 53, 'train_loss': 0.22543019168078898, 'val_loss': 0.5373692512512207, 'test_acc': 0.79}
{'fold': 9, 'epoch': 54, 'train_loss': 0.22543955836445093, 'val_loss': 0.5645591115951538, 'test_acc': 0.82}
{'fold': 9, 'epoch': 55, 'train_loss': 0.21581470482051374, 'val_loss': 0.5456590223312378, 'test_acc': 0.82}
{'fold': 9, 'epoch': 56, 'train_loss': 0.2078842304646969, 'val_loss': 0.5335168600082397, 'test_acc': 0.83}
{'fold': 9, 'epoch': 57, 'train_loss': 0.21098536550998687, 'val_loss': 0.5690707635879516, 'test_acc': 0.83}
{'fold': 9, 'epoch': 58, 'train_loss': 0.21118887960910798, 'val_loss': 0.4487387466430664, 'test_acc': 0.81}
{'fold': 9, 'epoch': 59, 'train_loss': 0.21290739960968494, 'val_loss': 0.5713681411743164, 'test_acc': 0.82}
{'fold': 9, 'epoch': 60, 'train_loss': 0.21706465720199047, 'val_loss': 0.6547628784179688, 'test_acc': 0.8}
{'fold': 9, 'epoch': 61, 'train_loss': 0.21762690469622611, 'val_loss': 0.5204367399215698, 'test_acc': 0.82}
{'fold': 9, 'epoch': 62, 'train_loss': 0.21676576174795628, 'val_loss': 0.517187705039978, 'test_acc': 0.82}
{'fold': 9, 'epoch': 63, 'train_loss': 0.21062215454876423, 'val_loss': 0.6299330425262452, 'test_acc': 0.81}
{'fold': 9, 'epoch': 64, 'train_loss': 0.22948201075196267, 'val_loss': 0.39822678565979003, 'test_acc': 0.81}
{'fold': 9, 'epoch': 65, 'train_loss': 0.23898058496415614, 'val_loss': 0.5006068706512451, 'test_acc': 0.82}
{'fold': 9, 'epoch': 66, 'train_loss': 0.22796614952385424, 'val_loss': 0.5382511901855469, 'test_acc': 0.83}
{'fold': 9, 'epoch': 67, 'train_loss': 0.21321846693754196, 'val_loss': 0.5465846633911133, 'test_acc': 0.82}
{'fold': 9, 'epoch': 68, 'train_loss': 0.21005501132458448, 'val_loss': 0.6040589618682861, 'test_acc': 0.84}
{'fold': 9, 'epoch': 69, 'train_loss': 0.20836471542716026, 'val_loss': 0.5311455631256103, 'test_acc': 0.84}
{'fold': 9, 'epoch': 70, 'train_loss': 0.206856644526124, 'val_loss': 0.5138966512680053, 'test_acc': 0.84}
{'fold': 9, 'epoch': 71, 'train_loss': 0.2044962015002966, 'val_loss': 0.6345225954055786, 'test_acc': 0.83}
{'fold': 9, 'epoch': 72, 'train_loss': 0.21410179510712624, 'val_loss': 0.524545955657959, 'test_acc': 0.86}
{'fold': 9, 'epoch': 73, 'train_loss': 0.2307640915736556, 'val_loss': 0.49409964084625246, 'test_acc': 0.82}
{'fold': 9, 'epoch': 74, 'train_loss': 0.21007169000804424, 'val_loss': 0.5954982995986938, 'test_acc': 0.82}
{'fold': 9, 'epoch': 75, 'train_loss': 0.21565336193889378, 'val_loss': 0.40994218349456785, 'test_acc': 0.82}
{'fold': 9, 'epoch': 76, 'train_loss': 0.21772892605513333, 'val_loss': 0.5743436384201049, 'test_acc': 0.83}
{'fold': 9, 'epoch': 77, 'train_loss': 0.22996000871062278, 'val_loss': 0.48481019496917727, 'test_acc': 0.82}
{'fold': 9, 'epoch': 78, 'train_loss': 0.22415925934910774, 'val_loss': 0.4841908884048462, 'test_acc': 0.82}
{'fold': 9, 'epoch': 79, 'train_loss': 0.21771201975643634, 'val_loss': 0.5192793655395508, 'test_acc': 0.83}
{'fold': 9, 'epoch': 80, 'train_loss': 0.23860887847840787, 'val_loss': 0.5584674549102783, 'test_acc': 0.84}
{'fold': 9, 'epoch': 81, 'train_loss': 0.2189116045832634, 'val_loss': 0.5571322822570801, 'test_acc': 0.82}
{'fold': 9, 'epoch': 82, 'train_loss': 0.20473238322883844, 'val_loss': 0.6459618663787842, 'test_acc': 0.83}
{'fold': 9, 'epoch': 83, 'train_loss': 0.20621297731995583, 'val_loss': 0.5712291097640991, 'test_acc': 0.82}
{'fold': 9, 'epoch': 84, 'train_loss': 0.20755098350346088, 'val_loss': 0.6473437643051148, 'test_acc': 0.82}
{'fold': 9, 'epoch': 85, 'train_loss': 0.20801685005426407, 'val_loss': 0.5267597818374634, 'test_acc': 0.84}
{'fold': 9, 'epoch': 86, 'train_loss': 0.2285032607614994, 'val_loss': 0.5960770893096924, 'test_acc': 0.81}
{'fold': 9, 'epoch': 87, 'train_loss': 0.21961795836687087, 'val_loss': 0.43443253993988035, 'test_acc': 0.8}
{'fold': 9, 'epoch': 88, 'train_loss': 0.2070476606488228, 'val_loss': 0.5593404340744018, 'test_acc': 0.83}
{'fold': 9, 'epoch': 89, 'train_loss': 0.23282866552472115, 'val_loss': 0.6063511991500854, 'test_acc': 0.82}
{'fold': 9, 'epoch': 90, 'train_loss': 0.22861591391265393, 'val_loss': 0.4702165985107422, 'test_acc': 0.83}
{'fold': 9, 'epoch': 91, 'train_loss': 0.21805690973997116, 'val_loss': 0.5494920063018799, 'test_acc': 0.83}
{'fold': 9, 'epoch': 92, 'train_loss': 0.1999897163361311, 'val_loss': 0.5445697641372681, 'test_acc': 0.84}
{'fold': 9, 'epoch': 93, 'train_loss': 0.20843619331717492, 'val_loss': 0.6143970632553101, 'test_acc': 0.84}
{'fold': 9, 'epoch': 94, 'train_loss': 0.2041698530316353, 'val_loss': 0.5288342952728271, 'test_acc': 0.83}
{'fold': 9, 'epoch': 95, 'train_loss': 0.2079218067228794, 'val_loss': 0.5816084575653077, 'test_acc': 0.84}
{'fold': 9, 'epoch': 96, 'train_loss': 0.20831434410065414, 'val_loss': 0.4807711410522461, 'test_acc': 0.82}
{'fold': 9, 'epoch': 97, 'train_loss': 0.20182482711970806, 'val_loss': 0.5692296934127807, 'test_acc': 0.82}
{'fold': 9, 'epoch': 98, 'train_loss': 0.20022664964199066, 'val_loss': 0.5591777849197388, 'test_acc': 0.83}
{'fold': 9, 'epoch': 99, 'train_loss': 0.2153426025994122, 'val_loss': 0.677235255241394, 'test_acc': 0.83}
{'fold': 9, 'epoch': 100, 'train_loss': 0.22594535015523434, 'val_loss': 0.46139102935791015, 'test_acc': 0.83}
{'fold': 9, 'epoch': 101, 'train_loss': 0.19993480257689952, 'val_loss': 0.5250566005706787, 'test_acc': 0.83}
{'fold': 9, 'epoch': 102, 'train_loss': 0.20569832511246205, 'val_loss': 0.7004092693328857, 'test_acc': 0.82}
{'fold': 9, 'epoch': 103, 'train_loss': 0.2149299792945385, 'val_loss': 0.6560001659393311, 'test_acc': 0.84}
{'fold': 9, 'epoch': 104, 'train_loss': 0.20334310717880727, 'val_loss': 0.6044406843185425, 'test_acc': 0.84}
{'fold': 9, 'epoch': 105, 'train_loss': 0.21119597591459752, 'val_loss': 0.568435959815979, 'test_acc': 0.84}
{'fold': 9, 'epoch': 106, 'train_loss': 0.20601156875491142, 'val_loss': 0.6403629541397095, 'test_acc': 0.84}
{'fold': 9, 'epoch': 107, 'train_loss': 0.21170503683388234, 'val_loss': 0.7211689662933349, 'test_acc': 0.82}
{'fold': 9, 'epoch': 108, 'train_loss': 0.20034314095973968, 'val_loss': 0.5117798042297363, 'test_acc': 0.83}
{'fold': 9, 'epoch': 109, 'train_loss': 0.21332361958920956, 'val_loss': 0.6141562223434448, 'test_acc': 0.82}
{'fold': 9, 'epoch': 110, 'train_loss': 0.20560653787106276, 'val_loss': 0.5919920015335083, 'test_acc': 0.82}
{'fold': 9, 'epoch': 111, 'train_loss': 0.20660029612481595, 'val_loss': 0.6220795011520386, 'test_acc': 0.83}
{'fold': 9, 'epoch': 112, 'train_loss': 0.2142657745629549, 'val_loss': 0.542884283065796, 'test_acc': 0.82}
{'fold': 9, 'epoch': 113, 'train_loss': 0.2044765818864107, 'val_loss': 0.6431608629226685, 'test_acc': 0.84}
{'fold': 9, 'epoch': 114, 'train_loss': 0.20576821975409984, 'val_loss': 0.5515974330902099, 'test_acc': 0.84}
{'fold': 9, 'epoch': 115, 'train_loss': 0.2137984536588192, 'val_loss': 0.678359375, 'test_acc': 0.84}
{'fold': 9, 'epoch': 116, 'train_loss': 0.21998471356928348, 'val_loss': 0.5556143426895142, 'test_acc': 0.84}
{'fold': 9, 'epoch': 117, 'train_loss': 0.20157760567963123, 'val_loss': 0.6177506828308106, 'test_acc': 0.82}
{'fold': 9, 'epoch': 118, 'train_loss': 0.1998098585754633, 'val_loss': 0.6222902011871337, 'test_acc': 0.82}
{'fold': 9, 'epoch': 119, 'train_loss': 0.1990174327045679, 'val_loss': 0.7678198957443237, 'test_acc': 0.81}
{'fold': 9, 'epoch': 120, 'train_loss': 0.20723183322697877, 'val_loss': 0.726848030090332, 'test_acc': 0.82}
{'fold': 9, 'epoch': 121, 'train_loss': 0.20560151077806949, 'val_loss': 0.6602781438827514, 'test_acc': 0.84}
{'fold': 9, 'epoch': 122, 'train_loss': 0.19616728499531746, 'val_loss': 0.7084132528305054, 'test_acc': 0.82}
{'fold': 9, 'epoch': 123, 'train_loss': 0.1980172824114561, 'val_loss': 0.6414319562911988, 'test_acc': 0.82}
{'fold': 9, 'epoch': 124, 'train_loss': 0.21040837690234185, 'val_loss': 0.7687670278549195, 'test_acc': 0.84}
{'fold': 9, 'epoch': 125, 'train_loss': 0.21161145642399787, 'val_loss': 0.5818144798278808, 'test_acc': 0.85}
{'fold': 9, 'epoch': 126, 'train_loss': 0.2105077899992466, 'val_loss': 0.7312394762039185, 'test_acc': 0.82}
{'fold': 9, 'epoch': 127, 'train_loss': 0.21551590859889985, 'val_loss': 0.6458581256866455, 'test_acc': 0.83}
{'fold': 9, 'epoch': 128, 'train_loss': 0.2016657715663314, 'val_loss': 0.6418080759048462, 'test_acc': 0.82}
{'fold': 9, 'epoch': 129, 'train_loss': 0.22961552496999502, 'val_loss': 0.7003294754028321, 'test_acc': 0.81}
{'fold': 9, 'epoch': 130, 'train_loss': 0.21190457940101623, 'val_loss': 0.4860347080230713, 'test_acc': 0.81}
{'fold': 9, 'epoch': 131, 'train_loss': 0.21126686222851276, 'val_loss': 0.7853097295761109, 'test_acc': 0.82}
{'fold': 9, 'epoch': 132, 'train_loss': 0.2112594209611416, 'val_loss': 0.6022373771667481, 'test_acc': 0.83}
{'fold': 9, 'epoch': 133, 'train_loss': 0.20144915021955967, 'val_loss': 0.6509992456436158, 'test_acc': 0.83}
{'fold': 9, 'epoch': 134, 'train_loss': 0.20771135948598385, 'val_loss': 0.5622266674041748, 'test_acc': 0.81}
{'fold': 9, 'epoch': 135, 'train_loss': 0.19703047443181276, 'val_loss': 0.6257898664474487, 'test_acc': 0.82}
{'fold': 9, 'epoch': 136, 'train_loss': 0.20094114877283573, 'val_loss': 0.7640486478805542, 'test_acc': 0.83}
{'fold': 9, 'epoch': 137, 'train_loss': 0.20326085910201072, 'val_loss': 0.6359256887435913, 'test_acc': 0.81}
{'fold': 9, 'epoch': 138, 'train_loss': 0.1989922896027565, 'val_loss': 0.6556005191802978, 'test_acc': 0.81}
{'fold': 9, 'epoch': 139, 'train_loss': 0.1977366091683507, 'val_loss': 0.7636856603622436, 'test_acc': 0.82}
{'fold': 9, 'epoch': 140, 'train_loss': 0.20386620834469796, 'val_loss': 0.6906631612777709, 'test_acc': 0.81}
{'fold': 9, 'epoch': 141, 'train_loss': 0.21232712008059024, 'val_loss': 0.8756017112731933, 'test_acc': 0.81}
{'fold': 9, 'epoch': 142, 'train_loss': 0.21417091749608516, 'val_loss': 0.5580667066574097, 'test_acc': 0.82}
{'fold': 9, 'epoch': 143, 'train_loss': 0.23051028847694396, 'val_loss': 0.5278345775604248, 'test_acc': 0.81}
{'fold': 9, 'epoch': 144, 'train_loss': 0.2208026982843876, 'val_loss': 0.4994442749023438, 'test_acc': 0.83}
{'fold': 9, 'epoch': 145, 'train_loss': 0.2082920964807272, 'val_loss': 0.7054170036315918, 'test_acc': 0.82}
{'fold': 9, 'epoch': 146, 'train_loss': 0.20001694820821286, 'val_loss': 0.6372723340988159, 'test_acc': 0.83}
{'fold': 9, 'epoch': 147, 'train_loss': 0.21267455406486988, 'val_loss': 0.5787024354934692, 'test_acc': 0.82}
{'fold': 9, 'epoch': 148, 'train_loss': 0.20077648498117923, 'val_loss': 0.5817529964447021, 'test_acc': 0.82}
{'fold': 9, 'epoch': 149, 'train_loss': 0.19745792783796787, 'val_loss': 0.7295625019073486, 'test_acc': 0.82}
{'fold': 9, 'epoch': 150, 'train_loss': 0.21537251491099596, 'val_loss': 0.5646887159347534, 'test_acc': 0.81}
{'fold': 9, 'epoch': 151, 'train_loss': 0.20135999135673047, 'val_loss': 0.5981465101242065, 'test_acc': 0.81}
{'fold': 9, 'epoch': 152, 'train_loss': 0.20622298680245876, 'val_loss': 0.7767700719833374, 'test_acc': 0.82}
{'fold': 9, 'epoch': 153, 'train_loss': 0.2004399484023452, 'val_loss': 0.6363933086395264, 'test_acc': 0.82}
{'fold': 9, 'epoch': 154, 'train_loss': 0.19567975755780936, 'val_loss': 0.7403177452087403, 'test_acc': 0.83}
{'fold': 9, 'epoch': 155, 'train_loss': 0.20614733453840017, 'val_loss': 0.7174921226501465, 'test_acc': 0.82}
{'fold': 9, 'epoch': 156, 'train_loss': 0.19879059866070747, 'val_loss': 0.578490343093872, 'test_acc': 0.83}
{'fold': 9, 'epoch': 157, 'train_loss': 0.201819889806211, 'val_loss': 0.7444226694107056, 'test_acc': 0.83}
{'fold': 9, 'epoch': 158, 'train_loss': 0.2188126841560006, 'val_loss': 0.5606855821609497, 'test_acc': 0.84}
{'fold': 9, 'epoch': 159, 'train_loss': 0.2097658932209015, 'val_loss': 0.6772033596038818, 'test_acc': 0.83}
{'fold': 9, 'epoch': 160, 'train_loss': 0.2143797818571329, 'val_loss': 0.6321198654174804, 'test_acc': 0.84}
{'fold': 9, 'epoch': 161, 'train_loss': 0.19813673086464406, 'val_loss': 0.7384333896636963, 'test_acc': 0.83}
{'fold': 9, 'epoch': 162, 'train_loss': 0.20739004090428353, 'val_loss': 0.6290835285186768, 'test_acc': 0.82}
{'fold': 9, 'epoch': 163, 'train_loss': 0.19364372324198484, 'val_loss': 0.7186714172363281, 'test_acc': 0.83}
{'fold': 9, 'epoch': 164, 'train_loss': 0.19266270846128464, 'val_loss': 0.7013717126846314, 'test_acc': 0.83}
{'fold': 9, 'epoch': 165, 'train_loss': 0.19594352282583713, 'val_loss': 0.7104858064651489, 'test_acc': 0.84}
{'fold': 9, 'epoch': 166, 'train_loss': 0.19980177227407694, 'val_loss': 0.6813025426864624, 'test_acc': 0.83}
{'fold': 9, 'epoch': 167, 'train_loss': 0.1974803525954485, 'val_loss': 0.8377197170257569, 'test_acc': 0.84}
{'fold': 9, 'epoch': 168, 'train_loss': 0.2012453034520149, 'val_loss': 0.6841547346115112, 'test_acc': 0.84}
{'fold': 9, 'epoch': 169, 'train_loss': 0.21692097038030625, 'val_loss': 0.7706112480163574, 'test_acc': 0.82}
{'fold': 9, 'epoch': 170, 'train_loss': 0.21550519578158855, 'val_loss': 0.6240561246871948, 'test_acc': 0.83}
{'fold': 9, 'epoch': 171, 'train_loss': 0.20157246403396128, 'val_loss': 0.6589156532287598, 'test_acc': 0.84}
{'fold': 9, 'epoch': 172, 'train_loss': 0.19356054067611694, 'val_loss': 0.7568098878860474, 'test_acc': 0.83}
{'fold': 9, 'epoch': 173, 'train_loss': 0.2020529881119728, 'val_loss': 0.6269206666946411, 'test_acc': 0.83}
{'fold': 9, 'epoch': 174, 'train_loss': 0.2037661276757717, 'val_loss': 0.6434212446212768, 'test_acc': 0.83}
{'fold': 9, 'epoch': 175, 'train_loss': 0.19841062966734171, 'val_loss': 0.772494707107544, 'test_acc': 0.82}
{'fold': 9, 'epoch': 176, 'train_loss': 0.1946903642266989, 'val_loss': 0.8559820365905761, 'test_acc': 0.82}
{'fold': 9, 'epoch': 177, 'train_loss': 0.19801160134375095, 'val_loss': 0.8301549053192139, 'test_acc': 0.83}
{'fold': 9, 'epoch': 178, 'train_loss': 0.19456779435276986, 'val_loss': 0.7607307052612304, 'test_acc': 0.83}
{'fold': 9, 'epoch': 179, 'train_loss': 0.19845135770738126, 'val_loss': 0.7656430578231812, 'test_acc': 0.83}
{'fold': 9, 'epoch': 180, 'train_loss': 0.19809028767049314, 'val_loss': 0.7866887283325196, 'test_acc': 0.83}
{'fold': 9, 'epoch': 181, 'train_loss': 0.19650054685771465, 'val_loss': 1.0308551168441773, 'test_acc': 0.83}
{'fold': 9, 'epoch': 182, 'train_loss': 0.20505117513239385, 'val_loss': 0.6463188886642456, 'test_acc': 0.83}
{'fold': 9, 'epoch': 183, 'train_loss': 0.20135590620338917, 'val_loss': 0.7470764064788818, 'test_acc': 0.82}
{'fold': 9, 'epoch': 184, 'train_loss': 0.20281017236411572, 'val_loss': 0.7482440090179443, 'test_acc': 0.83}
{'fold': 9, 'epoch': 185, 'train_loss': 0.20030571147799492, 'val_loss': 0.8339903259277344, 'test_acc': 0.83}
{'fold': 9, 'epoch': 186, 'train_loss': 0.19890982508659363, 'val_loss': 0.7620638656616211, 'test_acc': 0.84}
{'fold': 9, 'epoch': 187, 'train_loss': 0.20101170148700476, 'val_loss': 0.6429436302185059, 'test_acc': 0.84}
{'fold': 9, 'epoch': 188, 'train_loss': 0.20034908056259154, 'val_loss': 0.765130763053894, 'test_acc': 0.84}
{'fold': 9, 'epoch': 189, 'train_loss': 0.19895174857228995, 'val_loss': 0.7195986318588257, 'test_acc': 0.81}
{'fold': 9, 'epoch': 190, 'train_loss': 0.20584859438240527, 'val_loss': 0.647103238105774, 'test_acc': 0.81}
{'fold': 9, 'epoch': 191, 'train_loss': 0.19650350604206324, 'val_loss': 0.752988896369934, 'test_acc': 0.83}
{'fold': 9, 'epoch': 192, 'train_loss': 0.20433868635445834, 'val_loss': 0.8183259439468383, 'test_acc': 0.82}
{'fold': 9, 'epoch': 193, 'train_loss': 0.20179366432130336, 'val_loss': 0.8397427797317505, 'test_acc': 0.82}
{'fold': 9, 'epoch': 194, 'train_loss': 0.22106906175613403, 'val_loss': 0.61570143699646, 'test_acc': 0.83}
{'fold': 9, 'epoch': 195, 'train_loss': 0.19813964515924454, 'val_loss': 0.8181596040725708, 'test_acc': 0.83}
{'fold': 9, 'epoch': 196, 'train_loss': 0.2153274893760681, 'val_loss': 1.0003452110290527, 'test_acc': 0.83}
{'fold': 9, 'epoch': 197, 'train_loss': 0.1972799923270941, 'val_loss': 0.6807111310958862, 'test_acc': 0.83}
{'fold': 9, 'epoch': 198, 'train_loss': 0.20760651230812072, 'val_loss': 0.7094354820251465, 'test_acc': 0.83}
{'fold': 9, 'epoch': 199, 'train_loss': 0.2203241540119052, 'val_loss': 0.7643323802947998, 'test_acc': 0.83}
{'fold': 9, 'epoch': 200, 'train_loss': 0.19776430763304234, 'val_loss': 0.5476214790344238, 'test_acc': 0.83}
Val Loss: 0.4057, Test Accuracy: 0.792 ± 0.062, Duration: 76.387
Best result - 0.792 ± 0.062
--
REDDIT-BINARY - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.5939075995236636, 'val_loss': 0.7562849724292755, 'test_acc': 0.64}
{'fold': 9, 'epoch': 2, 'train_loss': 0.4905897554010153, 'val_loss': 0.6179897260665893, 'test_acc': 0.73}
{'fold': 9, 'epoch': 3, 'train_loss': 0.46453862339258195, 'val_loss': 0.5398408222198486, 'test_acc': 0.73}
{'fold': 9, 'epoch': 4, 'train_loss': 0.4418449677526951, 'val_loss': 0.6792139828205108, 'test_acc': 0.76}
{'fold': 9, 'epoch': 5, 'train_loss': 0.42213349752128126, 'val_loss': 0.6159926968812942, 'test_acc': 0.755}
{'fold': 9, 'epoch': 6, 'train_loss': 0.42079431302845477, 'val_loss': 0.46816582441329957, 'test_acc': 0.76}
{'fold': 9, 'epoch': 7, 'train_loss': 0.3988143544644117, 'val_loss': 0.5539524030685424, 'test_acc': 0.77}
{'fold': 9, 'epoch': 8, 'train_loss': 0.3865153849124908, 'val_loss': 0.5086169612407684, 'test_acc': 0.765}
{'fold': 9, 'epoch': 9, 'train_loss': 0.3831436488777399, 'val_loss': 0.4834101438522339, 'test_acc': 0.79}
{'fold': 9, 'epoch': 10, 'train_loss': 0.37709219828248025, 'val_loss': 0.36801034450531006, 'test_acc': 0.815}
{'fold': 9, 'epoch': 11, 'train_loss': 0.3685867179185152, 'val_loss': 0.41088634729385376, 'test_acc': 0.825}
{'fold': 9, 'epoch': 12, 'train_loss': 0.35335031114518645, 'val_loss': 0.36560410499572754, 'test_acc': 0.84}
{'fold': 9, 'epoch': 13, 'train_loss': 0.335706977173686, 'val_loss': 0.3832605218887329, 'test_acc': 0.82}
{'fold': 9, 'epoch': 14, 'train_loss': 0.34119553603231906, 'val_loss': 0.37502681612968447, 'test_acc': 0.795}
{'fold': 9, 'epoch': 15, 'train_loss': 0.33844946715980767, 'val_loss': 0.3668011027574539, 'test_acc': 0.8}
{'fold': 9, 'epoch': 16, 'train_loss': 0.33935663755983114, 'val_loss': 0.376390306353569, 'test_acc': 0.8}
{'fold': 9, 'epoch': 17, 'train_loss': 0.3517656110227108, 'val_loss': 0.33964136481285095, 'test_acc': 0.835}
{'fold': 9, 'epoch': 18, 'train_loss': 0.34364860951900483, 'val_loss': 0.33230136811733246, 'test_acc': 0.815}
{'fold': 9, 'epoch': 19, 'train_loss': 0.3109604811295867, 'val_loss': 0.37527531653642654, 'test_acc': 0.8}
{'fold': 9, 'epoch': 20, 'train_loss': 0.3218308001756668, 'val_loss': 0.33100263237953187, 'test_acc': 0.815}
{'fold': 9, 'epoch': 21, 'train_loss': 0.3060994740575552, 'val_loss': 0.374289231300354, 'test_acc': 0.835}
{'fold': 9, 'epoch': 22, 'train_loss': 0.3131351929157972, 'val_loss': 0.3363680708408356, 'test_acc': 0.82}
{'fold': 9, 'epoch': 23, 'train_loss': 0.3144299305975437, 'val_loss': 0.368876639008522, 'test_acc': 0.83}
{'fold': 9, 'epoch': 24, 'train_loss': 0.3449026634916663, 'val_loss': 0.3536724007129669, 'test_acc': 0.8}
{'fold': 9, 'epoch': 25, 'train_loss': 0.31108825020492076, 'val_loss': 0.35541839241981504, 'test_acc': 0.815}
{'fold': 9, 'epoch': 26, 'train_loss': 0.3096049003303051, 'val_loss': 0.3121274554729462, 'test_acc': 0.815}
{'fold': 9, 'epoch': 27, 'train_loss': 0.29220238961279393, 'val_loss': 0.3049888843297958, 'test_acc': 0.825}
{'fold': 9, 'epoch': 28, 'train_loss': 0.2987063605338335, 'val_loss': 0.30216081500053404, 'test_acc': 0.835}
{'fold': 9, 'epoch': 29, 'train_loss': 0.3064734973013401, 'val_loss': 0.33628335952758787, 'test_acc': 0.835}
{'fold': 9, 'epoch': 30, 'train_loss': 0.30836831219494343, 'val_loss': 0.33247849106788635, 'test_acc': 0.84}
{'fold': 9, 'epoch': 31, 'train_loss': 0.323293730430305, 'val_loss': 0.37204963743686675, 'test_acc': 0.835}
{'fold': 9, 'epoch': 32, 'train_loss': 0.3434939317405224, 'val_loss': 0.30832952439785005, 'test_acc': 0.8}
{'fold': 9, 'epoch': 33, 'train_loss': 0.31857015937566757, 'val_loss': 0.30236813008785246, 'test_acc': 0.835}
{'fold': 9, 'epoch': 34, 'train_loss': 0.29878289010375736, 'val_loss': 0.34075852572917936, 'test_acc': 0.83}
{'fold': 9, 'epoch': 35, 'train_loss': 0.2802755817770958, 'val_loss': 0.2959267592430115, 'test_acc': 0.835}
{'fold': 9, 'epoch': 36, 'train_loss': 0.26109667364507916, 'val_loss': 0.35784410119056703, 'test_acc': 0.84}
{'fold': 9, 'epoch': 37, 'train_loss': 0.2984067464247346, 'val_loss': 0.3613070118427277, 'test_acc': 0.835}
{'fold': 9, 'epoch': 38, 'train_loss': 0.2579201579093933, 'val_loss': 0.3522408878803253, 'test_acc': 0.835}
{'fold': 9, 'epoch': 39, 'train_loss': 0.2668129591271281, 'val_loss': 0.3334620332717895, 'test_acc': 0.83}
{'fold': 9, 'epoch': 40, 'train_loss': 0.26214621365070345, 'val_loss': 0.2869637644290924, 'test_acc': 0.865}
{'fold': 9, 'epoch': 41, 'train_loss': 0.274341250397265, 'val_loss': 0.33371766805648806, 'test_acc': 0.81}
{'fold': 9, 'epoch': 42, 'train_loss': 0.27082985881716015, 'val_loss': 0.38082210540771483, 'test_acc': 0.835}
{'fold': 9, 'epoch': 43, 'train_loss': 0.2805472664535046, 'val_loss': 0.32241002380847933, 'test_acc': 0.835}
{'fold': 9, 'epoch': 44, 'train_loss': 0.28649512846022845, 'val_loss': 0.292892330288887, 'test_acc': 0.83}
{'fold': 9, 'epoch': 45, 'train_loss': 0.28758334517478945, 'val_loss': 0.2941013300418854, 'test_acc': 0.83}
{'fold': 9, 'epoch': 46, 'train_loss': 0.2850484296679497, 'val_loss': 0.3411627662181854, 'test_acc': 0.83}
{'fold': 9, 'epoch': 47, 'train_loss': 0.26386719923466445, 'val_loss': 0.3143648386001587, 'test_acc': 0.825}
{'fold': 9, 'epoch': 48, 'train_loss': 0.27114946879446505, 'val_loss': 0.33419471740722656, 'test_acc': 0.815}
{'fold': 9, 'epoch': 49, 'train_loss': 0.25583502780646084, 'val_loss': 0.32643490910530093, 'test_acc': 0.82}
{'fold': 9, 'epoch': 50, 'train_loss': 0.25417418163269756, 'val_loss': 0.294864239692688, 'test_acc': 0.815}
{'fold': 9, 'epoch': 51, 'train_loss': 0.2415550945326686, 'val_loss': 0.2861823827028275, 'test_acc': 0.86}
{'fold': 9, 'epoch': 52, 'train_loss': 0.2619381805881858, 'val_loss': 0.27711448311805725, 'test_acc': 0.83}
{'fold': 9, 'epoch': 53, 'train_loss': 0.25207157991826534, 'val_loss': 0.2589138805866241, 'test_acc': 0.855}
{'fold': 9, 'epoch': 54, 'train_loss': 0.25000803228467705, 'val_loss': 0.3038086700439453, 'test_acc': 0.845}
{'fold': 9, 'epoch': 55, 'train_loss': 0.23317886851727962, 'val_loss': 0.286875524520874, 'test_acc': 0.84}
{'fold': 9, 'epoch': 56, 'train_loss': 0.2341670266352594, 'val_loss': 0.24372706532478333, 'test_acc': 0.86}
{'fold': 9, 'epoch': 57, 'train_loss': 0.24205592311918736, 'val_loss': 0.2797187727689743, 'test_acc': 0.82}
{'fold': 9, 'epoch': 58, 'train_loss': 0.2736857315525413, 'val_loss': 0.3329243052005768, 'test_acc': 0.83}
{'fold': 9, 'epoch': 59, 'train_loss': 0.23412048835307359, 'val_loss': 0.2877165269851685, 'test_acc': 0.865}
{'fold': 9, 'epoch': 60, 'train_loss': 0.2184205250814557, 'val_loss': 0.2590011405944824, 'test_acc': 0.865}
{'fold': 9, 'epoch': 61, 'train_loss': 0.21508327182382345, 'val_loss': 0.25178913831710814, 'test_acc': 0.865}
{'fold': 9, 'epoch': 62, 'train_loss': 0.22765600513666867, 'val_loss': 0.29504734188318255, 'test_acc': 0.845}
{'fold': 9, 'epoch': 63, 'train_loss': 0.25273897480219604, 'val_loss': 0.3353439509868622, 'test_acc': 0.865}
{'fold': 9, 'epoch': 64, 'train_loss': 0.2300188305787742, 'val_loss': 0.27075793385505675, 'test_acc': 0.865}
{'fold': 9, 'epoch': 65, 'train_loss': 0.24706023279577494, 'val_loss': 0.2455703568458557, 'test_acc': 0.855}
{'fold': 9, 'epoch': 66, 'train_loss': 0.22027721581980586, 'val_loss': 0.2469359540939331, 'test_acc': 0.835}
{'fold': 9, 'epoch': 67, 'train_loss': 0.2306690954603255, 'val_loss': 0.2914254069328308, 'test_acc': 0.825}
{'fold': 9, 'epoch': 68, 'train_loss': 0.21650392673909663, 'val_loss': 0.22332627534866334, 'test_acc': 0.875}
{'fold': 9, 'epoch': 69, 'train_loss': 0.2109513906762004, 'val_loss': 0.23935546875, 'test_acc': 0.85}
{'fold': 9, 'epoch': 70, 'train_loss': 0.21491263508796693, 'val_loss': 0.24898219108581543, 'test_acc': 0.865}
{'fold': 9, 'epoch': 71, 'train_loss': 0.20435703741386532, 'val_loss': 0.24320364236831665, 'test_acc': 0.85}
{'fold': 9, 'epoch': 72, 'train_loss': 0.22953577432781458, 'val_loss': 0.2247270131111145, 'test_acc': 0.87}
{'fold': 9, 'epoch': 73, 'train_loss': 0.20142703633755446, 'val_loss': 0.2564273500442505, 'test_acc': 0.865}
{'fold': 9, 'epoch': 74, 'train_loss': 0.2024285150691867, 'val_loss': 0.23501561641693114, 'test_acc': 0.855}
{'fold': 9, 'epoch': 75, 'train_loss': 0.20840302156284451, 'val_loss': 0.2404997479915619, 'test_acc': 0.875}
{'fold': 9, 'epoch': 76, 'train_loss': 0.21405420005321502, 'val_loss': 0.29620025157928465, 'test_acc': 0.85}
{'fold': 9, 'epoch': 77, 'train_loss': 0.20565710673108697, 'val_loss': 0.2745190191268921, 'test_acc': 0.87}
{'fold': 9, 'epoch': 78, 'train_loss': 0.21644713198766113, 'val_loss': 0.2942827820777893, 'test_acc': 0.865}
{'fold': 9, 'epoch': 79, 'train_loss': 0.20417736740782857, 'val_loss': 0.2617748987674713, 'test_acc': 0.855}
{'fold': 9, 'epoch': 80, 'train_loss': 0.19518531830981373, 'val_loss': 0.30086507558822634, 'test_acc': 0.835}
{'fold': 9, 'epoch': 81, 'train_loss': 0.19876703806221485, 'val_loss': 0.2559318423271179, 'test_acc': 0.87}
{'fold': 9, 'epoch': 82, 'train_loss': 0.19376811338588595, 'val_loss': 0.2807511043548584, 'test_acc': 0.87}
{'fold': 9, 'epoch': 83, 'train_loss': 0.21681282687932252, 'val_loss': 0.31541913747787476, 'test_acc': 0.835}
{'fold': 9, 'epoch': 84, 'train_loss': 0.21002925988286733, 'val_loss': 0.27980865716934206, 'test_acc': 0.85}
{'fold': 9, 'epoch': 85, 'train_loss': 0.18874328201636673, 'val_loss': 0.24785271644592285, 'test_acc': 0.875}
{'fold': 9, 'epoch': 86, 'train_loss': 0.17105198921635748, 'val_loss': 0.23958406686782838, 'test_acc': 0.885}
{'fold': 9, 'epoch': 87, 'train_loss': 0.1918335637077689, 'val_loss': 0.2914206314086914, 'test_acc': 0.87}
{'fold': 9, 'epoch': 88, 'train_loss': 0.17900424916297197, 'val_loss': 0.2915114736557007, 'test_acc': 0.875}
{'fold': 9, 'epoch': 89, 'train_loss': 0.17791368970647453, 'val_loss': 0.2462994313240051, 'test_acc': 0.88}
{'fold': 9, 'epoch': 90, 'train_loss': 0.16670459201559423, 'val_loss': 0.26673432350158693, 'test_acc': 0.89}
{'fold': 9, 'epoch': 91, 'train_loss': 0.19274053107947112, 'val_loss': 0.22704292297363282, 'test_acc': 0.87}
{'fold': 9, 'epoch': 92, 'train_loss': 0.20065384870395064, 'val_loss': 0.3024595308303833, 'test_acc': 0.86}
{'fold': 9, 'epoch': 93, 'train_loss': 0.20917491894215345, 'val_loss': 0.3649247634410858, 'test_acc': 0.84}
{'fold': 9, 'epoch': 94, 'train_loss': 0.23928428217768669, 'val_loss': 0.18058260440826415, 'test_acc': 0.88}
{'fold': 9, 'epoch': 95, 'train_loss': 0.18968791281804442, 'val_loss': 0.22638095617294313, 'test_acc': 0.89}
{'fold': 9, 'epoch': 96, 'train_loss': 0.18283437080681325, 'val_loss': 0.32818878650665284, 'test_acc': 0.86}
{'fold': 9, 'epoch': 97, 'train_loss': 0.15697363372892142, 'val_loss': 0.299510703086853, 'test_acc': 0.865}
{'fold': 9, 'epoch': 98, 'train_loss': 0.15317756906151772, 'val_loss': 0.27230987548828123, 'test_acc': 0.86}
{'fold': 9, 'epoch': 99, 'train_loss': 0.16450356002897024, 'val_loss': 0.2775228881835938, 'test_acc': 0.89}
{'fold': 9, 'epoch': 100, 'train_loss': 0.15608530277386307, 'val_loss': 0.27385478019714354, 'test_acc': 0.875}
{'fold': 9, 'epoch': 101, 'train_loss': 0.16146538248285652, 'val_loss': 0.32224733352661133, 'test_acc': 0.875}
{'fold': 9, 'epoch': 102, 'train_loss': 0.15425715716555716, 'val_loss': 0.2998478412628174, 'test_acc': 0.875}
{'fold': 9, 'epoch': 103, 'train_loss': 0.15685499794781207, 'val_loss': 0.29914643049240114, 'test_acc': 0.89}
{'fold': 9, 'epoch': 104, 'train_loss': 0.13965261224657297, 'val_loss': 0.2942343330383301, 'test_acc': 0.885}
{'fold': 9, 'epoch': 105, 'train_loss': 0.14209954342804848, 'val_loss': 0.2877511024475098, 'test_acc': 0.87}
{'fold': 9, 'epoch': 106, 'train_loss': 0.15464090034365655, 'val_loss': 0.3075690174102783, 'test_acc': 0.88}
{'fold': 9, 'epoch': 107, 'train_loss': 0.13790993997827172, 'val_loss': 0.3405907678604126, 'test_acc': 0.88}
{'fold': 9, 'epoch': 108, 'train_loss': 0.1302498184144497, 'val_loss': 0.322423415184021, 'test_acc': 0.88}
{'fold': 9, 'epoch': 109, 'train_loss': 0.13381157184485346, 'val_loss': 0.3384635639190674, 'test_acc': 0.87}
{'fold': 9, 'epoch': 110, 'train_loss': 0.13867528801783918, 'val_loss': 0.3195051336288452, 'test_acc': 0.875}
{'fold': 9, 'epoch': 111, 'train_loss': 0.14204811230301856, 'val_loss': 0.3359722185134888, 'test_acc': 0.87}
{'fold': 9, 'epoch': 112, 'train_loss': 0.14918663641437888, 'val_loss': 0.32890938758850097, 'test_acc': 0.875}
{'fold': 9, 'epoch': 113, 'train_loss': 0.14960333770141004, 'val_loss': 0.2938790655136108, 'test_acc': 0.885}
{'fold': 9, 'epoch': 114, 'train_loss': 0.137931649107486, 'val_loss': 0.3090296125411987, 'test_acc': 0.885}
{'fold': 9, 'epoch': 115, 'train_loss': 0.13620028346776963, 'val_loss': 0.2925303840637207, 'test_acc': 0.87}
{'fold': 9, 'epoch': 116, 'train_loss': 0.15033580167219043, 'val_loss': 0.2758333396911621, 'test_acc': 0.87}
{'fold': 9, 'epoch': 117, 'train_loss': 0.13986432598903775, 'val_loss': 0.29189553499221804, 'test_acc': 0.905}
{'fold': 9, 'epoch': 118, 'train_loss': 0.14971019024960697, 'val_loss': 0.3099841547012329, 'test_acc': 0.89}
{'fold': 9, 'epoch': 119, 'train_loss': 0.13639034437946976, 'val_loss': 0.24529057502746582, 'test_acc': 0.855}
{'fold': 9, 'epoch': 120, 'train_loss': 0.16056272126734256, 'val_loss': 0.27362746238708496, 'test_acc': 0.865}
{'fold': 9, 'epoch': 121, 'train_loss': 0.14857897409237922, 'val_loss': 0.294166111946106, 'test_acc': 0.9}
{'fold': 9, 'epoch': 122, 'train_loss': 0.14286577412858606, 'val_loss': 0.2955563735961914, 'test_acc': 0.87}
{'fold': 9, 'epoch': 123, 'train_loss': 0.13236780310980975, 'val_loss': 0.35553943395614623, 'test_acc': 0.855}
{'fold': 9, 'epoch': 124, 'train_loss': 0.15311852642334997, 'val_loss': 0.3119721174240112, 'test_acc': 0.905}
{'fold': 9, 'epoch': 125, 'train_loss': 0.1406612484715879, 'val_loss': 0.30785571575164794, 'test_acc': 0.89}
{'fold': 9, 'epoch': 126, 'train_loss': 0.1385534680914134, 'val_loss': 0.33790179252624514, 'test_acc': 0.9}
{'fold': 9, 'epoch': 127, 'train_loss': 0.13878672043792903, 'val_loss': 0.3195505237579346, 'test_acc': 0.87}
{'fold': 9, 'epoch': 128, 'train_loss': 0.12864986164495348, 'val_loss': 0.2932011318206787, 'test_acc': 0.895}
{'fold': 9, 'epoch': 129, 'train_loss': 0.13512683408334852, 'val_loss': 0.33342183589935304, 'test_acc': 0.88}
{'fold': 9, 'epoch': 130, 'train_loss': 0.13509271130897105, 'val_loss': 0.339873685836792, 'test_acc': 0.885}
{'fold': 9, 'epoch': 131, 'train_loss': 0.1273560268804431, 'val_loss': 0.36604272842407226, 'test_acc': 0.89}
{'fold': 9, 'epoch': 132, 'train_loss': 0.12488435036502779, 'val_loss': 0.31293673038482667, 'test_acc': 0.88}
{'fold': 9, 'epoch': 133, 'train_loss': 0.12661690223030747, 'val_loss': 0.3038076114654541, 'test_acc': 0.905}
{'fold': 9, 'epoch': 134, 'train_loss': 0.11724081193096936, 'val_loss': 0.32035140991210936, 'test_acc': 0.895}
{'fold': 9, 'epoch': 135, 'train_loss': 0.13321862458251416, 'val_loss': 0.2998840856552124, 'test_acc': 0.895}
{'fold': 9, 'epoch': 136, 'train_loss': 0.15014972793869674, 'val_loss': 0.33930238485336306, 'test_acc': 0.875}
{'fold': 9, 'epoch': 137, 'train_loss': 0.1372744832187891, 'val_loss': 0.306788477897644, 'test_acc': 0.875}
{'fold': 9, 'epoch': 138, 'train_loss': 0.12328460523858667, 'val_loss': 0.3048977756500244, 'test_acc': 0.895}
{'fold': 9, 'epoch': 139, 'train_loss': 0.1411352083552629, 'val_loss': 0.31029770374298093, 'test_acc': 0.87}
{'fold': 9, 'epoch': 140, 'train_loss': 0.1395641475915909, 'val_loss': 0.36392988681793215, 'test_acc': 0.875}
{'fold': 9, 'epoch': 141, 'train_loss': 0.1291990099940449, 'val_loss': 0.3421721410751343, 'test_acc': 0.885}
{'fold': 9, 'epoch': 142, 'train_loss': 0.12860765694640577, 'val_loss': 0.3554632472991943, 'test_acc': 0.89}
{'fold': 9, 'epoch': 143, 'train_loss': 0.1251681531779468, 'val_loss': 0.352865047454834, 'test_acc': 0.895}
{'fold': 9, 'epoch': 144, 'train_loss': 0.13562380522489548, 'val_loss': 0.32822447299957275, 'test_acc': 0.88}
{'fold': 9, 'epoch': 145, 'train_loss': 0.11930623957887292, 'val_loss': 0.37223531246185304, 'test_acc': 0.89}
{'fold': 9, 'epoch': 146, 'train_loss': 0.13014416811056434, 'val_loss': 0.3831541585922241, 'test_acc': 0.895}
{'fold': 9, 'epoch': 147, 'train_loss': 0.13882052013650537, 'val_loss': 0.3403924560546875, 'test_acc': 0.875}
{'fold': 9, 'epoch': 148, 'train_loss': 0.13559427277650685, 'val_loss': 0.3575940752029419, 'test_acc': 0.895}
{'fold': 9, 'epoch': 149, 'train_loss': 0.13618058571591973, 'val_loss': 0.34653244495391844, 'test_acc': 0.89}
{'fold': 9, 'epoch': 150, 'train_loss': 0.1479016099125147, 'val_loss': 0.37678038597106933, 'test_acc': 0.9}
{'fold': 9, 'epoch': 151, 'train_loss': 0.11409554290585219, 'val_loss': 0.33136649131774903, 'test_acc': 0.885}
{'fold': 9, 'epoch': 152, 'train_loss': 0.1318683710647747, 'val_loss': 0.3340168046951294, 'test_acc': 0.9}
{'fold': 9, 'epoch': 153, 'train_loss': 0.1234882791992277, 'val_loss': 0.37061444282531736, 'test_acc': 0.9}
{'fold': 9, 'epoch': 154, 'train_loss': 0.1214192686136812, 'val_loss': 0.37499398708343507, 'test_acc': 0.88}
{'fold': 9, 'epoch': 155, 'train_loss': 0.19877482261508703, 'val_loss': 0.24385674476623534, 'test_acc': 0.895}
{'fold': 9, 'epoch': 156, 'train_loss': 0.14063074942678214, 'val_loss': 0.3125536060333252, 'test_acc': 0.905}
{'fold': 9, 'epoch': 157, 'train_loss': 0.1431580181233585, 'val_loss': 0.36081279516220094, 'test_acc': 0.9}
{'fold': 9, 'epoch': 158, 'train_loss': 0.12334899790585041, 'val_loss': 0.3536698079109192, 'test_acc': 0.88}
{'fold': 9, 'epoch': 159, 'train_loss': 0.11670686092693358, 'val_loss': 0.3620255899429321, 'test_acc': 0.88}
{'fold': 9, 'epoch': 160, 'train_loss': 0.1148640913888812, 'val_loss': 0.32585500240325926, 'test_acc': 0.905}
{'fold': 9, 'epoch': 161, 'train_loss': 0.10997408288531005, 'val_loss': 0.36639650344848634, 'test_acc': 0.87}
{'fold': 9, 'epoch': 162, 'train_loss': 0.10410872120410204, 'val_loss': 0.39889429569244383, 'test_acc': 0.89}
{'fold': 9, 'epoch': 163, 'train_loss': 0.1042577053187415, 'val_loss': 0.3387910890579224, 'test_acc': 0.895}
{'fold': 9, 'epoch': 164, 'train_loss': 0.1272946442477405, 'val_loss': 0.32386537313461305, 'test_acc': 0.895}
{'fold': 9, 'epoch': 165, 'train_loss': 0.1282328384462744, 'val_loss': 0.3816785979270935, 'test_acc': 0.89}
{'fold': 9, 'epoch': 166, 'train_loss': 0.10714438990689815, 'val_loss': 0.3546166038513184, 'test_acc': 0.89}
{'fold': 9, 'epoch': 167, 'train_loss': 0.10402505692327395, 'val_loss': 0.367349705696106, 'test_acc': 0.885}
{'fold': 9, 'epoch': 168, 'train_loss': 0.12275014580227435, 'val_loss': 0.42120670795440673, 'test_acc': 0.895}
{'fold': 9, 'epoch': 169, 'train_loss': 0.11479741733055562, 'val_loss': 0.35051157951354983, 'test_acc': 0.885}
{'fold': 9, 'epoch': 170, 'train_loss': 0.11125448720995337, 'val_loss': 0.36128480672836305, 'test_acc': 0.91}
{'fold': 9, 'epoch': 171, 'train_loss': 0.11642275359481573, 'val_loss': 0.33300280570983887, 'test_acc': 0.895}
{'fold': 9, 'epoch': 172, 'train_loss': 0.11969781136140227, 'val_loss': 0.39031281471252444, 'test_acc': 0.875}
{'fold': 9, 'epoch': 173, 'train_loss': 0.11623308346606791, 'val_loss': 0.31804469108581546, 'test_acc': 0.89}
{'fold': 9, 'epoch': 174, 'train_loss': 0.11212919722311199, 'val_loss': 0.29068167209625245, 'test_acc': 0.905}
{'fold': 9, 'epoch': 175, 'train_loss': 0.10860432633198798, 'val_loss': 0.286419677734375, 'test_acc': 0.89}
{'fold': 9, 'epoch': 176, 'train_loss': 0.100538401119411, 'val_loss': 0.36191770553588865, 'test_acc': 0.88}
{'fold': 9, 'epoch': 177, 'train_loss': 0.11630497518926859, 'val_loss': 0.3853441309928894, 'test_acc': 0.9}
{'fold': 9, 'epoch': 178, 'train_loss': 0.12384737189859152, 'val_loss': 0.3149257516860962, 'test_acc': 0.895}
{'fold': 9, 'epoch': 179, 'train_loss': 0.12806513300165534, 'val_loss': 0.31686500549316404, 'test_acc': 0.89}
{'fold': 9, 'epoch': 180, 'train_loss': 0.1228720667771995, 'val_loss': 0.3088518047332764, 'test_acc': 0.87}
{'fold': 9, 'epoch': 181, 'train_loss': 0.1195957574993372, 'val_loss': 0.3304715013504028, 'test_acc': 0.88}
{'fold': 9, 'epoch': 182, 'train_loss': 0.10617977604269982, 'val_loss': 0.3773584604263306, 'test_acc': 0.885}
{'fold': 9, 'epoch': 183, 'train_loss': 0.13574505914002657, 'val_loss': 0.2722164058685303, 'test_acc': 0.845}
{'fold': 9, 'epoch': 184, 'train_loss': 0.1633856394328177, 'val_loss': 0.3109532117843628, 'test_acc': 0.87}
{'fold': 9, 'epoch': 185, 'train_loss': 0.15086564035154879, 'val_loss': 0.2829738140106201, 'test_acc': 0.885}
{'fold': 9, 'epoch': 186, 'train_loss': 0.13292055251076818, 'val_loss': 0.3500996685028076, 'test_acc': 0.915}
{'fold': 9, 'epoch': 187, 'train_loss': 0.11701666454318911, 'val_loss': 0.40712546825408935, 'test_acc': 0.905}
{'fold': 9, 'epoch': 188, 'train_loss': 0.12094462676905096, 'val_loss': 0.4147781848907471, 'test_acc': 0.875}
{'fold': 9, 'epoch': 189, 'train_loss': 0.11885168952867389, 'val_loss': 0.31860382556915284, 'test_acc': 0.895}
{'fold': 9, 'epoch': 190, 'train_loss': 0.10752539965324104, 'val_loss': 0.2971390724182129, 'test_acc': 0.88}
{'fold': 9, 'epoch': 191, 'train_loss': 0.23195139481686056, 'val_loss': 0.21748294353485106, 'test_acc': 0.885}
{'fold': 9, 'epoch': 192, 'train_loss': 0.15495011610910298, 'val_loss': 0.258777973651886, 'test_acc': 0.87}
{'fold': 9, 'epoch': 193, 'train_loss': 0.14463219633325936, 'val_loss': 0.3106080722808838, 'test_acc': 0.895}
{'fold': 9, 'epoch': 194, 'train_loss': 0.11509970976039767, 'val_loss': 0.35090128898620604, 'test_acc': 0.875}
{'fold': 9, 'epoch': 195, 'train_loss': 0.11069663837552071, 'val_loss': 0.34988898754119874, 'test_acc': 0.88}
{'fold': 9, 'epoch': 196, 'train_loss': 0.11377809261903167, 'val_loss': 0.3319671535491943, 'test_acc': 0.87}
{'fold': 9, 'epoch': 197, 'train_loss': 0.10713162616593763, 'val_loss': 0.30995855331420896, 'test_acc': 0.87}
{'fold': 9, 'epoch': 198, 'train_loss': 0.11044651339761913, 'val_loss': 0.32339498043060305, 'test_acc': 0.87}
{'fold': 9, 'epoch': 199, 'train_loss': 0.11367961028590798, 'val_loss': 0.2767611932754517, 'test_acc': 0.89}
{'fold': 9, 'epoch': 200, 'train_loss': 0.09146298796404154, 'val_loss': 0.321445426940918, 'test_acc': 0.88}
Val Loss: 0.2826, Test Accuracy: 0.872 ± 0.025, Duration: 210.854
Best result - 0.872 ± 0.025
--
DD - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.5645088160694656, 'val_loss': 0.4913308885362413, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 2, 'train_loss': 0.43619177225282635, 'val_loss': 0.42555510692107373, 'test_acc': 0.811965811965812}
{'fold': 9, 'epoch': 3, 'train_loss': 0.3443431999471228, 'val_loss': 0.3960983243762937, 'test_acc': 0.7948717948717948}
{'fold': 9, 'epoch': 4, 'train_loss': 0.31461258087370353, 'val_loss': 0.36534522945045406, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 5, 'train_loss': 0.28954216869453253, 'val_loss': 0.4025377452882946, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 6, 'train_loss': 0.2727571300909681, 'val_loss': 0.33432072044437766, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 7, 'train_loss': 0.22690743700427524, 'val_loss': 0.3589424557156033, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 8, 'train_loss': 0.2118760998029325, 'val_loss': 0.3336387699485844, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 9, 'train_loss': 0.2218775574623023, 'val_loss': 0.3467574486365685, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 10, 'train_loss': 0.1869557266265659, 'val_loss': 0.30084422510913295, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 11, 'train_loss': 0.16642827168107033, 'val_loss': 0.32266530420026207, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 12, 'train_loss': 0.14723251602154666, 'val_loss': 0.3314962142553085, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 13, 'train_loss': 0.13746313259826373, 'val_loss': 0.3329572270059178, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 14, 'train_loss': 0.13781010356368656, 'val_loss': 0.33507750584528995, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 15, 'train_loss': 0.11226521010936821, 'val_loss': 0.3185144734178853, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 16, 'train_loss': 0.12215663729456522, 'val_loss': 0.3674090295775324, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 17, 'train_loss': 0.15740563456853063, 'val_loss': 0.3333736729418111, 'test_acc': 0.9401709401709402}
{'fold': 9, 'epoch': 18, 'train_loss': 0.11967992567915027, 'val_loss': 0.3146785181811732, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 19, 'train_loss': 0.12674207786508537, 'val_loss': 0.32791561550564235, 'test_acc': 0.9487179487179487}
{'fold': 9, 'epoch': 20, 'train_loss': 0.09483357543378311, 'val_loss': 0.24787039634508964, 'test_acc': 0.9487179487179487}
{'fold': 9, 'epoch': 21, 'train_loss': 0.0677064457687281, 'val_loss': 0.3089896960136218, 'test_acc': 0.9487179487179487}
{'fold': 9, 'epoch': 22, 'train_loss': 0.10332687858144864, 'val_loss': 0.3257850092700404, 'test_acc': 0.9572649572649573}
{'fold': 9, 'epoch': 23, 'train_loss': 0.08842886462255176, 'val_loss': 0.33513863066322785, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 24, 'train_loss': 0.1008074685962776, 'val_loss': 0.32597774929470485, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 25, 'train_loss': 0.07876319360871942, 'val_loss': 0.3056801526974409, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 26, 'train_loss': 0.0797663897560057, 'val_loss': 0.3010215514745468, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 27, 'train_loss': 0.08337199773986713, 'val_loss': 0.28278136457133496, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 28, 'train_loss': 0.08790927846773954, 'val_loss': 0.37424351618840146, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 29, 'train_loss': 0.08321604500445774, 'val_loss': 0.3835661798460871, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 30, 'train_loss': 0.08774756294501534, 'val_loss': 0.35240126063681054, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 31, 'train_loss': 0.05614520392333299, 'val_loss': 0.4004234574798845, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 32, 'train_loss': 0.08998575509857323, 'val_loss': 0.3988063029753856, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 33, 'train_loss': 0.08008373244981265, 'val_loss': 0.4237967271071214, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 34, 'train_loss': 0.0865473180133218, 'val_loss': 0.37098125718597674, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 35, 'train_loss': 0.07440886655980247, 'val_loss': 0.4219026687817696, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 36, 'train_loss': 0.06451391413515845, 'val_loss': 0.3632127158662193, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 37, 'train_loss': 0.10042840378137968, 'val_loss': 0.32372761587811333, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 38, 'train_loss': 0.09581177006870256, 'val_loss': 0.25328354957776195, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 39, 'train_loss': 0.04675921592151083, 'val_loss': 0.2940639104598608, 'test_acc': 0.9401709401709402}
{'fold': 9, 'epoch': 40, 'train_loss': 0.053420551010739, 'val_loss': 0.33129969213762855, 'test_acc': 0.9487179487179487}
{'fold': 9, 'epoch': 41, 'train_loss': 0.03973871085129804, 'val_loss': 0.3683130394699227, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 42, 'train_loss': 0.05020280198146731, 'val_loss': 0.38473963941264355, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 43, 'train_loss': 0.06462812141152256, 'val_loss': 0.37901679471007776, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 44, 'train_loss': 0.08318038879088678, 'val_loss': 0.5827481522519364, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 45, 'train_loss': 0.107828636858928, 'val_loss': 0.42042723272600746, 'test_acc': 0.9487179487179487}
{'fold': 9, 'epoch': 46, 'train_loss': 0.07678489918994197, 'val_loss': 0.3503818838005392, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 47, 'train_loss': 0.07014818800518573, 'val_loss': 0.41994195921808225, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 48, 'train_loss': 0.07497470184245099, 'val_loss': 0.41002296382545406, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 49, 'train_loss': 0.040693337618673255, 'val_loss': 0.4854761270376352, 'test_acc': 0.9401709401709402}
{'fold': 9, 'epoch': 50, 'train_loss': 0.04035998426251492, 'val_loss': 0.421041749481462, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 51, 'train_loss': 0.04493848370956415, 'val_loss': 0.4346814929929554, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 52, 'train_loss': 0.04479974990915823, 'val_loss': 0.458918816004044, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 53, 'train_loss': 0.04233381896224504, 'val_loss': 0.4572462587275057, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 54, 'train_loss': 0.037737744524917106, 'val_loss': 0.4914926545232789, 'test_acc': 0.9401709401709402}
{'fold': 9, 'epoch': 55, 'train_loss': 0.06100667135048089, 'val_loss': 0.44041293706649387, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 56, 'train_loss': 0.048182010445428095, 'val_loss': 0.5472572848328159, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 57, 'train_loss': 0.06381483447848488, 'val_loss': 0.34058399689503205, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 58, 'train_loss': 0.051870502389462315, 'val_loss': 0.3398595467591897, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 59, 'train_loss': 0.04405316876610583, 'val_loss': 0.3780202295026209, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 60, 'train_loss': 0.037884842075596926, 'val_loss': 0.4230843405438285, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 61, 'train_loss': 0.04568631529523912, 'val_loss': 0.4941063416309846, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 62, 'train_loss': 0.059060608701764654, 'val_loss': 0.465269284370618, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 63, 'train_loss': 0.04878980844573639, 'val_loss': 0.484055535406129, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 64, 'train_loss': 0.07773121913610878, 'val_loss': 0.43331387511685365, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 65, 'train_loss': 0.04435351216313192, 'val_loss': 0.37534886547642893, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 66, 'train_loss': 0.04542034822595069, 'val_loss': 0.4165109650701539, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 67, 'train_loss': 0.04200012275704451, 'val_loss': 0.39677952497433394, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 68, 'train_loss': 0.05136342948476263, 'val_loss': 0.4522404548449394, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 69, 'train_loss': 0.03891372026370491, 'val_loss': 0.48475250831017125, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 70, 'train_loss': 0.03602851572115037, 'val_loss': 0.4384873545067942, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 71, 'train_loss': 0.03521442456344553, 'val_loss': 0.48647627871260685, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 72, 'train_loss': 0.045710727203463725, 'val_loss': 0.4201556311713325, 'test_acc': 0.9487179487179487}
{'fold': 9, 'epoch': 73, 'train_loss': 0.036464903549038634, 'val_loss': 0.41844454382219887, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 74, 'train_loss': 0.04111441769548376, 'val_loss': 0.34043374835935414, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 75, 'train_loss': 0.029491356706199377, 'val_loss': 0.36817037142240083, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 76, 'train_loss': 0.0351267518017883, 'val_loss': 0.3971925719171508, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 77, 'train_loss': 0.04137363593228224, 'val_loss': 0.4305520669007913, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 78, 'train_loss': 0.041241357294384835, 'val_loss': 0.45685774648291433, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 79, 'train_loss': 0.06648844664411271, 'val_loss': 0.638702637110001, 'test_acc': 0.8376068376068376}
{'fold': 9, 'epoch': 80, 'train_loss': 0.06441583341358513, 'val_loss': 0.3947062696147169, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 81, 'train_loss': 0.053561613003139275, 'val_loss': 0.42264100425263756, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 82, 'train_loss': 0.025917830946065992, 'val_loss': 0.46054690108340013, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 83, 'train_loss': 0.03298107957688429, 'val_loss': 0.5272379329061916, 'test_acc': 0.9401709401709402}
{'fold': 9, 'epoch': 84, 'train_loss': 0.02774279156968124, 'val_loss': 0.5607547923031017, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 85, 'train_loss': 0.015004889430849166, 'val_loss': 0.5488642831133981, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 86, 'train_loss': 0.031917935387247194, 'val_loss': 0.545382328522511, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 87, 'train_loss': 0.030002889552911333, 'val_loss': 0.522997081789196, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 88, 'train_loss': 0.02039433117873051, 'val_loss': 0.5022966596815321, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 89, 'train_loss': 0.02565507190617717, 'val_loss': 0.5554811689588759, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 90, 'train_loss': 0.0587940380476431, 'val_loss': 0.5801938945411617, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 91, 'train_loss': 0.04903955747160303, 'val_loss': 0.504639405470628, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 92, 'train_loss': 0.02789682882317042, 'val_loss': 0.4144019754523905, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 93, 'train_loss': 0.035917034305600545, 'val_loss': 0.48984718322753906, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 94, 'train_loss': 0.029441938725678976, 'val_loss': 0.5245470389341697, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 95, 'train_loss': 0.036744324159926844, 'val_loss': 0.525015138153337, 'test_acc': 0.9316239316239316}
{'fold': 9, 'epoch': 96, 'train_loss': 0.030466330768461575, 'val_loss': 0.5195681906154013, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 97, 'train_loss': 0.057612881193655714, 'val_loss': 0.4344149206438635, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 98, 'train_loss': 0.028771775493485127, 'val_loss': 0.5287940033480653, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 99, 'train_loss': 0.04469121124701953, 'val_loss': 0.6276890029255141, 'test_acc': 0.8461538461538461}
{'fold': 9, 'epoch': 100, 'train_loss': 0.05220986839178634, 'val_loss': 0.5428076034937149, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 101, 'train_loss': 0.02940105865472707, 'val_loss': 0.49231866689828724, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 102, 'train_loss': 0.020834666564350283, 'val_loss': 0.5255657098232172, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 103, 'train_loss': 0.01905413487357861, 'val_loss': 0.5756876529791416, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 104, 'train_loss': 0.03296456002073868, 'val_loss': 0.6264765160715479, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 105, 'train_loss': 0.039407589587272474, 'val_loss': 0.48314937363322985, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 106, 'train_loss': 0.03461785849886236, 'val_loss': 0.52086182944795, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 107, 'train_loss': 0.03528905365422715, 'val_loss': 0.4632611396985176, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 108, 'train_loss': 0.031237791896890455, 'val_loss': 0.5508969298794738, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 109, 'train_loss': 0.05926968193794522, 'val_loss': 0.627679694412101, 'test_acc': 0.8290598290598291}
{'fold': 9, 'epoch': 110, 'train_loss': 0.04841699278854244, 'val_loss': 0.44730577713403946, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 111, 'train_loss': 0.030279366198879807, 'val_loss': 0.5350372933933878, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 112, 'train_loss': 0.06579545191568086, 'val_loss': 0.5661933116423779, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 113, 'train_loss': 0.04967170779324942, 'val_loss': 0.477488770444169, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 114, 'train_loss': 0.046431661184908726, 'val_loss': 0.49513409280369425, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 115, 'train_loss': 0.03547601373392647, 'val_loss': 0.5261341323200454, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 116, 'train_loss': 0.04114178907110403, 'val_loss': 0.4750653291359926, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 117, 'train_loss': 0.03240905502234917, 'val_loss': 0.5802622412005042, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 118, 'train_loss': 0.023374057337404937, 'val_loss': 0.5868222000252488, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 119, 'train_loss': 0.01977117206719783, 'val_loss': 0.5947981451311682, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 120, 'train_loss': 0.039417205103160026, 'val_loss': 0.6952940296922994, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 121, 'train_loss': 0.031657652369053185, 'val_loss': 0.6403075487185748, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 122, 'train_loss': 0.034091302794522715, 'val_loss': 0.6610341846433461, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 123, 'train_loss': 0.038606139819406096, 'val_loss': 0.5917657053368723, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 124, 'train_loss': 0.028181300798955894, 'val_loss': 0.6496551057212373, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 125, 'train_loss': 0.04683755632387316, 'val_loss': 0.549113510001419, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 126, 'train_loss': 0.02252692425370974, 'val_loss': 0.6142004942282652, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 127, 'train_loss': 0.04537155087326922, 'val_loss': 0.5641619853484325, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 128, 'train_loss': 0.03816882981519191, 'val_loss': 0.7562527126736112, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 129, 'train_loss': 0.054986169711702455, 'val_loss': 0.8400965470534104, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 130, 'train_loss': 0.07544910533045876, 'val_loss': 0.6536738729884481, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 131, 'train_loss': 0.027552595162779976, 'val_loss': 0.5804597577478132, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 132, 'train_loss': 0.032605053284757976, 'val_loss': 0.5110491239107572, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 133, 'train_loss': 0.03988989418317264, 'val_loss': 0.5800062815348307, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 134, 'train_loss': 0.02871977210871607, 'val_loss': 0.5228889660957532, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 135, 'train_loss': 0.02913686296442855, 'val_loss': 0.6125847580086472, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 136, 'train_loss': 0.0414246603860362, 'val_loss': 0.5973214369553786, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 137, 'train_loss': 0.02635532225999129, 'val_loss': 0.5684438037057208, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 138, 'train_loss': 0.03125277787527875, 'val_loss': 0.6660801814152644, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 139, 'train_loss': 0.022447282332982238, 'val_loss': 0.6646159245417669, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 140, 'train_loss': 0.022913888598973784, 'val_loss': 0.7612759190746862, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 141, 'train_loss': 0.03873357817558256, 'val_loss': 0.647873283451439, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 142, 'train_loss': 0.20144686352272154, 'val_loss': 1.0955643694624941, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 143, 'train_loss': 0.11445918053357025, 'val_loss': 0.5235834529257228, 'test_acc': 0.811965811965812}
{'fold': 9, 'epoch': 144, 'train_loss': 0.05879455059372141, 'val_loss': 0.581534181904589, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 145, 'train_loss': 0.04437359642647838, 'val_loss': 0.5676333598601513, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 146, 'train_loss': 0.040643930253651685, 'val_loss': 0.5435601258889223, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 147, 'train_loss': 0.0443100016623325, 'val_loss': 0.5604948386167868, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 148, 'train_loss': 0.03751102777472223, 'val_loss': 0.6137230049850594, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 149, 'train_loss': 0.02985940582880561, 'val_loss': 0.6601419367341914, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 150, 'train_loss': 0.04021567398863766, 'val_loss': 0.7030597262912326, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 151, 'train_loss': 0.027749122007970965, 'val_loss': 0.7198298853686732, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 152, 'train_loss': 0.022292288374432, 'val_loss': 0.6589572450034639, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 153, 'train_loss': 0.030697002800248594, 'val_loss': 0.6072351993658603, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 154, 'train_loss': 0.030611901159271978, 'val_loss': 0.7060283758701422, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 155, 'train_loss': 0.027834909436316623, 'val_loss': 0.6621161566840278, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 156, 'train_loss': 0.032139253672370215, 'val_loss': 0.6928957099588509, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 157, 'train_loss': 0.025591641047898427, 'val_loss': 0.7248571999052651, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 158, 'train_loss': 0.015423448322492778, 'val_loss': 0.7721285371698885, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 159, 'train_loss': 0.022819717516293907, 'val_loss': 0.7452604505750868, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 160, 'train_loss': 0.019521629609467493, 'val_loss': 0.7012335948455029, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 161, 'train_loss': 0.021524140549630272, 'val_loss': 0.7003238710582765, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 162, 'train_loss': 0.02033474818720498, 'val_loss': 0.7038052550747863, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 163, 'train_loss': 0.020973687179182396, 'val_loss': 0.7317333710499299, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 164, 'train_loss': 0.04406126120997893, 'val_loss': 0.7518106118226663, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 165, 'train_loss': 0.025138996628318297, 'val_loss': 0.8836522550664396, 'test_acc': 0.8376068376068376}
{'fold': 9, 'epoch': 166, 'train_loss': 0.028767314284223022, 'val_loss': 0.7891471895397218, 'test_acc': 0.8205128205128205}
{'fold': 9, 'epoch': 167, 'train_loss': 0.023453530062274155, 'val_loss': 0.6837263840895432, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 168, 'train_loss': 0.013331033576524548, 'val_loss': 0.7443410107213208, 'test_acc': 0.8461538461538461}
{'fold': 9, 'epoch': 169, 'train_loss': 0.03095772884632224, 'val_loss': 0.7913436074542184, 'test_acc': 0.905982905982906}
{'fold': 9, 'epoch': 170, 'train_loss': 0.03679410661906162, 'val_loss': 0.8347316318088107, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 171, 'train_loss': 0.051582178206999896, 'val_loss': 0.7005141168578058, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 172, 'train_loss': 0.02608193987852231, 'val_loss': 0.6825150383843316, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 173, 'train_loss': 0.020471047782201036, 'val_loss': 0.6114495758317474, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 174, 'train_loss': 0.03372830177274517, 'val_loss': 0.7267125447591146, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 175, 'train_loss': 0.026594273865388676, 'val_loss': 0.611069328764565, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 176, 'train_loss': 0.044214333426572335, 'val_loss': 0.69414305075621, 'test_acc': 0.9401709401709402}
{'fold': 9, 'epoch': 177, 'train_loss': 0.03633744736405295, 'val_loss': 0.5782158028366219, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 178, 'train_loss': 0.028984695474688217, 'val_loss': 0.5696458571996444, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 179, 'train_loss': 0.03410899450201383, 'val_loss': 0.6576913197835287, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 180, 'train_loss': 0.04682421730042009, 'val_loss': 0.5984972603300698, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 181, 'train_loss': 0.04129507287131165, 'val_loss': 0.5505088577922593, 'test_acc': 0.9230769230769231}
{'fold': 9, 'epoch': 182, 'train_loss': 0.019772281169780862, 'val_loss': 0.5849415379711705, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 183, 'train_loss': 0.015130201456239604, 'val_loss': 0.6987786578316973, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 184, 'train_loss': 0.011689181292332882, 'val_loss': 0.7479118281959468, 'test_acc': 0.8974358974358975}
{'fold': 9, 'epoch': 185, 'train_loss': 0.019741320959884293, 'val_loss': 0.7495657765967214, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 186, 'train_loss': 0.030779249660800985, 'val_loss': 0.7844783424312233, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 187, 'train_loss': 0.04505630668772629, 'val_loss': 0.7010694161439553, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 188, 'train_loss': 0.04182566003306666, 'val_loss': 0.7131166702661759, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 189, 'train_loss': 0.021677024652628954, 'val_loss': 0.738061513656225, 'test_acc': 0.8461538461538461}
{'fold': 9, 'epoch': 190, 'train_loss': 0.024533292622960536, 'val_loss': 0.684203367966872, 'test_acc': 0.9145299145299145}
{'fold': 9, 'epoch': 191, 'train_loss': 0.02626651847464727, 'val_loss': 0.841080070560814, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 192, 'train_loss': 0.01005685655967435, 'val_loss': 0.7404910030528011, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 193, 'train_loss': 0.020414162217047446, 'val_loss': 0.7365788027771518, 'test_acc': 0.8547008547008547}
{'fold': 9, 'epoch': 194, 'train_loss': 0.014358104037424803, 'val_loss': 0.800179668980786, 'test_acc': 0.8632478632478633}
{'fold': 9, 'epoch': 195, 'train_loss': 0.019367588263190492, 'val_loss': 0.7813017755492121, 'test_acc': 0.8717948717948718}
{'fold': 9, 'epoch': 196, 'train_loss': 0.014809256855603622, 'val_loss': 0.7014448703863682, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 197, 'train_loss': 0.014911590743749757, 'val_loss': 0.7820772187322633, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 198, 'train_loss': 0.016168657265321382, 'val_loss': 0.7137543115860376, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 199, 'train_loss': 0.018484010434387868, 'val_loss': 0.7346254658495259, 'test_acc': 0.8803418803418803}
{'fold': 9, 'epoch': 200, 'train_loss': 0.02829298930404455, 'val_loss': 0.7793647896530281, 'test_acc': 0.8717948717948718}
Val Loss: 0.3722, Test Accuracy: 0.857 ± 0.068, Duration: 115.358
Best result - 0.857 ± 0.068
--
NCI1 - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6792355392971178, 'val_loss': 0.5333969320403978, 'test_acc': 0.6618004866180048}
{'fold': 9, 'epoch': 2, 'train_loss': 0.5727299524702295, 'val_loss': 0.49662586314254725, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5517782167579136, 'val_loss': 0.4883462251537908, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5375336823019669, 'val_loss': 0.48123211176145975, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 5, 'train_loss': 0.5207846804256857, 'val_loss': 0.47317186527298605, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5031189880449406, 'val_loss': 0.4496425897825663, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5121753197299302, 'val_loss': 0.4599959148397701, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 8, 'train_loss': 0.49898213528803664, 'val_loss': 0.4572521218998299, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5054531617321237, 'val_loss': 0.46208395575084826, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 10, 'train_loss': 0.4881884501363239, 'val_loss': 0.4576970306916249, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 11, 'train_loss': 0.4926040578280052, 'val_loss': 0.45083438567001455, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4845193683230964, 'val_loss': 0.4565148922068649, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 13, 'train_loss': 0.48567769399089533, 'val_loss': 0.45123733743263855, 'test_acc': 0.7420924574209246}
{'fold': 9, 'epoch': 14, 'train_loss': 0.4773501877802132, 'val_loss': 0.4511629693989626, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 15, 'train_loss': 0.47175464632302305, 'val_loss': 0.4519769895975897, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 16, 'train_loss': 0.4774954587656216, 'val_loss': 0.46235976486020425, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 17, 'train_loss': 0.4748430290143855, 'val_loss': 0.458263527157823, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 18, 'train_loss': 0.4712525152594504, 'val_loss': 0.4416608694406031, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 19, 'train_loss': 0.46009385999101793, 'val_loss': 0.44190383131486655, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 20, 'train_loss': 0.4813300951989028, 'val_loss': 0.44004636611381587, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 21, 'train_loss': 0.4654527038988406, 'val_loss': 0.4326445400859898, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 22, 'train_loss': 0.45537882151394865, 'val_loss': 0.4223176394645895, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 23, 'train_loss': 0.46909678417400724, 'val_loss': 0.4451042045351943, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 24, 'train_loss': 0.45113489804041645, 'val_loss': 0.4297636732857883, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 25, 'train_loss': 0.45004539343997507, 'val_loss': 0.42810294343897315, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 26, 'train_loss': 0.4447271786902073, 'val_loss': 0.41474658845404927, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 27, 'train_loss': 0.4532558215360572, 'val_loss': 0.42465000663070496, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 28, 'train_loss': 0.44087914132723843, 'val_loss': 0.4130328473093446, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 29, 'train_loss': 0.43909990907150465, 'val_loss': 0.4226050458105231, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 30, 'train_loss': 0.4509914132365345, 'val_loss': 0.425201615567915, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 31, 'train_loss': 0.4630805062551568, 'val_loss': 0.43110206121365807, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 32, 'train_loss': 0.44730420280111965, 'val_loss': 0.42480659020788186, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 33, 'train_loss': 0.44816723601878994, 'val_loss': 0.4248684421362958, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 34, 'train_loss': 0.4465349414487825, 'val_loss': 0.42566833124833675, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 35, 'train_loss': 0.4377868425672072, 'val_loss': 0.42076067680860085, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 36, 'train_loss': 0.4381349750243834, 'val_loss': 0.43139281818176417, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 37, 'train_loss': 0.4324255852151091, 'val_loss': 0.4128207325065223, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 38, 'train_loss': 0.4302943921002158, 'val_loss': 0.4308147291197394, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 39, 'train_loss': 0.43353794700037823, 'val_loss': 0.42599138320217456, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 40, 'train_loss': 0.43639565416931236, 'val_loss': 0.43856713603592845, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 41, 'train_loss': 0.42530411634131937, 'val_loss': 0.43097497424940123, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 42, 'train_loss': 0.4306505941344004, 'val_loss': 0.43349348657612674, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 43, 'train_loss': 0.42858290030573404, 'val_loss': 0.44178645744230916, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4168706815825762, 'val_loss': 0.4211039067474885, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 45, 'train_loss': 0.4150858130441965, 'val_loss': 0.42091880111508706, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 46, 'train_loss': 0.4227712194088602, 'val_loss': 0.4267984886819139, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 47, 'train_loss': 0.4178845997709427, 'val_loss': 0.42732527424239186, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 48, 'train_loss': 0.4201157366054772, 'val_loss': 0.42244002534815284, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 49, 'train_loss': 0.4156416607816724, 'val_loss': 0.4275485964587135, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 50, 'train_loss': 0.4218539703283867, 'val_loss': 0.4257420542176332, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 51, 'train_loss': 0.4211251801491654, 'val_loss': 0.4183031996381254, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 52, 'train_loss': 0.41616492495484597, 'val_loss': 0.43505038890235326, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 53, 'train_loss': 0.4159525568685392, 'val_loss': 0.41763928452837495, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 54, 'train_loss': 0.4120694469578945, 'val_loss': 0.41362514170997045, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 55, 'train_loss': 0.4057199423113009, 'val_loss': 0.4199904420950117, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 56, 'train_loss': 0.41036096644880127, 'val_loss': 0.41415448896496254, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 57, 'train_loss': 0.4056787344008467, 'val_loss': 0.4433874654653879, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 58, 'train_loss': 0.3954266251003655, 'val_loss': 0.4388571272801309, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 59, 'train_loss': 0.4081336681860207, 'val_loss': 0.4261620375361756, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 60, 'train_loss': 0.3970377832433603, 'val_loss': 0.44933753466084053, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 61, 'train_loss': 0.4001884296210143, 'val_loss': 0.4345863834204755, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 62, 'train_loss': 0.397303998579074, 'val_loss': 0.41688679604634754, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 63, 'train_loss': 0.401619460297762, 'val_loss': 0.4188355169737136, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 64, 'train_loss': 0.398411828018453, 'val_loss': 0.40655112440568686, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 65, 'train_loss': 0.3941284956505699, 'val_loss': 0.4421276036840286, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 66, 'train_loss': 0.40232011654081135, 'val_loss': 0.41575536240626426, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 67, 'train_loss': 0.39226095209809114, 'val_loss': 0.422593007702607, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 68, 'train_loss': 0.4032551956220265, 'val_loss': 0.41210987156034967, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 69, 'train_loss': 0.3989083759867362, 'val_loss': 0.443326012641554, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 70, 'train_loss': 0.40346988883331747, 'val_loss': 0.4232980305841079, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 71, 'train_loss': 0.38927515590712974, 'val_loss': 0.4177280667344439, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 72, 'train_loss': 0.38402660104045033, 'val_loss': 0.45125386314670535, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 73, 'train_loss': 0.3845125298647985, 'val_loss': 0.42825752221174773, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 74, 'train_loss': 0.3819132052960187, 'val_loss': 0.41273061376418513, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 75, 'train_loss': 0.3832041931195851, 'val_loss': 0.43480649887790357, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 76, 'train_loss': 0.39454619727865625, 'val_loss': 0.4239785085339326, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 77, 'train_loss': 0.3750344514302964, 'val_loss': 0.43192217935900906, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 78, 'train_loss': 0.37751344972065765, 'val_loss': 0.4205379718121531, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 79, 'train_loss': 0.39674298280347003, 'val_loss': 0.44845058970207713, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 80, 'train_loss': 0.38476042667009536, 'val_loss': 0.43775188603830456, 'test_acc': 0.7858880778588808}
{'fold': 9, 'epoch': 81, 'train_loss': 0.39445459701284, 'val_loss': 0.4153413563749216, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 82, 'train_loss': 0.37808999511664804, 'val_loss': 0.4209748660270895, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 83, 'train_loss': 0.37280556483425364, 'val_loss': 0.43357808746560644, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 84, 'train_loss': 0.3999869738617083, 'val_loss': 0.41314016931538455, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 85, 'train_loss': 0.37531738445489077, 'val_loss': 0.4274171889553395, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 86, 'train_loss': 0.37048976563841757, 'val_loss': 0.43157828579273827, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 87, 'train_loss': 0.372291148470266, 'val_loss': 0.4348635012215942, 'test_acc': 0.7858880778588808}
{'fold': 9, 'epoch': 88, 'train_loss': 0.3762356768885668, 'val_loss': 0.4355154385531906, 'test_acc': 0.7834549878345499}
{'fold': 9, 'epoch': 89, 'train_loss': 0.3681730816181559, 'val_loss': 0.4480921241778817, 'test_acc': 0.7907542579075426}
{'fold': 9, 'epoch': 90, 'train_loss': 0.3882852851474372, 'val_loss': 0.4407897819277724, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 91, 'train_loss': 0.3732966410308859, 'val_loss': 0.4466279858220233, 'test_acc': 0.7956204379562044}
{'fold': 9, 'epoch': 92, 'train_loss': 0.3738329256102987, 'val_loss': 0.4574991490719092, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 93, 'train_loss': 0.3689742804008679, 'val_loss': 0.44840680885779016, 'test_acc': 0.7907542579075426}
{'fold': 9, 'epoch': 94, 'train_loss': 0.37658846503409155, 'val_loss': 0.4416784254013767, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 95, 'train_loss': 0.3711959169293842, 'val_loss': 0.4435359787766951, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 96, 'train_loss': 0.38189706451048816, 'val_loss': 0.43566157115926996, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 97, 'train_loss': 0.3720284650360581, 'val_loss': 0.4402695955151189, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 98, 'train_loss': 0.36520780225957394, 'val_loss': 0.45018152946973367, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 99, 'train_loss': 0.3648717473023129, 'val_loss': 0.432993343566746, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 100, 'train_loss': 0.3829684689858534, 'val_loss': 0.43549905206165174, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 101, 'train_loss': 0.364953909259643, 'val_loss': 0.43928267021828904, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 102, 'train_loss': 0.35991385796644393, 'val_loss': 0.432675247934903, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 103, 'train_loss': 0.35645130494215194, 'val_loss': 0.45809949053465016, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 104, 'train_loss': 0.3614279408633274, 'val_loss': 0.4286749113505194, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 105, 'train_loss': 0.36930707064423246, 'val_loss': 0.4500586655888244, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 106, 'train_loss': 0.3624359648810686, 'val_loss': 0.44748961084370487, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 107, 'train_loss': 0.3650190221675991, 'val_loss': 0.44953935627809694, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 108, 'train_loss': 0.35949138198455755, 'val_loss': 0.4567429619113894, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 109, 'train_loss': 0.36472640866345735, 'val_loss': 0.43839230502608917, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 110, 'train_loss': 0.3469304694746533, 'val_loss': 0.448885762198418, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 111, 'train_loss': 0.36976059656726185, 'val_loss': 0.4439678609806256, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 112, 'train_loss': 0.3584187540368442, 'val_loss': 0.44097647469699236, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 113, 'train_loss': 0.35287217246572467, 'val_loss': 0.4454332105724771, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 114, 'train_loss': 0.34817521796174294, 'val_loss': 0.4489741824259143, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 115, 'train_loss': 0.3521941529899618, 'val_loss': 0.44688528297591384, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 116, 'train_loss': 0.35850345756668245, 'val_loss': 0.4624192871316506, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 117, 'train_loss': 0.3460446354681558, 'val_loss': 0.4743113923826937, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 118, 'train_loss': 0.3548474611483351, 'val_loss': 0.47777839066628414, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 119, 'train_loss': 0.3456040718150835, 'val_loss': 0.48821540702578503, 'test_acc': 0.7907542579075426}
{'fold': 9, 'epoch': 120, 'train_loss': 0.3411068642226449, 'val_loss': 0.44489259093347255, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 121, 'train_loss': 0.3333735930984908, 'val_loss': 0.4569123029128768, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 122, 'train_loss': 0.35285113311379496, 'val_loss': 0.4649919059734855, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 123, 'train_loss': 0.34656159639576056, 'val_loss': 0.4500846909200478, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 124, 'train_loss': 0.3382900221286899, 'val_loss': 0.4584333507974072, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 125, 'train_loss': 0.3423287828255744, 'val_loss': 0.46429250768211344, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 126, 'train_loss': 0.34366061984405033, 'val_loss': 0.4565938963507214, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 127, 'train_loss': 0.3436306287554929, 'val_loss': 0.43933298059913656, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 128, 'train_loss': 0.3423407305657429, 'val_loss': 0.4415052511396199, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 129, 'train_loss': 0.34241681214231645, 'val_loss': 0.44068173596458715, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 130, 'train_loss': 0.35071452494955413, 'val_loss': 0.4546014549088304, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 131, 'train_loss': 0.3333810515647387, 'val_loss': 0.46334681545730927, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 132, 'train_loss': 0.341823570647814, 'val_loss': 0.4422743767137365, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 133, 'train_loss': 0.3373075969462847, 'val_loss': 0.46973100369864135, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 134, 'train_loss': 0.32839451006946774, 'val_loss': 0.4672057622830653, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 135, 'train_loss': 0.34096291802660394, 'val_loss': 0.47787049100926904, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 136, 'train_loss': 0.342992312879893, 'val_loss': 0.4680743113051366, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 137, 'train_loss': 0.3241049873024008, 'val_loss': 0.49495493292518483, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 138, 'train_loss': 0.335940931237092, 'val_loss': 0.45631876479100136, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 139, 'train_loss': 0.32704710666715664, 'val_loss': 0.4625256670652515, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 140, 'train_loss': 0.3333890521830886, 'val_loss': 0.45884563511015436, 'test_acc': 0.7907542579075426}
{'fold': 9, 'epoch': 141, 'train_loss': 0.3288280833376585, 'val_loss': 0.4646995572278099, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 142, 'train_loss': 0.33897577324052797, 'val_loss': 0.4371941002616047, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 143, 'train_loss': 0.32620177124321026, 'val_loss': 0.4397298947447988, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 144, 'train_loss': 0.3368629124151529, 'val_loss': 0.472920215912979, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 145, 'train_loss': 0.3277353821118383, 'val_loss': 0.4690485302259162, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 146, 'train_loss': 0.32872599928918544, 'val_loss': 0.47226544308256346, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 147, 'train_loss': 0.3206736115861113, 'val_loss': 0.43443177680319534, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 148, 'train_loss': 0.32358470809285655, 'val_loss': 0.44571120083477084, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 149, 'train_loss': 0.3307945177937946, 'val_loss': 0.5039215784003265, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 150, 'train_loss': 0.3206072409440131, 'val_loss': 0.47659047444661456, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 151, 'train_loss': 0.3357134106276679, 'val_loss': 0.48298050539336934, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 152, 'train_loss': 0.32784680706741165, 'val_loss': 0.4843077601597547, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 153, 'train_loss': 0.3272011674886202, 'val_loss': 0.5118215611961346, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 154, 'train_loss': 0.32899053199012784, 'val_loss': 0.4891132206232298, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 155, 'train_loss': 0.33039527353796644, 'val_loss': 0.5242664541351244, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 156, 'train_loss': 0.3310752351136103, 'val_loss': 0.49918797068352244, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 157, 'train_loss': 0.4181555510654937, 'val_loss': 0.4733234275576552, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 158, 'train_loss': 0.3397922196636235, 'val_loss': 0.4523127061607194, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 159, 'train_loss': 0.3277665320647894, 'val_loss': 0.5219930013020834, 'test_acc': 0.7858880778588808}
{'fold': 9, 'epoch': 160, 'train_loss': 0.3289587067535324, 'val_loss': 0.47786347883461167, 'test_acc': 0.7858880778588808}
{'fold': 9, 'epoch': 161, 'train_loss': 0.3207647131524817, 'val_loss': 0.5239435692483201, 'test_acc': 0.7858880778588808}
{'fold': 9, 'epoch': 162, 'train_loss': 0.32595253545437414, 'val_loss': 0.46815656861539595, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 163, 'train_loss': 0.3139341860248225, 'val_loss': 0.4996519158356381, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 164, 'train_loss': 0.3148777201554201, 'val_loss': 0.5105114818489465, 'test_acc': 0.781021897810219}
{'fold': 9, 'epoch': 165, 'train_loss': 0.32423068833177104, 'val_loss': 0.4729226058997087, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 166, 'train_loss': 0.3313957746990406, 'val_loss': 0.47757666186404635, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 167, 'train_loss': 0.29969223218895225, 'val_loss': 0.4824818427835358, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 168, 'train_loss': 0.3112114577615348, 'val_loss': 0.4865965089078657, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 169, 'train_loss': 0.3069768505048578, 'val_loss': 0.5090122408530428, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 170, 'train_loss': 0.324294801284797, 'val_loss': 0.4719409153699295, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 171, 'train_loss': 0.3044916998839726, 'val_loss': 0.48627218074752177, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 172, 'train_loss': 0.3258931636810303, 'val_loss': 0.47106834281680066, 'test_acc': 0.7834549878345499}
{'fold': 9, 'epoch': 173, 'train_loss': 0.309908318530469, 'val_loss': 0.4798676196096007, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 174, 'train_loss': 0.31505607155552745, 'val_loss': 0.4591083108943744, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 175, 'train_loss': 0.317696229221612, 'val_loss': 0.5298704393298667, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 176, 'train_loss': 0.307839052574913, 'val_loss': 0.48961301557629067, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 177, 'train_loss': 0.30724965906056173, 'val_loss': 0.5052791465517958, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 178, 'train_loss': 0.2958961755146075, 'val_loss': 0.48807792942019274, 'test_acc': 0.7712895377128953}
{'fold': 9, 'epoch': 179, 'train_loss': 0.30742236455209065, 'val_loss': 0.494224195642773, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 180, 'train_loss': 0.3037051393711654, 'val_loss': 0.5118323230975446, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 181, 'train_loss': 0.3116990598666407, 'val_loss': 0.4787939540371117, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 182, 'train_loss': 0.3065072504164529, 'val_loss': 0.48900631164402275, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 183, 'train_loss': 0.3018767389937909, 'val_loss': 0.5054141506371417, 'test_acc': 0.7737226277372263}
{'fold': 9, 'epoch': 184, 'train_loss': 0.3134587310308958, 'val_loss': 0.5276944469071363, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 185, 'train_loss': 0.31093934118530175, 'val_loss': 0.4805362358000447, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 186, 'train_loss': 0.3052007214431345, 'val_loss': 0.5343199309931475, 'test_acc': 0.7858880778588808}
{'fold': 9, 'epoch': 187, 'train_loss': 0.31276701575648175, 'val_loss': 0.4971480218743466, 'test_acc': 0.7664233576642335}
{'fold': 9, 'epoch': 188, 'train_loss': 0.30440786100217027, 'val_loss': 0.5319708060754187, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 189, 'train_loss': 0.30754058144605945, 'val_loss': 0.5141309288006339, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 190, 'train_loss': 0.31275767468623, 'val_loss': 0.5560814748425263, 'test_acc': 0.7834549878345499}
{'fold': 9, 'epoch': 191, 'train_loss': 0.3047631455816492, 'val_loss': 0.5232969873432985, 'test_acc': 0.7785888077858881}
{'fold': 9, 'epoch': 192, 'train_loss': 0.2965210794749921, 'val_loss': 0.5193253073959165, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 193, 'train_loss': 0.2993214791164781, 'val_loss': 0.5502323744650884, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 194, 'train_loss': 0.29557567410660485, 'val_loss': 0.5155459540893852, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 195, 'train_loss': 0.2944936695760184, 'val_loss': 0.5308505288005745, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 196, 'train_loss': 0.3063334987981476, 'val_loss': 0.4749861575681218, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 197, 'train_loss': 0.3040041513469097, 'val_loss': 0.49595998963590376, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 198, 'train_loss': 0.307774199023299, 'val_loss': 0.49381517841868156, 'test_acc': 0.7639902676399026}
{'fold': 9, 'epoch': 199, 'train_loss': 0.28420715403817864, 'val_loss': 0.513592907982151, 'test_acc': 0.7761557177615572}
{'fold': 9, 'epoch': 200, 'train_loss': 0.29107174125030966, 'val_loss': 0.5162495429788483, 'test_acc': 0.7737226277372263}
Val Loss: 0.4769, Test Accuracy: 0.781 ± 0.017, Duration: 296.876
Best result - 0.781 ± 0.017
--
PROTEINS - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.9129591056393453, 'val_loss': 0.6179729324203354, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 2, 'train_loss': 0.5937268286441713, 'val_loss': 0.6012374946662972, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 3, 'train_loss': 0.5400535066922506, 'val_loss': 0.5628134753253009, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5476791199409601, 'val_loss': 0.5413768527744053, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 5, 'train_loss': 0.5245630082859335, 'val_loss': 0.5499356587727865, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5248076903699624, 'val_loss': 0.5375016358521607, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5297194849762451, 'val_loss': 0.5457894952447565, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 8, 'train_loss': 0.5135790966576599, 'val_loss': 0.5270208410314612, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5085659725497468, 'val_loss': 0.5328434609078072, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5201896982963639, 'val_loss': 0.5398755460172087, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5076834200207232, 'val_loss': 0.5394295357369088, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 12, 'train_loss': 0.49703087670233115, 'val_loss': 0.5291538754025021, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 13, 'train_loss': 0.49540317580354737, 'val_loss': 0.5299065564129803, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 14, 'train_loss': 0.4818019931163852, 'val_loss': 0.5581679129385734, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 15, 'train_loss': 0.4955282415003086, 'val_loss': 0.5317597088513074, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 16, 'train_loss': 0.48603993824836783, 'val_loss': 0.5420703372439822, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 17, 'train_loss': 0.4970248379691281, 'val_loss': 0.5368845484278224, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 18, 'train_loss': 0.47949760399683555, 'val_loss': 0.5446713593629029, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 19, 'train_loss': 0.475150567112547, 'val_loss': 0.5397770512211431, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 20, 'train_loss': 0.4968295451567229, 'val_loss': 0.5245803970474381, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 21, 'train_loss': 0.47004549150113706, 'val_loss': 0.5343745119936831, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 22, 'train_loss': 0.4734065689221777, 'val_loss': 0.5381330885328688, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 23, 'train_loss': 0.4596238567772939, 'val_loss': 0.5094455340961078, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 24, 'train_loss': 0.4666521832196399, 'val_loss': 0.507502100489161, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 25, 'train_loss': 0.4600202133157839, 'val_loss': 0.5397436124784453, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 26, 'train_loss': 0.4671059249828159, 'val_loss': 0.5342952625171559, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 27, 'train_loss': 0.46409111623009447, 'val_loss': 0.530871090588269, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 28, 'train_loss': 0.45590914038295294, 'val_loss': 0.527005925908819, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 29, 'train_loss': 0.4506022545825753, 'val_loss': 0.534765776213225, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 30, 'train_loss': 0.4479256305429671, 'val_loss': 0.5532567823255384, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 31, 'train_loss': 0.44516515490984676, 'val_loss': 0.5378654411247185, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 32, 'train_loss': 0.4504603687360231, 'val_loss': 0.5745013125307925, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 33, 'train_loss': 0.4456870742518492, 'val_loss': 0.5528010978355063, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 34, 'train_loss': 0.42946710088839035, 'val_loss': 0.5340917604463594, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 35, 'train_loss': 0.44798348538000576, 'val_loss': 0.5510524028056377, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 36, 'train_loss': 0.4428941954667319, 'val_loss': 0.5454057229531778, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 37, 'train_loss': 0.4477888957819955, 'val_loss': 0.5174787882212046, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 38, 'train_loss': 0.43069335326602565, 'val_loss': 0.5586738586425781, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 39, 'train_loss': 0.43080627787795533, 'val_loss': 0.5261807312836518, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 40, 'train_loss': 0.4253781217115897, 'val_loss': 0.5365244891192462, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 41, 'train_loss': 0.42103340760224595, 'val_loss': 0.532570641320031, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 42, 'train_loss': 0.4219602511988746, 'val_loss': 0.5617884386767138, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 43, 'train_loss': 0.40355554423749646, 'val_loss': 0.5598574286108619, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4176268794319846, 'val_loss': 0.5323135616543057, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 45, 'train_loss': 0.41638155557491163, 'val_loss': 0.5385725004178984, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 46, 'train_loss': 0.43177884265228555, 'val_loss': 0.5472928296338331, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 47, 'train_loss': 0.4257449857915692, 'val_loss': 0.5057731834617821, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 48, 'train_loss': 0.4162963128049767, 'val_loss': 0.535871815037083, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 49, 'train_loss': 0.40273417497323416, 'val_loss': 0.5746069555884009, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 50, 'train_loss': 0.40736019069498236, 'val_loss': 0.549036438400681, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 51, 'train_loss': 0.39939497376130484, 'val_loss': 0.574558636089703, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 52, 'train_loss': 0.41681301513504904, 'val_loss': 0.5225719761204075, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 53, 'train_loss': 0.38968614476297037, 'val_loss': 0.618159904136314, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 54, 'train_loss': 0.4069690977282797, 'val_loss': 0.5293612780871692, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 55, 'train_loss': 0.4294940744987642, 'val_loss': 0.5962202226793444, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 56, 'train_loss': 0.3971013108688573, 'val_loss': 0.5580649161124015, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 57, 'train_loss': 0.3745940593557326, 'val_loss': 0.5689756032582876, 'test_acc': 0.8018018018018018}
{'fold': 9, 'epoch': 58, 'train_loss': 0.40712227714984905, 'val_loss': 0.5864705781678896, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 59, 'train_loss': 0.39427248396054665, 'val_loss': 0.5363519857595632, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 60, 'train_loss': 0.3632221146665438, 'val_loss': 0.5952514442237647, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 61, 'train_loss': 0.36537217386443205, 'val_loss': 0.5931161416543497, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 62, 'train_loss': 0.3649071182064737, 'val_loss': 0.5736239665263408, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 63, 'train_loss': 0.37109635414097847, 'val_loss': 0.5738032744811462, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 64, 'train_loss': 0.3740823294979956, 'val_loss': 0.585944871644716, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 65, 'train_loss': 0.3589432798451446, 'val_loss': 0.5832315393396326, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 66, 'train_loss': 0.3707310803209491, 'val_loss': 0.5917531090813715, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 67, 'train_loss': 0.3772294729044943, 'val_loss': 0.577938784350146, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 68, 'train_loss': 0.36900469159036375, 'val_loss': 0.545007121455562, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 69, 'train_loss': 0.3495871135682771, 'val_loss': 0.5888343673568588, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 70, 'train_loss': 0.35531742966134944, 'val_loss': 0.5677942155717729, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 71, 'train_loss': 0.3473857843916023, 'val_loss': 0.6301068830060529, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 72, 'train_loss': 0.35426977046009667, 'val_loss': 0.6117651999533713, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 73, 'train_loss': 0.33980288017879834, 'val_loss': 0.6076406186765378, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 74, 'train_loss': 0.3454162217551209, 'val_loss': 0.6018230163299285, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 75, 'train_loss': 0.35556800337351535, 'val_loss': 0.61922685090486, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 76, 'train_loss': 0.34709448625744393, 'val_loss': 0.6096627175270974, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 77, 'train_loss': 0.35874479627769806, 'val_loss': 0.6722472250998557, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 78, 'train_loss': 0.348160592594532, 'val_loss': 0.6254421268497501, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 79, 'train_loss': 0.341147871330531, 'val_loss': 0.5907639855737085, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 80, 'train_loss': 0.34043691074005283, 'val_loss': 0.636095768696553, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 81, 'train_loss': 0.34336434023749546, 'val_loss': 0.6034743850295609, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 82, 'train_loss': 0.34691440176080773, 'val_loss': 0.6703706689783044, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 83, 'train_loss': 0.3392479105629905, 'val_loss': 0.6920729284887915, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 84, 'train_loss': 0.3322536614206102, 'val_loss': 0.6391868247642173, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 85, 'train_loss': 0.3419505780192738, 'val_loss': 0.6143597439602688, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 86, 'train_loss': 0.33325091154888425, 'val_loss': 0.6117831737071544, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 87, 'train_loss': 0.350009064481716, 'val_loss': 0.6991915316195101, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 88, 'train_loss': 0.31271858709027067, 'val_loss': 0.6899937294624947, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 89, 'train_loss': 0.3064539911168994, 'val_loss': 0.7629339544622747, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 90, 'train_loss': 0.2960110098023206, 'val_loss': 0.7103995933189048, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 91, 'train_loss': 0.30914044952151754, 'val_loss': 0.699708096615903, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 92, 'train_loss': 0.3138281837456957, 'val_loss': 0.7082187549487965, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 93, 'train_loss': 0.3094829863771445, 'val_loss': 0.7093280757869687, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 94, 'train_loss': 0.32337107511883234, 'val_loss': 0.6487141514683629, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 95, 'train_loss': 0.30560152418284303, 'val_loss': 0.6948877970377604, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 96, 'train_loss': 0.29884259277321273, 'val_loss': 0.6953959078402132, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 97, 'train_loss': 0.3238314385165269, 'val_loss': 0.6905614148389112, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 98, 'train_loss': 0.30553044443981414, 'val_loss': 0.7147004067360818, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 99, 'train_loss': 0.30444312316400035, 'val_loss': 0.7366835791785438, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 100, 'train_loss': 0.30438909827659427, 'val_loss': 0.6942920856647663, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 101, 'train_loss': 0.28603209409649527, 'val_loss': 0.7804037558065878, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 102, 'train_loss': 0.2906807777456162, 'val_loss': 0.750996495152379, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 103, 'train_loss': 0.31535989216682486, 'val_loss': 0.7990336718859973, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 104, 'train_loss': 0.3003894235148574, 'val_loss': 0.7269414265950521, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 105, 'train_loss': 0.28666142692870966, 'val_loss': 0.7793071506259678, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 106, 'train_loss': 0.28751124807881184, 'val_loss': 0.6798259975673916, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 107, 'train_loss': 0.2729534139536848, 'val_loss': 0.7737087215389218, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 108, 'train_loss': 0.3081127430252756, 'val_loss': 0.7488387511657165, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 109, 'train_loss': 0.28787998313253577, 'val_loss': 0.7437355797570031, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 110, 'train_loss': 0.28770575158122413, 'val_loss': 0.7817623035327809, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 111, 'train_loss': 0.2697612130481386, 'val_loss': 0.7669605564426731, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 112, 'train_loss': 0.2743427940089293, 'val_loss': 0.7833383921030406, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 113, 'train_loss': 0.2827715645936202, 'val_loss': 0.7713097580918321, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 114, 'train_loss': 0.26088277418605405, 'val_loss': 0.8100293477376302, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 115, 'train_loss': 0.2742528398630996, 'val_loss': 0.9041405755120355, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 116, 'train_loss': 0.28792688391024013, 'val_loss': 0.8357058000994159, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 117, 'train_loss': 0.25342336487689804, 'val_loss': 0.8336186108288465, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 118, 'train_loss': 0.2791367841489387, 'val_loss': 0.8110946449073585, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 119, 'train_loss': 0.29714082698227984, 'val_loss': 0.732991158425271, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 120, 'train_loss': 0.27982508519081156, 'val_loss': 0.7892759168470228, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 121, 'train_loss': 0.2632925433141214, 'val_loss': 0.8147484375549866, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 122, 'train_loss': 0.2628352874858612, 'val_loss': 0.7047323278478674, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 123, 'train_loss': 0.28669613845621295, 'val_loss': 0.855183609971055, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 124, 'train_loss': 0.2924244530273206, 'val_loss': 0.8102752410613738, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 125, 'train_loss': 0.2626468082168688, 'val_loss': 0.7797797263205588, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 126, 'train_loss': 0.2806873815228241, 'val_loss': 0.8003409789489196, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 127, 'train_loss': 0.2627607352203793, 'val_loss': 0.7813659874168603, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 128, 'train_loss': 0.2664462582832234, 'val_loss': 0.857777501011754, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 129, 'train_loss': 0.2696372886699458, 'val_loss': 0.8378918278324712, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 130, 'train_loss': 0.2603255634757405, 'val_loss': 0.8201212840037303, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 131, 'train_loss': 0.26228898061244976, 'val_loss': 0.8099444277651675, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 132, 'train_loss': 0.2561091317070855, 'val_loss': 0.8597193881198093, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 133, 'train_loss': 0.26343302273188374, 'val_loss': 0.766356322142455, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 134, 'train_loss': 0.26291995513118077, 'val_loss': 0.7937404529468434, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 135, 'train_loss': 0.24459826053192318, 'val_loss': 0.8522188341295397, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 136, 'train_loss': 0.23799300675440316, 'val_loss': 0.8184582134624859, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 137, 'train_loss': 0.24252363241682148, 'val_loss': 0.8685812391676344, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 138, 'train_loss': 0.2567385393963117, 'val_loss': 0.9243801220043285, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 139, 'train_loss': 0.24063033337143536, 'val_loss': 0.9378165167731207, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 140, 'train_loss': 0.24348229370534621, 'val_loss': 0.8905087720166456, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 141, 'train_loss': 0.2725485981714846, 'val_loss': 0.8662723506893124, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 142, 'train_loss': 0.2767807789121814, 'val_loss': 0.9017896566305075, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 143, 'train_loss': 0.2582191891140408, 'val_loss': 0.7740666844823338, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 144, 'train_loss': 0.2723647653053104, 'val_loss': 0.965761408075556, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 145, 'train_loss': 0.24305483352656315, 'val_loss': 0.8610710968842378, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 146, 'train_loss': 0.23420924399838303, 'val_loss': 0.9126377621212521, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 147, 'train_loss': 0.2463065022069597, 'val_loss': 0.8703194695550043, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 148, 'train_loss': 0.2790746803235526, 'val_loss': 0.8834139506022135, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 149, 'train_loss': 0.2599461419815166, 'val_loss': 0.9973342826774528, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 150, 'train_loss': 0.24781116035450187, 'val_loss': 1.0006372262765695, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 151, 'train_loss': 0.22893870559204307, 'val_loss': 0.9834737348126935, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 152, 'train_loss': 0.2562315724915527, 'val_loss': 0.8029870729188662, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 153, 'train_loss': 0.2576602466580041, 'val_loss': 0.9518628163380666, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 154, 'train_loss': 0.24129987385136512, 'val_loss': 0.8946565851434931, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 155, 'train_loss': 0.2799924363190879, 'val_loss': 0.9740154506923916, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 156, 'train_loss': 0.25208502445959485, 'val_loss': 0.8500333218961149, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 157, 'train_loss': 0.2609226355950038, 'val_loss': 0.8208465576171875, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 158, 'train_loss': 0.23555115668059198, 'val_loss': 0.8736283242165506, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 159, 'train_loss': 0.22257000589210177, 'val_loss': 0.879990105156426, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 160, 'train_loss': 0.23942883844528134, 'val_loss': 0.988594811241906, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 161, 'train_loss': 0.23046457611952567, 'val_loss': 0.9245112307436831, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 162, 'train_loss': 0.2267379323240081, 'val_loss': 0.8547096596107827, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 163, 'train_loss': 0.23745742390051433, 'val_loss': 0.8941321845527168, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 164, 'train_loss': 0.21525715988894503, 'val_loss': 1.068163553873698, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 165, 'train_loss': 0.22039417159878444, 'val_loss': 0.817946872195682, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 166, 'train_loss': 0.24301378278419225, 'val_loss': 0.9506786105869053, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 167, 'train_loss': 0.22146697337378557, 'val_loss': 0.9940109596596108, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 168, 'train_loss': 0.25382878704103157, 'val_loss': 0.9479278014586853, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 169, 'train_loss': 0.24239814617617764, 'val_loss': 0.9327553757676134, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 170, 'train_loss': 0.23602935777168083, 'val_loss': 0.9584010699847797, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 171, 'train_loss': 0.2308425733738074, 'val_loss': 1.0453504270261473, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 172, 'train_loss': 0.24434498614735073, 'val_loss': 0.9914044560612859, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 173, 'train_loss': 0.2628443136761084, 'val_loss': 1.0173188286858637, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 174, 'train_loss': 0.237957829076433, 'val_loss': 0.9375335762092659, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 175, 'train_loss': 0.25857172730795863, 'val_loss': 0.9089945887660121, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 176, 'train_loss': 0.2335143929679787, 'val_loss': 0.992611584362683, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 177, 'train_loss': 0.2456481898272479, 'val_loss': 0.9892542727358706, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 178, 'train_loss': 0.23870817830265573, 'val_loss': 0.8888704110910227, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 179, 'train_loss': 0.2858091081031645, 'val_loss': 1.0037739727948163, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 180, 'train_loss': 0.3093137628703005, 'val_loss': 0.8012700811162725, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 181, 'train_loss': 0.2740110972714344, 'val_loss': 0.7785741788846953, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 182, 'train_loss': 0.2455203075601597, 'val_loss': 0.9933389199746622, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 183, 'train_loss': 0.25259597940677747, 'val_loss': 0.9031381349305849, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 184, 'train_loss': 0.2708691257218319, 'val_loss': 0.8613436930888408, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 185, 'train_loss': 0.21985227471650248, 'val_loss': 0.9786620612617012, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 186, 'train_loss': 0.2418847503485503, 'val_loss': 0.9694861678389816, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 187, 'train_loss': 0.2442693090198016, 'val_loss': 1.0389081250439893, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 188, 'train_loss': 0.23607670367767514, 'val_loss': 0.9268119399612015, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 189, 'train_loss': 0.23288368170309548, 'val_loss': 0.8896444853361662, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 190, 'train_loss': 0.21984751444774042, 'val_loss': 1.0278494207708686, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 191, 'train_loss': 0.2148615366682059, 'val_loss': 1.098354099033115, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 192, 'train_loss': 0.2375464759491108, 'val_loss': 1.067424018103797, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 193, 'train_loss': 0.2621868682228756, 'val_loss': 1.0027943172970333, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 194, 'train_loss': 0.2835657050212224, 'val_loss': 0.7541387059667088, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 195, 'train_loss': 0.26600059836801854, 'val_loss': 0.7593658722198762, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 196, 'train_loss': 0.283108603573006, 'val_loss': 0.7788094185494088, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 197, 'train_loss': 0.2624624764879143, 'val_loss': 0.8596835780788112, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 198, 'train_loss': 0.32859930335873305, 'val_loss': 0.8764746726096213, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 199, 'train_loss': 0.3239982454263, 'val_loss': 0.9199624276375985, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 200, 'train_loss': 0.2681511377646064, 'val_loss': 0.9213164389670432, 'test_acc': 0.7297297297297297}
Val Loss: 0.5146, Test Accuracy: 0.742 ± 0.053, Duration: 80.946
Best result - 0.742 ± 0.053
--
COLLAB - Classifier
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'nn.glob.global_sort_pool' is deprecated, use 'nn.aggr.SortAggr' instead
  warnings.warn(out)
						{'fold': 9, 'epoch': 1, 'train_loss': 0.46925040289759634, 'val_loss': 0.35162708854675295, 'test_acc': 0.92}
{'fold': 9, 'epoch': 2, 'train_loss': 0.2322145888581872, 'val_loss': 0.328634081363678, 'test_acc': 0.946}
{'fold': 9, 'epoch': 3, 'train_loss': 0.1651907774619758, 'val_loss': 0.24086908102035523, 'test_acc': 0.95}
{'fold': 9, 'epoch': 4, 'train_loss': 0.15230061405338347, 'val_loss': 0.18939914679527284, 'test_acc': 0.956}
{'fold': 9, 'epoch': 5, 'train_loss': 0.1410125787369907, 'val_loss': 0.19513847637176512, 'test_acc': 0.948}
{'fold': 9, 'epoch': 6, 'train_loss': 0.12455088421702384, 'val_loss': 0.2021176767349243, 'test_acc': 0.946}
{'fold': 9, 'epoch': 7, 'train_loss': 0.13765102732926607, 'val_loss': 0.18497240877151488, 'test_acc': 0.956}
{'fold': 9, 'epoch': 8, 'train_loss': 0.11853283172473311, 'val_loss': 0.23051458501815797, 'test_acc': 0.962}
{'fold': 9, 'epoch': 9, 'train_loss': 0.12443043788895011, 'val_loss': 0.1835536231994629, 'test_acc': 0.958}
{'fold': 9, 'epoch': 10, 'train_loss': 0.11280915333889424, 'val_loss': 0.20953077125549316, 'test_acc': 0.956}
{'fold': 9, 'epoch': 11, 'train_loss': 0.10683359174057841, 'val_loss': 0.18562449359893798, 'test_acc': 0.952}
{'fold': 9, 'epoch': 12, 'train_loss': 0.1062265174742788, 'val_loss': 0.2244892451763153, 'test_acc': 0.956}
{'fold': 9, 'epoch': 13, 'train_loss': 0.10479845668189228, 'val_loss': 0.19250478315353395, 'test_acc': 0.966}
{'fold': 9, 'epoch': 14, 'train_loss': 0.09946980416774749, 'val_loss': 0.22635627722740173, 'test_acc': 0.954}
{'fold': 9, 'epoch': 15, 'train_loss': 0.10072492535226046, 'val_loss': 0.22476955342292787, 'test_acc': 0.958}
{'fold': 9, 'epoch': 16, 'train_loss': 0.10717035043053329, 'val_loss': 0.2211145830154419, 'test_acc': 0.962}
{'fold': 9, 'epoch': 17, 'train_loss': 0.11654717753641307, 'val_loss': 0.15730430412292482, 'test_acc': 0.962}
{'fold': 9, 'epoch': 18, 'train_loss': 0.09125282579101622, 'val_loss': 0.1697993230819702, 'test_acc': 0.954}
{'fold': 9, 'epoch': 19, 'train_loss': 0.09662841161713004, 'val_loss': 0.18446284985542297, 'test_acc': 0.952}
{'fold': 9, 'epoch': 20, 'train_loss': 0.095390216242522, 'val_loss': 0.17514334416389465, 'test_acc': 0.958}
{'fold': 9, 'epoch': 21, 'train_loss': 0.09059373639523984, 'val_loss': 0.18526832151412964, 'test_acc': 0.962}
{'fold': 9, 'epoch': 22, 'train_loss': 0.08509996019303799, 'val_loss': 0.2474854850769043, 'test_acc': 0.952}
{'fold': 9, 'epoch': 23, 'train_loss': 0.09996921240817755, 'val_loss': 0.23444214820861817, 'test_acc': 0.954}
{'fold': 9, 'epoch': 24, 'train_loss': 0.09420038585551083, 'val_loss': 0.173001953125, 'test_acc': 0.952}
{'fold': 9, 'epoch': 25, 'train_loss': 0.07906468202825635, 'val_loss': 0.22719154930114746, 'test_acc': 0.954}
{'fold': 9, 'epoch': 26, 'train_loss': 0.09146347112953662, 'val_loss': 0.23864267182350157, 'test_acc': 0.962}
{'fold': 9, 'epoch': 27, 'train_loss': 0.09275878017768263, 'val_loss': 0.21279055857658385, 'test_acc': 0.962}
{'fold': 9, 'epoch': 28, 'train_loss': 0.09393666326068342, 'val_loss': 0.19522689151763917, 'test_acc': 0.952}
{'fold': 9, 'epoch': 29, 'train_loss': 0.0916081178560853, 'val_loss': 0.20782847380638123, 'test_acc': 0.952}
{'fold': 9, 'epoch': 30, 'train_loss': 0.08437228902243078, 'val_loss': 0.20976310634613038, 'test_acc': 0.952}
{'fold': 9, 'epoch': 31, 'train_loss': 0.08328780780080706, 'val_loss': 0.23774166059494017, 'test_acc': 0.946}
{'fold': 9, 'epoch': 32, 'train_loss': 0.08562864575069398, 'val_loss': 0.23760215187072753, 'test_acc': 0.944}
{'fold': 9, 'epoch': 33, 'train_loss': 0.0974050332326442, 'val_loss': 0.18229973649978637, 'test_acc': 0.948}
{'fold': 9, 'epoch': 34, 'train_loss': 0.10157332832925021, 'val_loss': 0.2957280738353729, 'test_acc': 0.954}
{'fold': 9, 'epoch': 35, 'train_loss': 0.08875543028581888, 'val_loss': 0.22533990812301635, 'test_acc': 0.952}
{'fold': 9, 'epoch': 36, 'train_loss': 0.08126208727713674, 'val_loss': 0.27300706338882447, 'test_acc': 0.952}
{'fold': 9, 'epoch': 37, 'train_loss': 0.09564151844009756, 'val_loss': 0.15519457054138183, 'test_acc': 0.956}
{'fold': 9, 'epoch': 38, 'train_loss': 0.09613365777768194, 'val_loss': 0.17856897568702698, 'test_acc': 0.956}
{'fold': 9, 'epoch': 39, 'train_loss': 0.07707570215687155, 'val_loss': 0.16849440836906432, 'test_acc': 0.944}
{'fold': 9, 'epoch': 40, 'train_loss': 0.08058690771926194, 'val_loss': 0.19987893414497376, 'test_acc': 0.95}
{'fold': 9, 'epoch': 41, 'train_loss': 0.07989635892910883, 'val_loss': 0.18895959401130677, 'test_acc': 0.942}
{'fold': 9, 'epoch': 42, 'train_loss': 0.07267744242213667, 'val_loss': 0.21479968118667603, 'test_acc': 0.946}
{'fold': 9, 'epoch': 43, 'train_loss': 0.08037190473172813, 'val_loss': 0.21339209675788878, 'test_acc': 0.954}
{'fold': 9, 'epoch': 44, 'train_loss': 0.07614045259542764, 'val_loss': 0.1932989101409912, 'test_acc': 0.954}
{'fold': 9, 'epoch': 45, 'train_loss': 0.07856292869779281, 'val_loss': 0.1975248658657074, 'test_acc': 0.952}
{'fold': 9, 'epoch': 46, 'train_loss': 0.07960875590331852, 'val_loss': 1.778984218120575, 'test_acc': 0.946}
{'fold': 9, 'epoch': 47, 'train_loss': 0.07127371444366873, 'val_loss': 0.22242583537101746, 'test_acc': 0.952}
{'fold': 9, 'epoch': 48, 'train_loss': 0.08206536420155316, 'val_loss': 0.20412134766578674, 'test_acc': 0.948}
{'fold': 9, 'epoch': 49, 'train_loss': 0.08367942367680371, 'val_loss': 0.28038752484321594, 'test_acc': 0.954}
{'fold': 9, 'epoch': 50, 'train_loss': 0.08526546297594904, 'val_loss': 0.21155686569213866, 'test_acc': 0.956}
{'fold': 9, 'epoch': 51, 'train_loss': 0.07894797968212515, 'val_loss': 0.22106278610229493, 'test_acc': 0.958}
{'fold': 9, 'epoch': 52, 'train_loss': 0.0955177319701761, 'val_loss': 0.19446578812599183, 'test_acc': 0.944}
{'fold': 9, 'epoch': 53, 'train_loss': 0.07869799980893731, 'val_loss': 0.19906846714019774, 'test_acc': 0.954}
{'fold': 9, 'epoch': 54, 'train_loss': 0.06958561011124402, 'val_loss': 0.241729238986969, 'test_acc': 0.95}
{'fold': 9, 'epoch': 55, 'train_loss': 0.08227786640170962, 'val_loss': 0.19985270237922667, 'test_acc': 0.948}
{'fold': 9, 'epoch': 56, 'train_loss': 0.07924641173798591, 'val_loss': 0.26982453966140746, 'test_acc': 0.958}
{'fold': 9, 'epoch': 57, 'train_loss': 0.0756495261960663, 'val_loss': 0.26705921268463134, 'test_acc': 0.962}
{'fold': 9, 'epoch': 58, 'train_loss': 0.07672794931568205, 'val_loss': 0.2655670838356018, 'test_acc': 0.954}
{'fold': 9, 'epoch': 59, 'train_loss': 0.06800915091429488, 'val_loss': 0.312580502986908, 'test_acc': 0.95}
{'fold': 9, 'epoch': 60, 'train_loss': 0.0680021691834554, 'val_loss': 0.2958757038116455, 'test_acc': 0.958}
{'fold': 9, 'epoch': 61, 'train_loss': 0.0758388142939657, 'val_loss': 0.269456533908844, 'test_acc': 0.954}
{'fold': 9, 'epoch': 62, 'train_loss': 0.07177711687982083, 'val_loss': 0.3362375385761261, 'test_acc': 0.962}
{'fold': 9, 'epoch': 63, 'train_loss': 0.09140630030073225, 'val_loss': 0.324798773765564, 'test_acc': 0.95}
{'fold': 9, 'epoch': 64, 'train_loss': 0.07232402969500981, 'val_loss': 0.44123493337631225, 'test_acc': 0.954}
{'fold': 9, 'epoch': 65, 'train_loss': 0.07165864996612072, 'val_loss': 0.3265970468521118, 'test_acc': 0.958}
{'fold': 9, 'epoch': 66, 'train_loss': 0.08040961600840092, 'val_loss': 0.26846243238449097, 'test_acc': 0.964}
{'fold': 9, 'epoch': 67, 'train_loss': 0.07152279480360449, 'val_loss': 0.27978952598571777, 'test_acc': 0.964}
{'fold': 9, 'epoch': 68, 'train_loss': 0.06527140128426254, 'val_loss': 0.38226505017280576, 'test_acc': 0.958}
{'fold': 9, 'epoch': 69, 'train_loss': 0.07092201219405979, 'val_loss': 0.31008279347419737, 'test_acc': 0.954}
{'fold': 9, 'epoch': 70, 'train_loss': 0.0781943502498325, 'val_loss': 2.181749581336975, 'test_acc': 0.948}
{'fold': 9, 'epoch': 71, 'train_loss': 0.0717964096320793, 'val_loss': 0.2606826868057251, 'test_acc': 0.956}
{'fold': 9, 'epoch': 72, 'train_loss': 0.07464359247591347, 'val_loss': 0.21533849740028382, 'test_acc': 0.964}
{'fold': 9, 'epoch': 73, 'train_loss': 0.06878901322837919, 'val_loss': 0.3127039940357208, 'test_acc': 0.954}
{'fold': 9, 'epoch': 74, 'train_loss': 0.08075066002085805, 'val_loss': 0.2320266785621643, 'test_acc': 0.958}
{'fold': 9, 'epoch': 75, 'train_loss': 0.07339459993876517, 'val_loss': 0.2965825977325439, 'test_acc': 0.96}
{'fold': 9, 'epoch': 76, 'train_loss': 0.06833910026587546, 'val_loss': 0.283134096622467, 'test_acc': 0.962}
{'fold': 9, 'epoch': 77, 'train_loss': 0.07138967670151032, 'val_loss': 0.2858345520496368, 'test_acc': 0.962}
{'fold': 9, 'epoch': 78, 'train_loss': 0.06565346023533493, 'val_loss': 0.3174053611755371, 'test_acc': 0.966}
{'fold': 9, 'epoch': 79, 'train_loss': 0.07411909641232342, 'val_loss': 0.35814030075073244, 'test_acc': 0.946}
{'fold': 9, 'epoch': 80, 'train_loss': 0.10310821935534477, 'val_loss': 0.34830439376831057, 'test_acc': 0.952}
{'fold': 9, 'epoch': 81, 'train_loss': 0.07710227753035724, 'val_loss': 0.3702645900249481, 'test_acc': 0.95}
{'fold': 9, 'epoch': 82, 'train_loss': 0.06791874632239342, 'val_loss': 0.3941166682243347, 'test_acc': 0.956}
{'fold': 9, 'epoch': 83, 'train_loss': 0.06662036255002021, 'val_loss': 0.5452134704589844, 'test_acc': 0.952}
{'fold': 9, 'epoch': 84, 'train_loss': 0.08271070146001876, 'val_loss': 0.42688349533081055, 'test_acc': 0.964}
{'fold': 9, 'epoch': 85, 'train_loss': 0.06402002732269466, 'val_loss': 0.5205340142250061, 'test_acc': 0.956}
{'fold': 9, 'epoch': 86, 'train_loss': 0.07946572250220925, 'val_loss': 0.39724127531051634, 'test_acc': 0.954}
{'fold': 9, 'epoch': 87, 'train_loss': 0.08044337262865156, 'val_loss': 0.2794076600074768, 'test_acc': 0.952}
{'fold': 9, 'epoch': 88, 'train_loss': 0.07225969390943647, 'val_loss': 0.32092401933670045, 'test_acc': 0.954}
{'fold': 9, 'epoch': 89, 'train_loss': 0.07232303438708186, 'val_loss': 0.9252670645713806, 'test_acc': 0.952}
{'fold': 9, 'epoch': 90, 'train_loss': 0.0657994111487642, 'val_loss': 0.4037889633178711, 'test_acc': 0.956}
{'fold': 9, 'epoch': 91, 'train_loss': 0.06719690466299653, 'val_loss': 1.3558225955963135, 'test_acc': 0.94}
{'fold': 9, 'epoch': 92, 'train_loss': 0.09650988439098, 'val_loss': 2.7567390158176424, 'test_acc': 0.956}
{'fold': 9, 'epoch': 93, 'train_loss': 0.06974445518571884, 'val_loss': 0.4577983591556549, 'test_acc': 0.954}
{'fold': 9, 'epoch': 94, 'train_loss': 0.06677911789622158, 'val_loss': 1.6038213109970092, 'test_acc': 0.936}
{'fold': 9, 'epoch': 95, 'train_loss': 0.07538804169744254, 'val_loss': 3.1375230553150177, 'test_acc': 0.958}
{'fold': 9, 'epoch': 96, 'train_loss': 0.0706785387173295, 'val_loss': 0.42842200660705565, 'test_acc': 0.958}
{'fold': 9, 'epoch': 97, 'train_loss': 0.07276780721731484, 'val_loss': 0.2972362504005432, 'test_acc': 0.95}
{'fold': 9, 'epoch': 98, 'train_loss': 0.08681615118402988, 'val_loss': 0.3391481566429138, 'test_acc': 0.948}
{'fold': 9, 'epoch': 99, 'train_loss': 0.07243813791079447, 'val_loss': 0.32222066831588747, 'test_acc': 0.958}
{'fold': 9, 'epoch': 100, 'train_loss': 0.0685550616006367, 'val_loss': 0.4718387019634247, 'test_acc': 0.952}
{'fold': 9, 'epoch': 101, 'train_loss': 0.06349804554251022, 'val_loss': 0.4380600757598877, 'test_acc': 0.96}
{'fold': 9, 'epoch': 102, 'train_loss': 0.07922988155856729, 'val_loss': 0.36576760959625243, 'test_acc': 0.952}
{'fold': 9, 'epoch': 103, 'train_loss': 0.06394728539511561, 'val_loss': 0.4334082970619202, 'test_acc': 0.96}
{'fold': 9, 'epoch': 104, 'train_loss': 0.06483103939332067, 'val_loss': 0.3774547128677368, 'test_acc': 0.958}
{'fold': 9, 'epoch': 105, 'train_loss': 0.06305290264077484, 'val_loss': 0.4896889934539795, 'test_acc': 0.956}
{'fold': 9, 'epoch': 106, 'train_loss': 0.06116187183186412, 'val_loss': 0.4907684769630432, 'test_acc': 0.964}
{'fold': 9, 'epoch': 107, 'train_loss': 0.06995718275429681, 'val_loss': 0.3702438144683838, 'test_acc': 0.962}
{'fold': 9, 'epoch': 108, 'train_loss': 0.0699713872326538, 'val_loss': 0.3050433430671692, 'test_acc': 0.954}
{'fold': 9, 'epoch': 109, 'train_loss': 0.0701671583019197, 'val_loss': 0.30090617990493773, 'test_acc': 0.962}
{'fold': 9, 'epoch': 110, 'train_loss': 0.07202298951800913, 'val_loss': 0.35224755477905273, 'test_acc': 0.956}
{'fold': 9, 'epoch': 111, 'train_loss': 0.07427250534063205, 'val_loss': 0.36765090155601504, 'test_acc': 0.96}
{'fold': 9, 'epoch': 112, 'train_loss': 0.06498736998531968, 'val_loss': 0.3235937201976776, 'test_acc': 0.946}
{'fold': 9, 'epoch': 113, 'train_loss': 0.06578295262064784, 'val_loss': 0.27733874607086184, 'test_acc': 0.954}
{'fold': 9, 'epoch': 114, 'train_loss': 0.06806387153454124, 'val_loss': 0.34095031452178953, 'test_acc': 0.966}
{'fold': 9, 'epoch': 115, 'train_loss': 0.055975157623179256, 'val_loss': 0.3954272677898407, 'test_acc': 0.972}
{'fold': 9, 'epoch': 116, 'train_loss': 0.06037303446326405, 'val_loss': 0.38960841608047486, 'test_acc': 0.964}
{'fold': 9, 'epoch': 117, 'train_loss': 0.06621204291470349, 'val_loss': 0.43925929403305053, 'test_acc': 0.95}
{'fold': 9, 'epoch': 118, 'train_loss': 0.0653753570234403, 'val_loss': 0.44684999322891233, 'test_acc': 0.96}
{'fold': 9, 'epoch': 119, 'train_loss': 0.06592910889070482, 'val_loss': 0.3661706898212433, 'test_acc': 0.946}
{'fold': 9, 'epoch': 120, 'train_loss': 0.06511254378885496, 'val_loss': 0.40596199774742125, 'test_acc': 0.96}
{'fold': 9, 'epoch': 121, 'train_loss': 0.07092604399658739, 'val_loss': 0.4180022292137146, 'test_acc': 0.948}
{'fold': 9, 'epoch': 122, 'train_loss': 0.06317439642269164, 'val_loss': 0.32135748863220215, 'test_acc': 0.954}
{'fold': 9, 'epoch': 123, 'train_loss': 0.07354373116977513, 'val_loss': 0.35443649554252626, 'test_acc': 0.958}
{'fold': 9, 'epoch': 124, 'train_loss': 0.0643740092543885, 'val_loss': 0.4762943136692047, 'test_acc': 0.96}
{'fold': 9, 'epoch': 125, 'train_loss': 0.06437872029840946, 'val_loss': 0.3763079104423523, 'test_acc': 0.964}
{'fold': 9, 'epoch': 126, 'train_loss': 0.06461064138216897, 'val_loss': 0.3703603882789612, 'test_acc': 0.964}
{'fold': 9, 'epoch': 127, 'train_loss': 0.06493329472374171, 'val_loss': 0.32326127314567565, 'test_acc': 0.954}
{'fold': 9, 'epoch': 128, 'train_loss': 0.06268334926338866, 'val_loss': 0.45111868834495544, 'test_acc': 0.96}
{'fold': 9, 'epoch': 129, 'train_loss': 0.06256488622166216, 'val_loss': 0.6232899949550629, 'test_acc': 0.958}
{'fold': 9, 'epoch': 130, 'train_loss': 0.06503868253901601, 'val_loss': 0.40784348392486575, 'test_acc': 0.946}
{'fold': 9, 'epoch': 131, 'train_loss': 0.08528780383989215, 'val_loss': 0.4721821026802063, 'test_acc': 0.954}
{'fold': 9, 'epoch': 132, 'train_loss': 0.06963880014605821, 'val_loss': 0.4459614553451538, 'test_acc': 0.956}
{'fold': 9, 'epoch': 133, 'train_loss': 0.07078334922436624, 'val_loss': 0.37726057744026187, 'test_acc': 0.96}
{'fold': 9, 'epoch': 134, 'train_loss': 0.06667647789232432, 'val_loss': 0.4106090557575226, 'test_acc': 0.958}
{'fold': 9, 'epoch': 135, 'train_loss': 0.06690794108435512, 'val_loss': 0.449769912481308, 'test_acc': 0.952}
{'fold': 9, 'epoch': 136, 'train_loss': 0.059864886663854124, 'val_loss': 0.44740165519714353, 'test_acc': 0.956}
{'fold': 9, 'epoch': 137, 'train_loss': 0.06508557748049498, 'val_loss': 0.4511784229278564, 'test_acc': 0.958}
{'fold': 9, 'epoch': 138, 'train_loss': 0.06222702356404625, 'val_loss': 0.44239417219161986, 'test_acc': 0.962}
{'fold': 9, 'epoch': 139, 'train_loss': 0.06268376613035798, 'val_loss': 0.6173155999183655, 'test_acc': 0.956}
{'fold': 9, 'epoch': 140, 'train_loss': 0.05574246377218515, 'val_loss': 0.5210912911891937, 'test_acc': 0.956}
{'fold': 9, 'epoch': 141, 'train_loss': 0.057118035367457194, 'val_loss': 0.49536407804489135, 'test_acc': 0.948}
{'fold': 9, 'epoch': 142, 'train_loss': 0.11848808190319687, 'val_loss': 0.3902186903953552, 'test_acc': 0.958}
{'fold': 9, 'epoch': 143, 'train_loss': 0.06761950829066336, 'val_loss': 0.4437148728370667, 'test_acc': 0.96}
{'fold': 9, 'epoch': 144, 'train_loss': 0.06797332590911537, 'val_loss': 0.5753567700386047, 'test_acc': 0.954}
{'fold': 9, 'epoch': 145, 'train_loss': 0.06592255470808596, 'val_loss': 0.39248964953422544, 'test_acc': 0.946}
{'fold': 9, 'epoch': 146, 'train_loss': 0.06236916204914451, 'val_loss': 0.3929191496372223, 'test_acc': 0.962}
{'fold': 9, 'epoch': 147, 'train_loss': 0.061773423330741935, 'val_loss': 0.4149265239238739, 'test_acc': 0.956}
{'fold': 9, 'epoch': 148, 'train_loss': 0.06096255265525542, 'val_loss': 0.4939849989414215, 'test_acc': 0.954}
{'fold': 9, 'epoch': 149, 'train_loss': 0.06752751536900177, 'val_loss': 0.3760140781402588, 'test_acc': 0.958}
{'fold': 9, 'epoch': 150, 'train_loss': 0.06211485140724107, 'val_loss': 0.3643425130844116, 'test_acc': 0.956}
{'fold': 9, 'epoch': 151, 'train_loss': 0.05714219456072897, 'val_loss': 0.6732726969718933, 'test_acc': 0.958}
{'fold': 9, 'epoch': 152, 'train_loss': 0.055401350143365564, 'val_loss': 0.6412665419578553, 'test_acc': 0.958}
{'fold': 9, 'epoch': 153, 'train_loss': 0.057590470721479506, 'val_loss': 0.6321566133499146, 'test_acc': 0.958}
{'fold': 9, 'epoch': 154, 'train_loss': 0.07357452426105737, 'val_loss': 0.4513826222419739, 'test_acc': 0.95}
{'fold': 9, 'epoch': 155, 'train_loss': 0.07961379100568593, 'val_loss': 0.459255809545517, 'test_acc': 0.942}
{'fold': 9, 'epoch': 156, 'train_loss': 0.06894453530665487, 'val_loss': 0.43866130113601687, 'test_acc': 0.95}
{'fold': 9, 'epoch': 157, 'train_loss': 0.06016713623423129, 'val_loss': 0.49706378936767576, 'test_acc': 0.962}
{'fold': 9, 'epoch': 158, 'train_loss': 0.058167218365706506, 'val_loss': 0.43184481048583984, 'test_acc': 0.952}
{'fold': 9, 'epoch': 159, 'train_loss': 0.06656122437678277, 'val_loss': 0.4726364850997925, 'test_acc': 0.956}
{'fold': 9, 'epoch': 160, 'train_loss': 0.06741220769006759, 'val_loss': 0.6150111606121064, 'test_acc': 0.95}
{'fold': 9, 'epoch': 161, 'train_loss': 0.06665688905399293, 'val_loss': 0.484925333738327, 'test_acc': 0.94}
{'fold': 9, 'epoch': 162, 'train_loss': 0.07340226675383747, 'val_loss': 0.44142348098754886, 'test_acc': 0.948}
{'fold': 9, 'epoch': 163, 'train_loss': 0.06392026125453412, 'val_loss': 0.37080867862701417, 'test_acc': 0.942}
{'fold': 9, 'epoch': 164, 'train_loss': 0.07264643405098468, 'val_loss': 0.34132325506210326, 'test_acc': 0.944}
{'fold': 9, 'epoch': 165, 'train_loss': 0.06594456826802343, 'val_loss': 0.3080401885509491, 'test_acc': 0.934}
{'fold': 9, 'epoch': 166, 'train_loss': 0.07374565844977042, 'val_loss': 0.3328592829704285, 'test_acc': 0.944}
{'fold': 9, 'epoch': 167, 'train_loss': 0.06258544215699657, 'val_loss': 0.41388764142990114, 'test_acc': 0.95}
{'fold': 9, 'epoch': 168, 'train_loss': 0.06099468378350139, 'val_loss': 0.534974089384079, 'test_acc': 0.952}
{'fold': 9, 'epoch': 169, 'train_loss': 0.05718160237942357, 'val_loss': 0.5909821701049804, 'test_acc': 0.944}
{'fold': 9, 'epoch': 170, 'train_loss': 0.0682075406767035, 'val_loss': 0.5222291519641876, 'test_acc': 0.944}
{'fold': 9, 'epoch': 171, 'train_loss': 0.06981009573210031, 'val_loss': 0.3833893265724182, 'test_acc': 0.944}
{'fold': 9, 'epoch': 172, 'train_loss': 0.06797419618116692, 'val_loss': 0.37999071407318114, 'test_acc': 0.944}
{'fold': 9, 'epoch': 173, 'train_loss': 0.06125170411542058, 'val_loss': 0.3481375229358673, 'test_acc': 0.946}
{'fold': 9, 'epoch': 174, 'train_loss': 0.06340994943864643, 'val_loss': 0.4073381313085556, 'test_acc': 0.954}
{'fold': 9, 'epoch': 175, 'train_loss': 0.06633166133542545, 'val_loss': 0.4244492950439453, 'test_acc': 0.948}
{'fold': 9, 'epoch': 176, 'train_loss': 0.06677084463502979, 'val_loss': 0.4354285933971405, 'test_acc': 0.948}
{'fold': 9, 'epoch': 177, 'train_loss': 0.059372620629146694, 'val_loss': 0.4220269069671631, 'test_acc': 0.948}
{'fold': 9, 'epoch': 178, 'train_loss': 0.059965963810682296, 'val_loss': 0.4814315347671509, 'test_acc': 0.946}
{'fold': 9, 'epoch': 179, 'train_loss': 0.05822752012871206, 'val_loss': 0.4967356081008911, 'test_acc': 0.95}
{'fold': 9, 'epoch': 180, 'train_loss': 0.06426489537698217, 'val_loss': 0.5124721376895904, 'test_acc': 0.954}
{'fold': 9, 'epoch': 181, 'train_loss': 0.06463757624384016, 'val_loss': 0.5038776450157165, 'test_acc': 0.942}
{'fold': 9, 'epoch': 182, 'train_loss': 0.06752986320992932, 'val_loss': 0.3773254222869873, 'test_acc': 0.944}
{'fold': 9, 'epoch': 183, 'train_loss': 0.06678062223596498, 'val_loss': 0.408975344657898, 'test_acc': 0.94}
{'fold': 9, 'epoch': 184, 'train_loss': 0.06702082283445635, 'val_loss': 0.4448831038475037, 'test_acc': 0.934}
{'fold': 9, 'epoch': 185, 'train_loss': 0.060255395888816565, 'val_loss': 0.44273163461685183, 'test_acc': 0.942}
{'fold': 9, 'epoch': 186, 'train_loss': 0.06750643720850348, 'val_loss': 0.3934518599510193, 'test_acc': 0.954}
{'fold': 9, 'epoch': 187, 'train_loss': 0.06738428817596287, 'val_loss': 0.41476857805252076, 'test_acc': 0.944}
{'fold': 9, 'epoch': 188, 'train_loss': 0.06881249526049942, 'val_loss': 0.4177203459739685, 'test_acc': 0.952}
{'fold': 9, 'epoch': 189, 'train_loss': 0.06033575067529455, 'val_loss': 0.4056258013248444, 'test_acc': 0.95}
{'fold': 9, 'epoch': 190, 'train_loss': 0.056258951448835436, 'val_loss': 0.44173869705200197, 'test_acc': 0.954}
{'fold': 9, 'epoch': 191, 'train_loss': 0.0650394587451592, 'val_loss': 0.5034351079463959, 'test_acc': 0.952}
{'fold': 9, 'epoch': 192, 'train_loss': 0.06053730736573925, 'val_loss': 0.5581962020397186, 'test_acc': 0.958}
{'fold': 9, 'epoch': 193, 'train_loss': 0.07218228674959391, 'val_loss': 0.551130172252655, 'test_acc': 0.944}
{'fold': 9, 'epoch': 194, 'train_loss': 0.06445114595349878, 'val_loss': 0.6377118711471558, 'test_acc': 0.94}
{'fold': 9, 'epoch': 195, 'train_loss': 0.05581576515163761, 'val_loss': 0.6853399908542633, 'test_acc': 0.946}
{'fold': 9, 'epoch': 196, 'train_loss': 0.06975146556971595, 'val_loss': 0.40736506509780884, 'test_acc': 0.95}
{'fold': 9, 'epoch': 197, 'train_loss': 0.057149971374310556, 'val_loss': 0.4692530064582825, 'test_acc': 0.954}
{'fold': 9, 'epoch': 198, 'train_loss': 0.05640779970679432, 'val_loss': 0.5561376271247864, 'test_acc': 0.952}
{'fold': 9, 'epoch': 199, 'train_loss': 0.06337505311239511, 'val_loss': 0.6091167039871216, 'test_acc': 0.958}
{'fold': 9, 'epoch': 200, 'train_loss': 0.059222595687024294, 'val_loss': 0.6073405904769897, 'test_acc': 0.946}
Val Loss: 0.3070, Test Accuracy: 0.914 ± 0.045, Duration: 575.870
Best result - 0.914 ± 0.045
--
IMDB-MULTI - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=89, out_features=48, bias=False)
        (lin): Linear(in_features=89, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.6925, 0.7815, 1.0255], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.6744, 0.8451, 0.9818], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9019, 0.8769, 0.8789], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9789, 0.9653, 0.9600], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9890, 0.9970, 0.9978], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9737, 0.9657, 0.9638], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9059, 0.8816, 0.8793], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8973, 0.9361, 0.9860], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8659, 0.9036, 0.9390], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8742, 0.9142, 0.9511], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=32, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=3, bias=True)
  )
): 0.579 ± 0.039
MUTAG - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=7, out_features=48, bias=False)
        (lin): Linear(in_features=7, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9824, 0.9946, 0.9927], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9780, 0.9750, 0.9743], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9795, 0.9795, 0.9798], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9881, 0.9877, 0.9871], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9957, 0.9951, 0.9949], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9858, 0.9866, 0.9863], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9781, 0.9753, 0.9758], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9628, 0.9662, 0.9695], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9926, 0.9957, 0.9963], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([1.0012, 1.0051, 1.0052], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=160, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=2, bias=True)
  )
): 0.830 ± 0.092
IMDB-BINARY - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=136, out_features=48, bias=False)
        (lin): Linear(in_features=136, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9593, 0.9853, 1.0124], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9693, 0.9674, 0.9656], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9764, 0.9738, 0.9719], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9752, 0.9767, 0.9773], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9795, 0.9768, 0.9739], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9778, 0.9775, 0.9786], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9720, 0.9721, 0.9754], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9657, 0.9597, 0.9692], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9868, 1.0065, 1.0156], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9878, 1.0092, 1.0191], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=160, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=2, bias=True)
  )
): 0.792 ± 0.062
REDDIT-BINARY - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=1, out_features=48, bias=False)
        (lin): Linear(in_features=1, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9968, 0.9490, 0.9424], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9518, 0.9481, 0.8967], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9436, 0.9280, 0.9174], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9554, 0.9514, 0.9495], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9885, 0.9810, 0.9779], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9604, 0.9523, 0.9492], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9477, 0.9180, 0.9060], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9223, 0.9328, 0.9360], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9118, 0.9524, 0.9923], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8854, 0.9063, 0.9706], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=5760, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=2, bias=True)
  )
): 0.872 ± 0.025
DD - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=89, out_features=48, bias=False)
        (lin): Linear(in_features=89, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8244, 0.8893, 1.0906], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8551, 0.9111, 0.9508], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9254, 0.9214, 0.9207], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9557, 0.9563, 0.9573], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9664, 0.9692, 0.9786], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9454, 0.9498, 0.9532], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9074, 0.9063, 0.9072], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9100, 0.9343, 0.9606], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9244, 0.9900, 1.1118], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9196, 0.9929, 1.1344], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=4512, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=2, bias=True)
  )
): 0.857 ± 0.068
NCI1 - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=37, out_features=48, bias=False)
        (lin): Linear(in_features=37, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.7776, 0.8126, 1.0662], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8598, 0.8753, 0.8645], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9172, 0.9253, 0.9272], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9391, 0.9396, 0.9428], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9541, 0.9543, 0.9566], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9383, 0.9413, 0.9438], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9257, 0.9357, 0.9344], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8686, 0.8669, 0.8698], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.6091, 0.7816, 0.9823], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.7587, 0.8379, 1.0096], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=352, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=2, bias=True)
  )
): 0.781 ± 0.017
PROTEINS - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=3, out_features=48, bias=False)
        (lin): Linear(in_features=3, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9372, 1.0112, 1.0194], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9421, 0.9286, 0.9261], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9146, 0.9138, 0.9166], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9168, 0.9228, 0.9291], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9325, 0.9325, 0.9400], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9218, 0.9217, 0.9284], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9233, 0.9178, 0.9209], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9594, 0.9388, 0.9350], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9911, 1.0078, 1.0131], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9930, 1.0150, 1.0206], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=384, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=2, bias=True)
  )
): 0.742 ± 0.053
COLLAB - Classifier(
  (feature_extractor): GraUNet(
    (down_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=492, out_features=48, bias=False)
        (lin): Linear(in_features=492, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.3689, 0.4477, 1.0910], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.6016, 0.6942, 0.8123], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9010, 0.8513, 0.8221], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9455, 0.9242, 0.9183], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9384, 0.9355, 0.9348], device='cuda:0', requires_grad=True))
      )
    )
    (pools): ModuleList(
      (0): TopKPooling(48, ratio=0.9, multiplier=1.0)
      (1): TopKPooling(48, ratio=0.7, multiplier=1.0)
      (2): TopKPooling(48, ratio=0.6, multiplier=1.0)
      (3): TopKPooling(48, ratio=0.5, multiplier=1.0)
    )
    (bn_layers): ModuleList(
      (0): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (3): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (7): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (up_convs): ModuleList(
      (0): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.9327, 0.9246, 0.9331], device='cuda:0', requires_grad=True))
      )
      (1): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8889, 0.8399, 0.8469], device='cuda:0', requires_grad=True))
      )
      (2): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.8571, 0.7904, 0.8793], device='cuda:0', requires_grad=True))
      )
      (3): ConvG(
        (lin11): Linear(in_features=48, out_features=48, bias=False)
        (lin): Linear(in_features=48, out_features=48, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.5457, 0.6601, 0.8109], device='cuda:0', requires_grad=True))
      )
      (4): ConvG(
        (lin11): Linear(in_features=96, out_features=97, bias=False)
        (lin): Linear(in_features=96, out_features=97, bias=False)
        (prop11): Gra_inc(K=2, temp=Parameter containing:
        tensor([0.4446, 0.5829, 0.8180], device='cuda:0', requires_grad=True))
      )
    )
    (drop): Dropout(p=0.3, inplace=False)
  )
  (readout): Conv1dReadout(
    (conv1d_p1): Conv1d(1, 16, kernel_size=(97,), stride=(97,))
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv1d_p2): Conv1d(16, 32, kernel_size=(5,), stride=(1,))
  )
  (mlp): MLPClassifier(
    (h1_weights): Linear(in_features=832, out_features=128, bias=True)
    (h2_weights): Linear(in_features=128, out_features=3, bias=True)
  )
): 0.914 ± 0.045

Process finished with exit code 0
