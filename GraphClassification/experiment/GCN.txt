D:\Program\Anaconda\envs\pyg\python.exe F:/Project/BernNet/GraphClassification/main.py
--
IMDB-MULTI - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 2.76091325600942, 'val_loss': 1.1670846811930338, 'test_acc': 0.34}
{'fold': 9, 'epoch': 2, 'train_loss': 1.3480753167470296, 'val_loss': 1.114120864868164, 'test_acc': 0.3933333333333333}
{'fold': 9, 'epoch': 3, 'train_loss': 1.195647972424825, 'val_loss': 1.0716445287068685, 'test_acc': 0.37333333333333335}
{'fold': 9, 'epoch': 4, 'train_loss': 1.7752489471435546, 'val_loss': 1.0722030639648437, 'test_acc': 0.3933333333333333}
{'fold': 9, 'epoch': 5, 'train_loss': 1.1126642894744874, 'val_loss': 1.0614851888020833, 'test_acc': 0.37333333333333335}
{'fold': 9, 'epoch': 6, 'train_loss': 1.0967983547846476, 'val_loss': 1.13632568359375, 'test_acc': 0.43333333333333335}
{'fold': 9, 'epoch': 7, 'train_loss': 1.0984389209747314, 'val_loss': 1.051018778483073, 'test_acc': 0.4266666666666667}
{'fold': 9, 'epoch': 8, 'train_loss': 1.0721902831395467, 'val_loss': 1.067818094889323, 'test_acc': 0.44}
{'fold': 9, 'epoch': 9, 'train_loss': 1.1762198559443156, 'val_loss': 1.0674162928263347, 'test_acc': 0.4066666666666667}
{'fold': 9, 'epoch': 10, 'train_loss': 1.0968838373819987, 'val_loss': 1.0593125406901043, 'test_acc': 0.44}
{'fold': 9, 'epoch': 11, 'train_loss': 1.0463706763585408, 'val_loss': 1.0471326700846355, 'test_acc': 0.38}
{'fold': 9, 'epoch': 12, 'train_loss': 1.0603631528218587, 'val_loss': 1.0562266286214193, 'test_acc': 0.38}
{'fold': 9, 'epoch': 13, 'train_loss': 1.0663307396570842, 'val_loss': 1.0493523025512694, 'test_acc': 0.37333333333333335}
{'fold': 9, 'epoch': 14, 'train_loss': 1.2134896659851073, 'val_loss': 1.0426574325561524, 'test_acc': 0.4266666666666667}
{'fold': 9, 'epoch': 15, 'train_loss': 1.2301360925038656, 'val_loss': 1.028926010131836, 'test_acc': 0.4666666666666667}
{'fold': 9, 'epoch': 16, 'train_loss': 1.0593924808502198, 'val_loss': 1.0472512817382813, 'test_acc': 0.41333333333333333}
{'fold': 9, 'epoch': 17, 'train_loss': 1.0628737020492554, 'val_loss': 1.054784393310547, 'test_acc': 0.38666666666666666}
{'fold': 9, 'epoch': 18, 'train_loss': 1.3030306180318196, 'val_loss': 1.0169234212239584, 'test_acc': 0.44666666666666666}
{'fold': 9, 'epoch': 19, 'train_loss': 1.2031006336212158, 'val_loss': 1.0187349192301431, 'test_acc': 0.46}
{'fold': 9, 'epoch': 20, 'train_loss': 1.04414360443751, 'val_loss': 1.0229932530721029, 'test_acc': 0.4533333333333333}
{'fold': 9, 'epoch': 21, 'train_loss': 1.043551460901896, 'val_loss': 1.0168211364746094, 'test_acc': 0.4266666666666667}
{'fold': 9, 'epoch': 22, 'train_loss': 1.0481156587600708, 'val_loss': 1.0144210052490235, 'test_acc': 0.4266666666666667}
{'fold': 9, 'epoch': 23, 'train_loss': 1.0436813441912334, 'val_loss': 1.0116999053955078, 'test_acc': 0.44}
{'fold': 9, 'epoch': 24, 'train_loss': 1.0406019401550293, 'val_loss': 1.0066666793823242, 'test_acc': 0.43333333333333335}
{'fold': 9, 'epoch': 25, 'train_loss': 1.118476611773173, 'val_loss': 1.01599978129069, 'test_acc': 0.43333333333333335}
{'fold': 9, 'epoch': 26, 'train_loss': 1.0426267910003661, 'val_loss': 1.0198479588826497, 'test_acc': 0.44}
{'fold': 9, 'epoch': 27, 'train_loss': 1.0735525353749593, 'val_loss': 1.0401024627685547, 'test_acc': 0.4266666666666667}
{'fold': 9, 'epoch': 28, 'train_loss': 1.0575560935338337, 'val_loss': 1.0216736857096353, 'test_acc': 0.42}
{'fold': 9, 'epoch': 29, 'train_loss': 1.9394710381825766, 'val_loss': 1.0909986877441407, 'test_acc': 0.36}
{'fold': 9, 'epoch': 30, 'train_loss': 2.732019918759664, 'val_loss': 1.1022755559285482, 'test_acc': 0.38}
{'fold': 9, 'epoch': 31, 'train_loss': 1.3785694026947022, 'val_loss': 1.1212301635742188, 'test_acc': 0.36}
{'fold': 9, 'epoch': 32, 'train_loss': 1.1346157296498616, 'val_loss': 1.0967090606689454, 'test_acc': 0.34}
{'fold': 9, 'epoch': 33, 'train_loss': 1.1881706492106119, 'val_loss': 1.0965991083780924, 'test_acc': 0.34}
{'fold': 9, 'epoch': 34, 'train_loss': 1.1686319303512573, 'val_loss': 1.0971266810099285, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 35, 'train_loss': 1.1185301224390665, 'val_loss': 1.0971164576212564, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 36, 'train_loss': 1.1089947748184203, 'val_loss': 1.098284683227539, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 37, 'train_loss': 1.1091260226567585, 'val_loss': 1.0982866287231445, 'test_acc': 0.34}
{'fold': 9, 'epoch': 38, 'train_loss': 1.1102969773610434, 'val_loss': 1.0967730204264323, 'test_acc': 0.32666666666666666}
{'fold': 9, 'epoch': 39, 'train_loss': 1.1120947329203288, 'val_loss': 1.0968466440836588, 'test_acc': 0.32666666666666666}
{'fold': 9, 'epoch': 40, 'train_loss': 1.1004546721776327, 'val_loss': 1.0968347295125325, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 41, 'train_loss': 1.1046548414230346, 'val_loss': 1.0968554178873697, 'test_acc': 0.32666666666666666}
{'fold': 9, 'epoch': 42, 'train_loss': 1.1097587569554648, 'val_loss': 1.0954051462809244, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 43, 'train_loss': 1.09720814704895, 'val_loss': 1.096440264383952, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 44, 'train_loss': 1.3979053036371867, 'val_loss': 1.0962850952148437, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 45, 'train_loss': 1.0982292143503825, 'val_loss': 1.096420987447103, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 46, 'train_loss': 1.1291068998972575, 'val_loss': 1.0965251541137695, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 47, 'train_loss': 1.0978962262471517, 'val_loss': 1.0966295623779296, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 48, 'train_loss': 1.0979908768335978, 'val_loss': 1.0966129811604817, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 49, 'train_loss': 1.0968838469187419, 'val_loss': 1.0964907709757488, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 50, 'train_loss': 1.0978913275400797, 'val_loss': 1.0964822006225585, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 51, 'train_loss': 1.0976098203659057, 'val_loss': 1.0964219665527344, 'test_acc': 0.32666666666666666}
{'fold': 9, 'epoch': 52, 'train_loss': 1.098260210355123, 'val_loss': 1.0964936701456707, 'test_acc': 0.32666666666666666}
{'fold': 9, 'epoch': 53, 'train_loss': 1.097823247909546, 'val_loss': 1.0967901102701823, 'test_acc': 0.34}
{'fold': 9, 'epoch': 54, 'train_loss': 1.0975571823120118, 'val_loss': 1.0970084126790365, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 55, 'train_loss': 1.0975468889872233, 'val_loss': 1.0971472040812174, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 56, 'train_loss': 1.0974728600184123, 'val_loss': 1.0971865590413412, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 57, 'train_loss': 1.0976797024408975, 'val_loss': 1.0973855463663738, 'test_acc': 0.34}
{'fold': 9, 'epoch': 58, 'train_loss': 1.0970377349853515, 'val_loss': 1.0972584533691405, 'test_acc': 0.34}
{'fold': 9, 'epoch': 59, 'train_loss': 1.0978944285710652, 'val_loss': 1.0978892771402995, 'test_acc': 0.34}
{'fold': 9, 'epoch': 60, 'train_loss': 1.097219869295756, 'val_loss': 1.0978356297810872, 'test_acc': 0.34}
{'fold': 9, 'epoch': 61, 'train_loss': 1.1263652722040811, 'val_loss': 1.0978599421183268, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 62, 'train_loss': 1.097529756228129, 'val_loss': 1.0947345733642577, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 63, 'train_loss': 1.097405007680257, 'val_loss': 1.0947501373291015, 'test_acc': 0.34}
{'fold': 9, 'epoch': 64, 'train_loss': 1.0974554030100505, 'val_loss': 1.0946979141235351, 'test_acc': 0.34}
{'fold': 9, 'epoch': 65, 'train_loss': 2.6786008214950563, 'val_loss': 1.09475523630778, 'test_acc': 0.34}
{'fold': 9, 'epoch': 66, 'train_loss': 1.0940908432006835, 'val_loss': 1.0769495391845703, 'test_acc': 0.36666666666666664}
{'fold': 9, 'epoch': 67, 'train_loss': 1.1270791737238566, 'val_loss': 1.0987090301513671, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 68, 'train_loss': 1.0976185337702433, 'val_loss': 1.0947903569539388, 'test_acc': 0.34}
{'fold': 9, 'epoch': 69, 'train_loss': 1.0918429390589397, 'val_loss': 1.0891571172078451, 'test_acc': 0.34}
{'fold': 9, 'epoch': 70, 'train_loss': 1.0938948154449464, 'val_loss': 1.0872933451334637, 'test_acc': 0.36}
{'fold': 9, 'epoch': 71, 'train_loss': 1.1019736735026042, 'val_loss': 1.085470542907715, 'test_acc': 0.3466666666666667}
{'fold': 9, 'epoch': 72, 'train_loss': 1.0927001237869263, 'val_loss': 1.0835202407836915, 'test_acc': 0.3466666666666667}
{'fold': 9, 'epoch': 73, 'train_loss': 1.0922347450256347, 'val_loss': 1.0787167612711588, 'test_acc': 0.3466666666666667}
{'fold': 9, 'epoch': 74, 'train_loss': 1.0976932922999063, 'val_loss': 1.077278594970703, 'test_acc': 0.3466666666666667}
{'fold': 9, 'epoch': 75, 'train_loss': 1.171244708697001, 'val_loss': 1.085757039388021, 'test_acc': 0.3466666666666667}
{'fold': 9, 'epoch': 76, 'train_loss': 1.0910660600662232, 'val_loss': 1.0842499033610027, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 77, 'train_loss': 1.092850562731425, 'val_loss': 1.0869801839192708, 'test_acc': 0.34}
{'fold': 9, 'epoch': 78, 'train_loss': 1.0919899829228719, 'val_loss': 1.0846248118082682, 'test_acc': 0.34}
{'fold': 9, 'epoch': 79, 'train_loss': 1.0921104383468627, 'val_loss': 1.079594014485677, 'test_acc': 0.34}
{'fold': 9, 'epoch': 80, 'train_loss': 1.0931012392044068, 'val_loss': 1.0786984125773111, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 81, 'train_loss': 1.090529236793518, 'val_loss': 1.0777080408732096, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 82, 'train_loss': 1.63883429368337, 'val_loss': 1.0771760813395181, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 83, 'train_loss': 1.0934689013163248, 'val_loss': 1.0764512634277343, 'test_acc': 0.34}
{'fold': 9, 'epoch': 84, 'train_loss': 1.0910309441884358, 'val_loss': 1.0788747024536134, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 85, 'train_loss': 1.0914988533655803, 'val_loss': 1.079342041015625, 'test_acc': 0.34}
{'fold': 9, 'epoch': 86, 'train_loss': 1.0917246389389037, 'val_loss': 1.0794934209187825, 'test_acc': 0.34}
{'fold': 9, 'epoch': 87, 'train_loss': 1.0914003769556682, 'val_loss': 1.0790347035725911, 'test_acc': 0.34}
{'fold': 9, 'epoch': 88, 'train_loss': 1.0912213786443075, 'val_loss': 1.0787788645426433, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 89, 'train_loss': 1.0919370635350545, 'val_loss': 1.07767941792806, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 90, 'train_loss': 1.0916977071762084, 'val_loss': 1.0771217981974284, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 91, 'train_loss': 1.092507874170939, 'val_loss': 1.0766585286458332, 'test_acc': 0.34}
{'fold': 9, 'epoch': 92, 'train_loss': 1.0917189041773478, 'val_loss': 1.077444839477539, 'test_acc': 0.34}
{'fold': 9, 'epoch': 93, 'train_loss': 1.0915584548314412, 'val_loss': 1.0770803197224934, 'test_acc': 0.34}
{'fold': 9, 'epoch': 94, 'train_loss': 1.0916297801335653, 'val_loss': 1.0770775731404623, 'test_acc': 0.34}
{'fold': 9, 'epoch': 95, 'train_loss': 1.0913727362950643, 'val_loss': 1.0768672434488933, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 96, 'train_loss': 1.0921041170756023, 'val_loss': 1.0766448593139648, 'test_acc': 0.34}
{'fold': 9, 'epoch': 97, 'train_loss': 1.0911170212427774, 'val_loss': 1.076822230021159, 'test_acc': 0.3333333333333333}
{'fold': 9, 'epoch': 98, 'train_loss': 1.0908516311645509, 'val_loss': 1.076569569905599, 'test_acc': 0.34}
{'fold': 9, 'epoch': 99, 'train_loss': 1.0912937275568644, 'val_loss': 1.0768559646606446, 'test_acc': 0.34}
{'fold': 9, 'epoch': 100, 'train_loss': 1.0899847730000813, 'val_loss': 1.076786944071452, 'test_acc': 0.3333333333333333}
Val Loss: 1.0117, Test Accuracy: 0.461 ± 0.034, Duration: 37.831
Best result - 0.461 ± 0.034
--
MUTAG - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.6536503716519004, 'val_loss': 0.6179595523410373, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6150355746871546, 'val_loss': 0.8497858047485352, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6951442172652796, 'val_loss': 0.6132994757758247, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 4, 'train_loss': 0.618254520391163, 'val_loss': 0.6300198237101237, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6304832194980822, 'val_loss': 0.5988864898681641, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6024045818730405, 'val_loss': 0.5508104960123698, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5395273519189734, 'val_loss': 0.5958325597974989, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 8, 'train_loss': 0.4754381869968615, 'val_loss': 0.5496856371561686, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5581318775289937, 'val_loss': 1.0800944434271917, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5687707976291054, 'val_loss': 0.48951201968722874, 'test_acc': 0.6111111111111112}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5146701680986505, 'val_loss': 0.4908936818440755, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4730154542546523, 'val_loss': 0.7898620499504937, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5455252528190613, 'val_loss': 0.6089995702107748, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5039186069839879, 'val_loss': 0.4596858024597168, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 15, 'train_loss': 0.47268864355589213, 'val_loss': 0.41583998998006183, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 16, 'train_loss': 0.44665359039055674, 'val_loss': 0.44065308570861816, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 17, 'train_loss': 0.42441330614842865, 'val_loss': 0.6157850689358182, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 18, 'train_loss': 0.4277247661038449, 'val_loss': 0.5146865314907498, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 19, 'train_loss': 0.40457362093423543, 'val_loss': 0.41386551327175564, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 20, 'train_loss': 0.4468957750420821, 'val_loss': 0.527787102593316, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 21, 'train_loss': 0.4022381493919774, 'val_loss': 0.8070222006903754, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 22, 'train_loss': 0.45002342917417226, 'val_loss': 0.7305267651875814, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 23, 'train_loss': 0.4345931285306027, 'val_loss': 0.4393223391638862, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 24, 'train_loss': 0.41593561674419205, 'val_loss': 0.4512682490878635, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 25, 'train_loss': 0.472733188616602, 'val_loss': 0.4122857517666287, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 26, 'train_loss': 0.4251714897783179, 'val_loss': 0.46428966522216797, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 27, 'train_loss': 0.4005698153847142, 'val_loss': 0.6494149102105035, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 28, 'train_loss': 0.3782098167820981, 'val_loss': 0.5965073373582628, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 29, 'train_loss': 0.36068982042764364, 'val_loss': 0.5205774307250977, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 30, 'train_loss': 0.33040950957097504, 'val_loss': 0.6113333172268338, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 31, 'train_loss': 0.35915742422405045, 'val_loss': 0.6969143549601237, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 32, 'train_loss': 0.34065588524467066, 'val_loss': 0.6835000250074599, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 33, 'train_loss': 0.3400102351841174, 'val_loss': 0.4449281692504883, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 34, 'train_loss': 0.34230655274893107, 'val_loss': 0.4654530949062771, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 35, 'train_loss': 0.4421281359697643, 'val_loss': 0.6771386464436849, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 36, 'train_loss': 0.3311536735609958, 'val_loss': 0.8913501103719076, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 37, 'train_loss': 0.38950779563502264, 'val_loss': 0.5407516691419814, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 38, 'train_loss': 0.34288060821984945, 'val_loss': 0.46506214141845703, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 39, 'train_loss': 0.3785755838218488, 'val_loss': 0.5962807867262099, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 40, 'train_loss': 0.2994042234985452, 'val_loss': 0.8014978302849664, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 41, 'train_loss': 0.2878063462282482, 'val_loss': 0.6265234417385526, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 42, 'train_loss': 0.294542333013133, 'val_loss': 0.5057520336574979, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 43, 'train_loss': 0.345890569843744, 'val_loss': 0.5838277075025771, 'test_acc': 0.7222222222222222}
{'fold': 9, 'epoch': 44, 'train_loss': 0.30209705390428243, 'val_loss': 0.9359620412190756, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 45, 'train_loss': 0.2973559138021971, 'val_loss': 0.35601597362094456, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 46, 'train_loss': 0.3092734390183499, 'val_loss': 0.4935230678982205, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 47, 'train_loss': 0.3131715184763858, 'val_loss': 0.5433967378404405, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 48, 'train_loss': 0.30840797643912465, 'val_loss': 0.5550386110941569, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 49, 'train_loss': 0.2914541504885021, 'val_loss': 0.5361073282029893, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 50, 'train_loss': 0.35037950622408015, 'val_loss': 0.6619669066535102, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 51, 'train_loss': 0.31246501991623327, 'val_loss': 0.8086144659254286, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 52, 'train_loss': 0.32111947316872447, 'val_loss': 0.9547052383422852, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 53, 'train_loss': 0.32160693799194534, 'val_loss': 0.6500639915466309, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 54, 'train_loss': 0.2982014182366823, 'val_loss': 0.4791008101569282, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 55, 'train_loss': 0.2845651679917386, 'val_loss': 0.539605458577474, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 56, 'train_loss': 0.2676906224928404, 'val_loss': 0.5614474084642198, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 57, 'train_loss': 0.25937432442840774, 'val_loss': 0.5858168601989746, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 58, 'train_loss': 0.25091323413346944, 'val_loss': 0.627300951215956, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 59, 'train_loss': 0.2540668609895204, 'val_loss': 0.497207694583469, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 60, 'train_loss': 0.24792162446599258, 'val_loss': 0.3212612999810113, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 61, 'train_loss': 0.3358337738012013, 'val_loss': 1.2417368359035916, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 62, 'train_loss': 0.4142802012594123, 'val_loss': 0.7596893310546875, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 63, 'train_loss': 0.3217372408038692, 'val_loss': 0.5192511346605089, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 64, 'train_loss': 0.29799597671157435, 'val_loss': 0.7705476548936632, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 65, 'train_loss': 0.3066335461641613, 'val_loss': 0.9008566538492838, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 66, 'train_loss': 0.32353065986382334, 'val_loss': 1.325523058573405, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 67, 'train_loss': 0.37360175973490667, 'val_loss': 0.5282518068949381, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 68, 'train_loss': 0.32432955189755086, 'val_loss': 0.33414236704508465, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 69, 'train_loss': 0.3884876715509515, 'val_loss': 0.287337064743042, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 70, 'train_loss': 0.3888168397702669, 'val_loss': 0.3505249553256565, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 71, 'train_loss': 0.36205985671595525, 'val_loss': 0.900711801317003, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 72, 'train_loss': 0.37028986686154414, 'val_loss': 0.995716412862142, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 73, 'train_loss': 0.48551070062737717, 'val_loss': 0.34554362297058105, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 74, 'train_loss': 0.43360386396709244, 'val_loss': 0.32481813430786133, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 75, 'train_loss': 0.42417726234385844, 'val_loss': 0.41261325942145455, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 76, 'train_loss': 0.36530664406324687, 'val_loss': 0.718342834048801, 'test_acc': 0.7777777777777778}
{'fold': 9, 'epoch': 77, 'train_loss': 0.38475364289785685, 'val_loss': 1.1416011386447482, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 78, 'train_loss': 0.36969948128650065, 'val_loss': 0.9383959240383573, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 79, 'train_loss': 0.35079080807535273, 'val_loss': 0.5016378826565213, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 80, 'train_loss': 0.3735472230534804, 'val_loss': 0.414291434817844, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 81, 'train_loss': 0.36346519934503657, 'val_loss': 0.43665117687649196, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 82, 'train_loss': 0.37156253739407186, 'val_loss': 0.5075332853529189, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 83, 'train_loss': 0.3676698725474508, 'val_loss': 0.5597897635565864, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 84, 'train_loss': 0.3389434045866916, 'val_loss': 0.6698611577351888, 'test_acc': 0.9444444444444444}
{'fold': 9, 'epoch': 85, 'train_loss': 0.385634593273464, 'val_loss': 0.4881172709994846, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 86, 'train_loss': 0.3823181014311941, 'val_loss': 0.5042151345147027, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 87, 'train_loss': 0.3459378214258897, 'val_loss': 0.6496148109436035, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 88, 'train_loss': 0.32375424002346237, 'val_loss': 0.5553151766459147, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 89, 'train_loss': 0.3251084104964608, 'val_loss': 0.459574646419949, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 90, 'train_loss': 0.3116920982536517, 'val_loss': 0.4030020236968994, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 91, 'train_loss': 0.351511382742932, 'val_loss': 0.43046122127109104, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 92, 'train_loss': 0.29947340410006673, 'val_loss': 0.48844030168321395, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 93, 'train_loss': 0.2765233650019294, 'val_loss': 0.9928276273939345, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 94, 'train_loss': 0.3103093975468686, 'val_loss': 0.9403245713975694, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 95, 'train_loss': 0.2917099202934064, 'val_loss': 0.8476930194430881, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 96, 'train_loss': 0.27659496037583603, 'val_loss': 0.8006863064236112, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 97, 'train_loss': 0.2781245708465576, 'val_loss': 0.7540965610080295, 'test_acc': 0.8888888888888888}
{'fold': 9, 'epoch': 98, 'train_loss': 0.2860628648808128, 'val_loss': 0.5823865996466743, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 99, 'train_loss': 0.2924995281194386, 'val_loss': 0.4756734636094835, 'test_acc': 0.8333333333333334}
{'fold': 9, 'epoch': 100, 'train_loss': 0.30748082618964345, 'val_loss': 0.43468334939744735, 'test_acc': 0.8333333333333334}
Val Loss: 0.3534, Test Accuracy: 0.782 ± 0.090, Duration: 7.308
Best result - 0.782 ± 0.090
--
IMDB-BINARY - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 3.6712294960021974, 'val_loss': 5.674599609375, 'test_acc': 0.54}
{'fold': 9, 'epoch': 2, 'train_loss': 3.6771076488494874, 'val_loss': 0.7524420166015625, 'test_acc': 0.54}
{'fold': 9, 'epoch': 3, 'train_loss': 0.8986490535736084, 'val_loss': 0.5984687805175781, 'test_acc': 0.68}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6322573924064636, 'val_loss': 0.6422040557861328, 'test_acc': 0.63}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6005197358131409, 'val_loss': 0.5644943237304687, 'test_acc': 0.7}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5607735347747803, 'val_loss': 0.5250515365600585, 'test_acc': 0.66}
{'fold': 9, 'epoch': 7, 'train_loss': 0.563245701789856, 'val_loss': 0.5294973373413085, 'test_acc': 0.71}
{'fold': 9, 'epoch': 8, 'train_loss': 0.5800131487846375, 'val_loss': 0.5421211242675781, 'test_acc': 0.68}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5166213369369507, 'val_loss': 0.5053057098388671, 'test_acc': 0.65}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5136414003372193, 'val_loss': 0.4982233810424805, 'test_acc': 0.67}
{'fold': 9, 'epoch': 11, 'train_loss': 0.502330721616745, 'val_loss': 0.5225558471679688, 'test_acc': 0.68}
{'fold': 9, 'epoch': 12, 'train_loss': 0.48767013430595396, 'val_loss': 0.49096778869628904, 'test_acc': 0.69}
{'fold': 9, 'epoch': 13, 'train_loss': 0.492658326625824, 'val_loss': 0.4968433380126953, 'test_acc': 0.66}
{'fold': 9, 'epoch': 14, 'train_loss': 0.4749550747871399, 'val_loss': 0.4935304260253906, 'test_acc': 0.66}
{'fold': 9, 'epoch': 15, 'train_loss': 0.4675858354568481, 'val_loss': 0.48488861083984375, 'test_acc': 0.73}
{'fold': 9, 'epoch': 16, 'train_loss': 0.4642142379283905, 'val_loss': 0.4839426040649414, 'test_acc': 0.69}
{'fold': 9, 'epoch': 17, 'train_loss': 0.71436408162117, 'val_loss': 0.5257369995117187, 'test_acc': 0.66}
{'fold': 9, 'epoch': 18, 'train_loss': 0.47276849269866944, 'val_loss': 0.48071468353271485, 'test_acc': 0.68}
{'fold': 9, 'epoch': 19, 'train_loss': 0.4815341019630432, 'val_loss': 0.47768680572509764, 'test_acc': 0.69}
{'fold': 9, 'epoch': 20, 'train_loss': 0.4480306303501129, 'val_loss': 0.4702779006958008, 'test_acc': 0.69}
{'fold': 9, 'epoch': 21, 'train_loss': 0.4640896129608154, 'val_loss': 0.48151809692382813, 'test_acc': 0.68}
{'fold': 9, 'epoch': 22, 'train_loss': 0.44707624793052675, 'val_loss': 0.45698047637939454, 'test_acc': 0.73}
{'fold': 9, 'epoch': 23, 'train_loss': 0.43434953451156616, 'val_loss': 0.4848393249511719, 'test_acc': 0.67}
{'fold': 9, 'epoch': 24, 'train_loss': 0.478035489320755, 'val_loss': 0.4748556137084961, 'test_acc': 0.67}
{'fold': 9, 'epoch': 25, 'train_loss': 0.42654460191726684, 'val_loss': 0.5271551132202148, 'test_acc': 0.68}
{'fold': 9, 'epoch': 26, 'train_loss': 0.5591298019886017, 'val_loss': 0.5051200485229492, 'test_acc': 0.71}
{'fold': 9, 'epoch': 27, 'train_loss': 1.1107426297664642, 'val_loss': 0.561953239440918, 'test_acc': 0.69}
{'fold': 9, 'epoch': 28, 'train_loss': 0.502789431810379, 'val_loss': 0.4851547622680664, 'test_acc': 0.66}
{'fold': 9, 'epoch': 29, 'train_loss': 0.4355848503112793, 'val_loss': 0.49162437438964846, 'test_acc': 0.67}
{'fold': 9, 'epoch': 30, 'train_loss': 0.4254604721069336, 'val_loss': 2.206592254638672, 'test_acc': 0.7}
{'fold': 9, 'epoch': 31, 'train_loss': 0.6637333524227143, 'val_loss': 0.5680059432983399, 'test_acc': 0.68}
{'fold': 9, 'epoch': 32, 'train_loss': 0.4865853750705719, 'val_loss': 0.6379084014892578, 'test_acc': 0.64}
{'fold': 9, 'epoch': 33, 'train_loss': 0.43790380120277406, 'val_loss': 0.543544921875, 'test_acc': 0.68}
{'fold': 9, 'epoch': 34, 'train_loss': 0.49885100245475766, 'val_loss': 0.5808579254150391, 'test_acc': 0.67}
{'fold': 9, 'epoch': 35, 'train_loss': 0.8594849944114685, 'val_loss': 0.5845208358764649, 'test_acc': 0.67}
{'fold': 9, 'epoch': 36, 'train_loss': 0.5035871601104737, 'val_loss': 0.6204666137695313, 'test_acc': 0.67}
{'fold': 9, 'epoch': 37, 'train_loss': 0.5239365315437317, 'val_loss': 0.5495312118530273, 'test_acc': 0.73}
{'fold': 9, 'epoch': 38, 'train_loss': 0.5762360632419586, 'val_loss': 0.6419672393798828, 'test_acc': 0.6}
{'fold': 9, 'epoch': 39, 'train_loss': 0.5985349035263061, 'val_loss': 0.6050590896606445, 'test_acc': 0.67}
{'fold': 9, 'epoch': 40, 'train_loss': 0.5232867825031281, 'val_loss': 0.6769538116455078, 'test_acc': 0.68}
{'fold': 9, 'epoch': 41, 'train_loss': 0.4831736278533936, 'val_loss': 0.6945845031738281, 'test_acc': 0.63}
{'fold': 9, 'epoch': 42, 'train_loss': 0.5115282392501831, 'val_loss': 0.6194720840454102, 'test_acc': 0.69}
{'fold': 9, 'epoch': 43, 'train_loss': 0.4580954682826996, 'val_loss': 0.633770866394043, 'test_acc': 0.67}
{'fold': 9, 'epoch': 44, 'train_loss': 0.46046323895454405, 'val_loss': 0.6364965438842773, 'test_acc': 0.63}
{'fold': 9, 'epoch': 45, 'train_loss': 0.4495394492149353, 'val_loss': 0.635260009765625, 'test_acc': 0.67}
{'fold': 9, 'epoch': 46, 'train_loss': 0.4323707580566406, 'val_loss': 0.616890640258789, 'test_acc': 0.67}
{'fold': 9, 'epoch': 47, 'train_loss': 0.40252283215522766, 'val_loss': 0.647156982421875, 'test_acc': 0.7}
{'fold': 9, 'epoch': 48, 'train_loss': 0.43134365081787107, 'val_loss': 0.5476943206787109, 'test_acc': 0.64}
{'fold': 9, 'epoch': 49, 'train_loss': 0.38652258038520815, 'val_loss': 0.675518798828125, 'test_acc': 0.66}
{'fold': 9, 'epoch': 50, 'train_loss': 0.3998895943164825, 'val_loss': 0.5967404556274414, 'test_acc': 0.64}
{'fold': 9, 'epoch': 51, 'train_loss': 0.4010892295837402, 'val_loss': 0.6123562622070312, 'test_acc': 0.71}
{'fold': 9, 'epoch': 52, 'train_loss': 0.38045608878135684, 'val_loss': 0.6309680557250976, 'test_acc': 0.72}
{'fold': 9, 'epoch': 53, 'train_loss': 0.3774377775192261, 'val_loss': 0.6702007293701172, 'test_acc': 0.69}
{'fold': 9, 'epoch': 54, 'train_loss': 0.36149672627449037, 'val_loss': 0.640556411743164, 'test_acc': 0.63}
{'fold': 9, 'epoch': 55, 'train_loss': 0.3524997329711914, 'val_loss': 0.7654244995117188, 'test_acc': 0.65}
{'fold': 9, 'epoch': 56, 'train_loss': 0.37279716014862063, 'val_loss': 0.9080743408203125, 'test_acc': 0.64}
{'fold': 9, 'epoch': 57, 'train_loss': 0.3808793020248413, 'val_loss': 0.6775995635986328, 'test_acc': 0.72}
{'fold': 9, 'epoch': 58, 'train_loss': 0.3737508368492126, 'val_loss': 0.5691915130615235, 'test_acc': 0.68}
{'fold': 9, 'epoch': 59, 'train_loss': 0.38089901447296143, 'val_loss': 0.6227316665649414, 'test_acc': 0.67}
{'fold': 9, 'epoch': 60, 'train_loss': 0.3558691120147705, 'val_loss': 0.5801688385009766, 'test_acc': 0.66}
{'fold': 9, 'epoch': 61, 'train_loss': 0.39298863053321836, 'val_loss': 0.5869196319580078, 'test_acc': 0.66}
{'fold': 9, 'epoch': 62, 'train_loss': 0.34848527193069456, 'val_loss': 0.6393391036987305, 'test_acc': 0.67}
{'fold': 9, 'epoch': 63, 'train_loss': 0.3562593185901642, 'val_loss': 0.6854366302490235, 'test_acc': 0.65}
{'fold': 9, 'epoch': 64, 'train_loss': 0.33242363572120665, 'val_loss': 0.5971691513061523, 'test_acc': 0.72}
{'fold': 9, 'epoch': 65, 'train_loss': 0.35593067049980165, 'val_loss': 0.6032835388183594, 'test_acc': 0.69}
{'fold': 9, 'epoch': 66, 'train_loss': 0.4051637756824493, 'val_loss': 0.6142567062377929, 'test_acc': 0.68}
{'fold': 9, 'epoch': 67, 'train_loss': 0.3738001358509064, 'val_loss': 0.6743392181396485, 'test_acc': 0.67}
{'fold': 9, 'epoch': 68, 'train_loss': 0.3254167366027832, 'val_loss': 0.705853500366211, 'test_acc': 0.66}
{'fold': 9, 'epoch': 69, 'train_loss': 0.3143104088306427, 'val_loss': 0.8046388244628906, 'test_acc': 0.72}
{'fold': 9, 'epoch': 70, 'train_loss': 0.4490481948852539, 'val_loss': 0.6475034332275391, 'test_acc': 0.64}
{'fold': 9, 'epoch': 71, 'train_loss': 0.3477472698688507, 'val_loss': 0.6764187622070312, 'test_acc': 0.68}
{'fold': 9, 'epoch': 72, 'train_loss': 0.33937479078769683, 'val_loss': 0.7745126342773437, 'test_acc': 0.71}
{'fold': 9, 'epoch': 73, 'train_loss': 0.7299982625246048, 'val_loss': 0.71596435546875, 'test_acc': 0.74}
{'fold': 9, 'epoch': 74, 'train_loss': 0.6661689639091491, 'val_loss': 0.7156455993652344, 'test_acc': 0.69}
{'fold': 9, 'epoch': 75, 'train_loss': 0.4194022178649902, 'val_loss': 0.7457181549072266, 'test_acc': 0.65}
{'fold': 9, 'epoch': 76, 'train_loss': 0.4032567501068115, 'val_loss': 0.7193196868896484, 'test_acc': 0.7}
{'fold': 9, 'epoch': 77, 'train_loss': 0.3766990756988525, 'val_loss': 0.7427369689941407, 'test_acc': 0.71}
{'fold': 9, 'epoch': 78, 'train_loss': 0.38984495401382446, 'val_loss': 0.7946047210693359, 'test_acc': 0.69}
{'fold': 9, 'epoch': 79, 'train_loss': 0.5102623605728149, 'val_loss': 0.7228044891357421, 'test_acc': 0.66}
{'fold': 9, 'epoch': 80, 'train_loss': 0.5533561837673188, 'val_loss': 0.8324523162841797, 'test_acc': 0.69}
{'fold': 9, 'epoch': 81, 'train_loss': 0.4187198531627655, 'val_loss': 0.750063247680664, 'test_acc': 0.64}
{'fold': 9, 'epoch': 82, 'train_loss': 0.41134522438049315, 'val_loss': 0.6356755065917968, 'test_acc': 0.72}
{'fold': 9, 'epoch': 83, 'train_loss': 0.4192809629440308, 'val_loss': 0.654116439819336, 'test_acc': 0.65}
{'fold': 9, 'epoch': 84, 'train_loss': 0.44594318151474, 'val_loss': 0.6531655883789063, 'test_acc': 0.65}
{'fold': 9, 'epoch': 85, 'train_loss': 0.4321028172969818, 'val_loss': 0.6225376510620118, 'test_acc': 0.7}
{'fold': 9, 'epoch': 86, 'train_loss': 1.12729887008667, 'val_loss': 0.6278173065185547, 'test_acc': 0.71}
{'fold': 9, 'epoch': 87, 'train_loss': 0.9695181941986084, 'val_loss': 0.6019820022583008, 'test_acc': 0.62}
{'fold': 9, 'epoch': 88, 'train_loss': 0.7944796872138977, 'val_loss': 0.6786087036132813, 'test_acc': 0.6}
{'fold': 9, 'epoch': 89, 'train_loss': 0.4544319796562195, 'val_loss': 0.6852135467529297, 'test_acc': 0.63}
{'fold': 9, 'epoch': 90, 'train_loss': 0.5354226613044739, 'val_loss': 0.682716064453125, 'test_acc': 0.71}
{'fold': 9, 'epoch': 91, 'train_loss': 0.4816950941085815, 'val_loss': 0.7071012115478515, 'test_acc': 0.7}
{'fold': 9, 'epoch': 92, 'train_loss': 0.4953339469432831, 'val_loss': 0.7665927886962891, 'test_acc': 0.69}
{'fold': 9, 'epoch': 93, 'train_loss': 4.655799984931946, 'val_loss': 0.6694377136230468, 'test_acc': 0.61}
{'fold': 9, 'epoch': 94, 'train_loss': 1.1701434421539307, 'val_loss': 0.6896085357666015, 'test_acc': 0.65}
{'fold': 9, 'epoch': 95, 'train_loss': 0.555517828464508, 'val_loss': 0.605351448059082, 'test_acc': 0.62}
{'fold': 9, 'epoch': 96, 'train_loss': 30.094692608118056, 'val_loss': 0.6449873352050781, 'test_acc': 0.63}
{'fold': 9, 'epoch': 97, 'train_loss': 0.6407167148590088, 'val_loss': 0.6799141693115235, 'test_acc': 0.65}
{'fold': 9, 'epoch': 98, 'train_loss': 0.5228699004650116, 'val_loss': 0.677066650390625, 'test_acc': 0.66}
{'fold': 9, 'epoch': 99, 'train_loss': 0.4789276349544525, 'val_loss': 0.6923932647705078, 'test_acc': 0.67}
{'fold': 9, 'epoch': 100, 'train_loss': 0.5179062056541442, 'val_loss': 0.7028207397460937, 'test_acc': 0.69}
Val Loss: 0.4870, Test Accuracy: 0.705 ± 0.049, Duration: 27.161
Best result - 0.705 ± 0.049
--
REDDIT-BINARY - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 44.0006432390213, 'val_loss': 0.6600317573547363, 'test_acc': 0.505}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6945399236679077, 'val_loss': 0.583440113067627, 'test_acc': 0.675}
{'fold': 9, 'epoch': 3, 'train_loss': 0.592069103717804, 'val_loss': 0.4960982990264893, 'test_acc': 0.715}
{'fold': 9, 'epoch': 4, 'train_loss': 0.5536035823822022, 'val_loss': 0.5012921524047852, 'test_acc': 0.725}
{'fold': 9, 'epoch': 5, 'train_loss': 0.8444285988807678, 'val_loss': 0.5315514087677002, 'test_acc': 0.695}
{'fold': 9, 'epoch': 6, 'train_loss': 1.102638680934906, 'val_loss': 0.4685219669342041, 'test_acc': 0.75}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5795151948928833, 'val_loss': 0.4675784969329834, 'test_acc': 0.795}
{'fold': 9, 'epoch': 8, 'train_loss': 1.7060868763923644, 'val_loss': 0.7410376071929932, 'test_acc': 0.745}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5480257534980774, 'val_loss': 0.5275967502593994, 'test_acc': 0.685}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5061969304084778, 'val_loss': 0.4589305686950684, 'test_acc': 0.72}
{'fold': 9, 'epoch': 11, 'train_loss': 0.6025126242637634, 'val_loss': 0.46246315956115724, 'test_acc': 0.745}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4771389317512512, 'val_loss': 0.44629197120666503, 'test_acc': 0.735}
{'fold': 9, 'epoch': 13, 'train_loss': 0.4599468469619751, 'val_loss': 0.43017939567565916, 'test_acc': 0.77}
{'fold': 9, 'epoch': 14, 'train_loss': 0.469849112033844, 'val_loss': 0.3985705661773682, 'test_acc': 0.805}
{'fold': 9, 'epoch': 15, 'train_loss': 2.008711712360382, 'val_loss': 0.40997260093688964, 'test_acc': 0.82}
{'fold': 9, 'epoch': 16, 'train_loss': 0.44239362597465515, 'val_loss': 0.3508348655700684, 'test_acc': 0.825}
{'fold': 9, 'epoch': 17, 'train_loss': 0.43780087113380434, 'val_loss': 0.39791152000427243, 'test_acc': 0.82}
{'fold': 9, 'epoch': 18, 'train_loss': 0.4370730924606323, 'val_loss': 0.3953192710876465, 'test_acc': 0.785}
{'fold': 9, 'epoch': 19, 'train_loss': 0.42766775727272033, 'val_loss': 0.35677906990051267, 'test_acc': 0.82}
{'fold': 9, 'epoch': 20, 'train_loss': 0.3875122046470642, 'val_loss': 0.3259413623809814, 'test_acc': 0.85}
{'fold': 9, 'epoch': 21, 'train_loss': 0.39101338386535645, 'val_loss': 0.3698772048950195, 'test_acc': 0.84}
{'fold': 9, 'epoch': 22, 'train_loss': 0.3870484864711761, 'val_loss': 0.32415719985961916, 'test_acc': 0.86}
{'fold': 9, 'epoch': 23, 'train_loss': 0.38226624608039855, 'val_loss': 0.31694162845611573, 'test_acc': 0.835}
{'fold': 9, 'epoch': 24, 'train_loss': 0.395796115398407, 'val_loss': 0.351899209022522, 'test_acc': 0.87}
{'fold': 9, 'epoch': 25, 'train_loss': 0.4104136073589325, 'val_loss': 0.3180933713912964, 'test_acc': 0.865}
{'fold': 9, 'epoch': 26, 'train_loss': 0.42466556906700137, 'val_loss': 0.3416260242462158, 'test_acc': 0.885}
{'fold': 9, 'epoch': 27, 'train_loss': 0.39289997935295107, 'val_loss': 0.33538965225219725, 'test_acc': 0.88}
{'fold': 9, 'epoch': 28, 'train_loss': 0.38608837127685547, 'val_loss': 0.2976051235198975, 'test_acc': 0.865}
{'fold': 9, 'epoch': 29, 'train_loss': 0.3636741244792938, 'val_loss': 0.3598240852355957, 'test_acc': 0.86}
{'fold': 9, 'epoch': 30, 'train_loss': 0.3759116101264954, 'val_loss': 0.3258014106750488, 'test_acc': 0.875}
{'fold': 9, 'epoch': 31, 'train_loss': 0.3555147063732147, 'val_loss': 0.30839491844177247, 'test_acc': 0.88}
{'fold': 9, 'epoch': 32, 'train_loss': 0.37366777181625366, 'val_loss': 0.292254056930542, 'test_acc': 0.865}
{'fold': 9, 'epoch': 33, 'train_loss': 1.0845661628246308, 'val_loss': 0.3241700077056885, 'test_acc': 0.895}
{'fold': 9, 'epoch': 34, 'train_loss': 0.36399766206741335, 'val_loss': 0.29125969409942626, 'test_acc': 0.885}
{'fold': 9, 'epoch': 35, 'train_loss': 0.33958402514457703, 'val_loss': 0.3184689140319824, 'test_acc': 0.895}
{'fold': 9, 'epoch': 36, 'train_loss': 0.3743641996383667, 'val_loss': 0.30730256557464597, 'test_acc': 0.845}
{'fold': 9, 'epoch': 37, 'train_loss': 0.35377912402153017, 'val_loss': 0.32870912551879883, 'test_acc': 0.88}
{'fold': 9, 'epoch': 38, 'train_loss': 0.3477967941761017, 'val_loss': 0.3007968330383301, 'test_acc': 0.88}
{'fold': 9, 'epoch': 39, 'train_loss': 0.33971947312355044, 'val_loss': 0.37711678504943846, 'test_acc': 0.845}
{'fold': 9, 'epoch': 40, 'train_loss': 0.36304002404212954, 'val_loss': 0.2940348482131958, 'test_acc': 0.84}
{'fold': 9, 'epoch': 41, 'train_loss': 0.3438996016979218, 'val_loss': 0.2972456693649292, 'test_acc': 0.83}
{'fold': 9, 'epoch': 42, 'train_loss': 0.33031095564365387, 'val_loss': 0.28103845596313476, 'test_acc': 0.895}
{'fold': 9, 'epoch': 43, 'train_loss': 0.3257161521911621, 'val_loss': 0.29692753314971926, 'test_acc': 0.875}
{'fold': 9, 'epoch': 44, 'train_loss': 0.34470497608184814, 'val_loss': 0.39648898124694826, 'test_acc': 0.82}
{'fold': 9, 'epoch': 45, 'train_loss': 0.34213004767894745, 'val_loss': 0.3046386098861694, 'test_acc': 0.85}
{'fold': 9, 'epoch': 46, 'train_loss': 0.3375431561470032, 'val_loss': 0.31118825912475584, 'test_acc': 0.875}
{'fold': 9, 'epoch': 47, 'train_loss': 0.35020660638809203, 'val_loss': 0.2963465881347656, 'test_acc': 0.865}
{'fold': 9, 'epoch': 48, 'train_loss': 0.35087764978408814, 'val_loss': 0.29109424114227295, 'test_acc': 0.895}
{'fold': 9, 'epoch': 49, 'train_loss': 0.33297094106674197, 'val_loss': 0.29281245231628417, 'test_acc': 0.865}
{'fold': 9, 'epoch': 50, 'train_loss': 0.3359426784515381, 'val_loss': 0.2540833377838135, 'test_acc': 0.86}
{'fold': 9, 'epoch': 51, 'train_loss': 0.333708256483078, 'val_loss': 0.27045943260192873, 'test_acc': 0.86}
{'fold': 9, 'epoch': 52, 'train_loss': 0.3319106805324554, 'val_loss': 0.2716844081878662, 'test_acc': 0.87}
{'fold': 9, 'epoch': 53, 'train_loss': 0.3486736154556274, 'val_loss': 0.27001405239105225, 'test_acc': 0.895}
{'fold': 9, 'epoch': 54, 'train_loss': 0.32539835572242737, 'val_loss': 0.3351530838012695, 'test_acc': 0.845}
{'fold': 9, 'epoch': 55, 'train_loss': 0.34839140057563783, 'val_loss': 0.31757164478302, 'test_acc': 0.855}
{'fold': 9, 'epoch': 56, 'train_loss': 0.335704460144043, 'val_loss': 0.2926850080490112, 'test_acc': 0.9}
{'fold': 9, 'epoch': 57, 'train_loss': 0.30132102966308594, 'val_loss': 0.2883825016021728, 'test_acc': 0.88}
{'fold': 9, 'epoch': 58, 'train_loss': 0.309498233795166, 'val_loss': 0.27961001396179197, 'test_acc': 0.865}
{'fold': 9, 'epoch': 59, 'train_loss': 0.2991320836544037, 'val_loss': 0.26587882995605466, 'test_acc': 0.865}
{'fold': 9, 'epoch': 60, 'train_loss': 0.32226953983306883, 'val_loss': 0.331283802986145, 'test_acc': 0.87}
{'fold': 9, 'epoch': 61, 'train_loss': 0.32421451330184936, 'val_loss': 0.31265677452087404, 'test_acc': 0.85}
{'fold': 9, 'epoch': 62, 'train_loss': 0.31517683148384096, 'val_loss': 0.2659814023971558, 'test_acc': 0.855}
{'fold': 9, 'epoch': 63, 'train_loss': 0.30566108226776123, 'val_loss': 0.27647974491119387, 'test_acc': 0.875}
{'fold': 9, 'epoch': 64, 'train_loss': 0.2851735246181488, 'val_loss': 0.282313814163208, 'test_acc': 0.875}
{'fold': 9, 'epoch': 65, 'train_loss': 0.3119274580478668, 'val_loss': 0.2850929355621338, 'test_acc': 0.87}
{'fold': 9, 'epoch': 66, 'train_loss': 0.29992427706718444, 'val_loss': 0.2872690773010254, 'test_acc': 0.86}
{'fold': 9, 'epoch': 67, 'train_loss': 0.2912373208999634, 'val_loss': 0.25420910358428955, 'test_acc': 0.84}
{'fold': 9, 'epoch': 68, 'train_loss': 0.3407460260391235, 'val_loss': 0.287271203994751, 'test_acc': 0.87}
{'fold': 9, 'epoch': 69, 'train_loss': 0.32212782025337217, 'val_loss': 0.3053538751602173, 'test_acc': 0.865}
{'fold': 9, 'epoch': 70, 'train_loss': 0.3062904441356659, 'val_loss': 0.28178112030029295, 'test_acc': 0.865}
{'fold': 9, 'epoch': 71, 'train_loss': 0.31783784449100494, 'val_loss': 0.26305528163909914, 'test_acc': 0.885}
{'fold': 9, 'epoch': 72, 'train_loss': 0.3081694394350052, 'val_loss': 0.2884492778778076, 'test_acc': 0.885}
{'fold': 9, 'epoch': 73, 'train_loss': 0.2946882855892181, 'val_loss': 0.26744377613067627, 'test_acc': 0.865}
{'fold': 9, 'epoch': 74, 'train_loss': 0.3030633521080017, 'val_loss': 0.2787838363647461, 'test_acc': 0.855}
{'fold': 9, 'epoch': 75, 'train_loss': 0.29772107541561127, 'val_loss': 0.23743291854858398, 'test_acc': 0.875}
{'fold': 9, 'epoch': 76, 'train_loss': 0.28956768989562987, 'val_loss': 0.2564339971542358, 'test_acc': 0.86}
{'fold': 9, 'epoch': 77, 'train_loss': 0.27653659343719483, 'val_loss': 0.26740766525268556, 'test_acc': 0.885}
{'fold': 9, 'epoch': 78, 'train_loss': 0.28083309054374694, 'val_loss': 0.2410317087173462, 'test_acc': 0.9}
{'fold': 9, 'epoch': 79, 'train_loss': 0.3366242289543152, 'val_loss': 0.31630610466003417, 'test_acc': 0.83}
{'fold': 9, 'epoch': 80, 'train_loss': 1.24340989112854, 'val_loss': 0.414395809173584, 'test_acc': 0.83}
{'fold': 9, 'epoch': 81, 'train_loss': 2.6386181712150574, 'val_loss': 0.5800728130340577, 'test_acc': 0.73}
{'fold': 9, 'epoch': 82, 'train_loss': 0.7315207123756409, 'val_loss': 0.32840115547180176, 'test_acc': 0.84}
{'fold': 9, 'epoch': 83, 'train_loss': 5.398284213542938, 'val_loss': 0.45096626281738283, 'test_acc': 0.75}
{'fold': 9, 'epoch': 84, 'train_loss': 0.459815434217453, 'val_loss': 0.3540672397613525, 'test_acc': 0.84}
{'fold': 9, 'epoch': 85, 'train_loss': 0.40803097128868104, 'val_loss': 0.32884461402893067, 'test_acc': 0.855}
{'fold': 9, 'epoch': 86, 'train_loss': 0.4516464352607727, 'val_loss': 0.38157492637634277, 'test_acc': 0.855}
{'fold': 9, 'epoch': 87, 'train_loss': 0.44946494698524475, 'val_loss': 0.3186816644668579, 'test_acc': 0.875}
{'fold': 9, 'epoch': 88, 'train_loss': 0.38587054967880247, 'val_loss': 0.2900449323654175, 'test_acc': 0.875}
{'fold': 9, 'epoch': 89, 'train_loss': 0.3631381249427795, 'val_loss': 0.29900598526000977, 'test_acc': 0.88}
{'fold': 9, 'epoch': 90, 'train_loss': 0.3457159876823425, 'val_loss': 0.2876515579223633, 'test_acc': 0.865}
{'fold': 9, 'epoch': 91, 'train_loss': 0.35274056434631346, 'val_loss': 0.30693681716918947, 'test_acc': 0.83}
{'fold': 9, 'epoch': 92, 'train_loss': 17.44248851418495, 'val_loss': 0.26243521690368654, 'test_acc': 0.87}
{'fold': 9, 'epoch': 93, 'train_loss': 0.38517364621162414, 'val_loss': 0.3177350091934204, 'test_acc': 0.835}
{'fold': 9, 'epoch': 94, 'train_loss': 0.41095666408538817, 'val_loss': 0.3523652935028076, 'test_acc': 0.83}
{'fold': 9, 'epoch': 95, 'train_loss': 0.36626420497894285, 'val_loss': 0.27272640228271483, 'test_acc': 0.83}
{'fold': 9, 'epoch': 96, 'train_loss': 0.37287477612495423, 'val_loss': 0.2901860618591309, 'test_acc': 0.815}
{'fold': 9, 'epoch': 97, 'train_loss': 0.35414706349372865, 'val_loss': 0.3091679573059082, 'test_acc': 0.83}
{'fold': 9, 'epoch': 98, 'train_loss': 0.3645974934101105, 'val_loss': 0.27965476989746096, 'test_acc': 0.865}
{'fold': 9, 'epoch': 99, 'train_loss': 0.3881558489799499, 'val_loss': 0.3030397415161133, 'test_acc': 0.865}
{'fold': 9, 'epoch': 100, 'train_loss': 0.33154082655906675, 'val_loss': 0.27640031814575194, 'test_acc': 0.845}
Val Loss: 0.3092, Test Accuracy: 0.860 ± 0.032, Duration: 47.347
Best result - 0.860 ± 0.032
--
DD - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.7079252903744325, 'val_loss': 0.6787246964935564, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6975930666519423, 'val_loss': 0.6799986749632746, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6796576320114782, 'val_loss': 0.6775116227630876, 'test_acc': 0.5897435897435898}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6725189039262675, 'val_loss': 0.6715400239341279, 'test_acc': 0.5555555555555556}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6368207557726715, 'val_loss': 0.6591605162009214, 'test_acc': 0.6410256410256411}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5938155863244655, 'val_loss': 0.6947348178961338, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 7, 'train_loss': 0.5440914560172517, 'val_loss': 0.6203884548611112, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 8, 'train_loss': 0.505667176287053, 'val_loss': 0.6240701267861912, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5075040873834642, 'val_loss': 0.5755572848849826, 'test_acc': 0.7692307692307693}
{'fold': 9, 'epoch': 10, 'train_loss': 0.45842037615129505, 'val_loss': 0.612141340206831, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 11, 'train_loss': 0.4650060170787876, 'val_loss': 0.6019572070521167, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 12, 'train_loss': 0.4014177236516597, 'val_loss': 0.611235594138121, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 13, 'train_loss': 0.3783137121443021, 'val_loss': 0.5988807352180154, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 14, 'train_loss': 0.37408580214290293, 'val_loss': 0.6589425241845286, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 15, 'train_loss': 0.34891584010447485, 'val_loss': 0.5976318620209001, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 16, 'train_loss': 0.31592314298880303, 'val_loss': 0.6476289798051883, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 17, 'train_loss': 0.3281263580261651, 'val_loss': 0.6076979025816306, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 18, 'train_loss': 0.29174590792696353, 'val_loss': 0.6700260129749266, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 19, 'train_loss': 0.2518448587191307, 'val_loss': 0.6933761987930689, 'test_acc': 0.7521367521367521}
{'fold': 9, 'epoch': 20, 'train_loss': 0.25445035049470804, 'val_loss': 0.7408691797501001, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 21, 'train_loss': 0.23703745209564597, 'val_loss': 0.7014537061381544, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 22, 'train_loss': 0.2828445659350541, 'val_loss': 0.8878748315012354, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 23, 'train_loss': 0.25094476443226055, 'val_loss': 0.7296353201580863, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 24, 'train_loss': 0.2040211305779926, 'val_loss': 1.0396137074527578, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 25, 'train_loss': 0.2441096944829165, 'val_loss': 0.8260181785648705, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 26, 'train_loss': 0.17467028540322335, 'val_loss': 1.0256063347188835, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 27, 'train_loss': 0.11193511745560977, 'val_loss': 1.2163070287459936, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 28, 'train_loss': 0.1399702293387914, 'val_loss': 1.187382103031517, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 29, 'train_loss': 0.22349095748642744, 'val_loss': 0.9947246975368924, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 30, 'train_loss': 0.2335346184544644, 'val_loss': 0.7884812314286191, 'test_acc': 0.6495726495726496}
{'fold': 9, 'epoch': 31, 'train_loss': 0.19404594903275119, 'val_loss': 0.8069124140291133, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 32, 'train_loss': 0.13439779311923658, 'val_loss': 0.9512202597071981, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 33, 'train_loss': 0.10437090922210177, 'val_loss': 1.20047610030215, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 34, 'train_loss': 0.08874873835909164, 'val_loss': 1.0296270908453526, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 35, 'train_loss': 0.04855366019626795, 'val_loss': 1.4979128063234508, 'test_acc': 0.6495726495726496}
{'fold': 9, 'epoch': 36, 'train_loss': 0.18348222617375648, 'val_loss': 1.2105059501452324, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 37, 'train_loss': 0.1343895430534573, 'val_loss': 0.9159824990818644, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 38, 'train_loss': 0.1239128482796378, 'val_loss': 1.2374932704827724, 'test_acc': 0.6581196581196581}
{'fold': 9, 'epoch': 39, 'train_loss': 0.06537300449306682, 'val_loss': 1.535750951522436, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 40, 'train_loss': 0.05277717691230572, 'val_loss': 1.5238458356286726, 'test_acc': 0.7521367521367521}
{'fold': 9, 'epoch': 41, 'train_loss': 0.09303499758243561, 'val_loss': 1.5728514581663995, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 42, 'train_loss': 0.08575058917878038, 'val_loss': 1.3319569123096955, 'test_acc': 0.6324786324786325}
{'fold': 9, 'epoch': 43, 'train_loss': 0.040664274940046215, 'val_loss': 1.426646697215545, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 44, 'train_loss': 0.03409181996169737, 'val_loss': 1.9133076464009082, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 45, 'train_loss': 0.028797998511374504, 'val_loss': 1.7790269118088942, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 46, 'train_loss': 0.02636466743582386, 'val_loss': 1.7983760996761484, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 47, 'train_loss': 0.02847578913225966, 'val_loss': 1.9416449131109776, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 48, 'train_loss': 0.11080640185056097, 'val_loss': 1.494426499065171, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 49, 'train_loss': 0.11818167696691166, 'val_loss': 1.1260818090194311, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 50, 'train_loss': 0.09071128657561238, 'val_loss': 1.039249778812767, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 51, 'train_loss': 0.04334255383681443, 'val_loss': 1.519103743072249, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 52, 'train_loss': 0.026086836370592146, 'val_loss': 2.049873547676282, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 53, 'train_loss': 0.01953092820437278, 'val_loss': 2.1179136618589745, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 54, 'train_loss': 0.0334525890648365, 'val_loss': 2.1253002199352298, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 55, 'train_loss': 0.01582602595405306, 'val_loss': 2.1369669335520167, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 56, 'train_loss': 0.013276227086908737, 'val_loss': 2.0377315945095487, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 57, 'train_loss': 0.011366221722278554, 'val_loss': 2.2243289784488516, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 58, 'train_loss': 0.016659399462958514, 'val_loss': 2.147161369649773, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 59, 'train_loss': 0.020168498880624518, 'val_loss': 2.198223929119925, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 60, 'train_loss': 0.008145694999750389, 'val_loss': 2.4165790264423075, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 61, 'train_loss': 0.00670484655883984, 'val_loss': 2.5519260504306893, 'test_acc': 0.7948717948717948}
{'fold': 9, 'epoch': 62, 'train_loss': 0.011241482847007149, 'val_loss': 2.562073014740251, 'test_acc': 0.7521367521367521}
{'fold': 9, 'epoch': 63, 'train_loss': 0.010486424979517016, 'val_loss': 2.7794698079427085, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 64, 'train_loss': 0.026947327612769016, 'val_loss': 2.622454781817575, 'test_acc': 0.7435897435897436}
{'fold': 9, 'epoch': 65, 'train_loss': 0.009894736632073329, 'val_loss': 2.7306265545706463, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 66, 'train_loss': 0.006721631477792132, 'val_loss': 2.416736309344952, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 67, 'train_loss': 0.005326197567490577, 'val_loss': 2.5049023750500803, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 68, 'train_loss': 0.012085799222527924, 'val_loss': 2.6878346500233707, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 69, 'train_loss': 0.005808390668096116, 'val_loss': 2.888053698417468, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 70, 'train_loss': 0.009889004823072986, 'val_loss': 2.6732151650974894, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 71, 'train_loss': 0.006717446759358933, 'val_loss': 2.7792376656817575, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 72, 'train_loss': 0.0013748350142801212, 'val_loss': 3.0606532952724357, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 73, 'train_loss': 0.002072973531879035, 'val_loss': 3.039306640625, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 74, 'train_loss': 0.004332210326476528, 'val_loss': 3.176300831330128, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 75, 'train_loss': 0.022879739658970197, 'val_loss': 3.0497457390157585, 'test_acc': 0.7008547008547008}
{'fold': 9, 'epoch': 76, 'train_loss': 0.035523352165848524, 'val_loss': 2.58535401433961, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 77, 'train_loss': 0.015454440930132138, 'val_loss': 2.4783043494591346, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 78, 'train_loss': 0.02680513132521409, 'val_loss': 2.3020786383213143, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 79, 'train_loss': 0.02138420699511544, 'val_loss': 2.046720716688368, 'test_acc': 0.7264957264957265}
{'fold': 9, 'epoch': 80, 'train_loss': 0.08421769990759381, 'val_loss': 1.7747320191473024, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 81, 'train_loss': 0.05469235083309271, 'val_loss': 1.8652527637970753, 'test_acc': 0.6581196581196581}
{'fold': 9, 'epoch': 82, 'train_loss': 0.09622251785407632, 'val_loss': 1.3201236561832266, 'test_acc': 0.7350427350427351}
{'fold': 9, 'epoch': 83, 'train_loss': 0.07511169463396072, 'val_loss': 1.6081664256560497, 'test_acc': 0.6837606837606838}
{'fold': 9, 'epoch': 84, 'train_loss': 0.16807869708133957, 'val_loss': 1.1091187305939503, 'test_acc': 0.6923076923076923}
{'fold': 9, 'epoch': 85, 'train_loss': 0.07236851050944651, 'val_loss': 1.1924335447132077, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 86, 'train_loss': 0.032352015754934085, 'val_loss': 1.5007664607121394, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 87, 'train_loss': 0.024326743779041, 'val_loss': 1.9492308787810497, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 88, 'train_loss': 0.014249047106605465, 'val_loss': 2.110142112797142, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 89, 'train_loss': 0.0075416282545459476, 'val_loss': 2.096810234917535, 'test_acc': 0.6752136752136753}
{'fold': 9, 'epoch': 90, 'train_loss': 0.012275025564230095, 'val_loss': 2.292376135149573, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 91, 'train_loss': 0.02056064766089795, 'val_loss': 2.3480777577457266, 'test_acc': 0.6410256410256411}
{'fold': 9, 'epoch': 92, 'train_loss': 0.003410128436556433, 'val_loss': 2.1797894860944176, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 93, 'train_loss': 0.013503147277318068, 'val_loss': 2.3949858836638622, 'test_acc': 0.6495726495726496}
{'fold': 9, 'epoch': 94, 'train_loss': 0.0036944488805355663, 'val_loss': 2.2986353686732106, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 95, 'train_loss': 0.006212633814449596, 'val_loss': 2.9412526187733707, 'test_acc': 0.6153846153846154}
{'fold': 9, 'epoch': 96, 'train_loss': 0.0057659065972429585, 'val_loss': 2.5659902197682958, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 97, 'train_loss': 0.012721003889563983, 'val_loss': 2.7895296536959133, 'test_acc': 0.717948717948718}
{'fold': 9, 'epoch': 98, 'train_loss': 0.0236539439303392, 'val_loss': 2.758369641426282, 'test_acc': 0.7094017094017094}
{'fold': 9, 'epoch': 99, 'train_loss': 0.025267125849249, 'val_loss': 2.7292809119591346, 'test_acc': 0.6239316239316239}
{'fold': 9, 'epoch': 100, 'train_loss': 0.029316901339817854, 'val_loss': 1.8310018686147838, 'test_acc': 0.6837606837606838}
Val Loss: 0.5100, Test Accuracy: 0.754 ± 0.027, Duration: 28.157
Best result - 0.754 ± 0.027
--
NCI1 - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 0.708962493424288, 'val_loss': 0.7013539548627942, 'test_acc': 0.49878345498783455}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6987029195121895, 'val_loss': 0.6741138397922191, 'test_acc': 0.5109489051094891}
{'fold': 9, 'epoch': 3, 'train_loss': 0.6837372295467813, 'val_loss': 0.6849776080054958, 'test_acc': 0.5644768856447688}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6794860873489195, 'val_loss': 0.6789753442843175, 'test_acc': 0.5669099756690997}
{'fold': 9, 'epoch': 5, 'train_loss': 0.6647392124155141, 'val_loss': 0.6351758348970807, 'test_acc': 0.6228710462287105}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6339462695330599, 'val_loss': 0.5847895847329838, 'test_acc': 0.6253041362530414}
{'fold': 9, 'epoch': 7, 'train_loss': 0.6361014119319962, 'val_loss': 0.6277495613933479, 'test_acc': 0.5815085158150851}
{'fold': 9, 'epoch': 8, 'train_loss': 0.6275983489052803, 'val_loss': 0.5510356466845584, 'test_acc': 0.6618004866180048}
{'fold': 9, 'epoch': 9, 'train_loss': 0.6117919502177087, 'val_loss': 0.5541682231745291, 'test_acc': 0.6788321167883211}
{'fold': 9, 'epoch': 10, 'train_loss': 0.6061816390993532, 'val_loss': 0.5737635758671448, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 11, 'train_loss': 0.6007870261686562, 'val_loss': 0.5500617294125894, 'test_acc': 0.6618004866180048}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5881466355056948, 'val_loss': 0.535864486601521, 'test_acc': 0.6739659367396593}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5882373476840573, 'val_loss': 0.5829011958881016, 'test_acc': 0.6399026763990268}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5750054856286432, 'val_loss': 0.5582439394762916, 'test_acc': 0.6788321167883211}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5635524457388551, 'val_loss': 0.5406254213801845, 'test_acc': 0.681265206812652}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5586659425366534, 'val_loss': 0.55849718989537, 'test_acc': 0.6690997566909975}
{'fold': 9, 'epoch': 17, 'train_loss': 0.5579742826974595, 'val_loss': 0.5465563381965433, 'test_acc': 0.6520681265206812}
{'fold': 9, 'epoch': 18, 'train_loss': 0.569924715806678, 'val_loss': 0.5184564474435328, 'test_acc': 0.6715328467153284}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5516858235762937, 'val_loss': 0.5474812491386766, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 20, 'train_loss': 0.5449082641125885, 'val_loss': 0.5707490624012448, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 21, 'train_loss': 0.5476984645618429, 'val_loss': 0.5870700520610578, 'test_acc': 0.6885644768856448}
{'fold': 9, 'epoch': 22, 'train_loss': 0.5311611581892863, 'val_loss': 0.5442565653445947, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 23, 'train_loss': 0.5377388930088702, 'val_loss': 0.5475563921777581, 'test_acc': 0.7031630170316302}
{'fold': 9, 'epoch': 24, 'train_loss': 0.5476348539338495, 'val_loss': 0.5049408798960293, 'test_acc': 0.6958637469586375}
{'fold': 9, 'epoch': 25, 'train_loss': 0.5353853911378957, 'val_loss': 0.5498063442480825, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 26, 'train_loss': 0.5345802138901685, 'val_loss': 0.5247664880868582, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 27, 'train_loss': 0.5443698140246445, 'val_loss': 0.5389340208104637, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 28, 'train_loss': 0.5341403010842863, 'val_loss': 0.5432470395907288, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 29, 'train_loss': 0.5178526744355251, 'val_loss': 0.534286396926917, 'test_acc': 0.7201946472019465}
{'fold': 9, 'epoch': 30, 'train_loss': 0.5256979594555504, 'val_loss': 0.5465028419401814, 'test_acc': 0.6934306569343066}
{'fold': 9, 'epoch': 31, 'train_loss': 0.5289222600976337, 'val_loss': 0.5487385074587634, 'test_acc': 0.708029197080292}
{'fold': 9, 'epoch': 32, 'train_loss': 0.5193492020423685, 'val_loss': 0.5002974674939529, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 33, 'train_loss': 0.5655653521672362, 'val_loss': 0.5721564606158402, 'test_acc': 0.681265206812652}
{'fold': 9, 'epoch': 34, 'train_loss': 0.5478317484513396, 'val_loss': 0.5129167552121945, 'test_acc': 0.6715328467153284}
{'fold': 9, 'epoch': 35, 'train_loss': 0.5406740023561928, 'val_loss': 0.507898602172406, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 36, 'train_loss': 0.5165167787359288, 'val_loss': 0.5096478659451152, 'test_acc': 0.7104622871046229}
{'fold': 9, 'epoch': 37, 'train_loss': 0.5201830067773805, 'val_loss': 0.5099373142214587, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 38, 'train_loss': 0.5197109294343749, 'val_loss': 0.48173678414374954, 'test_acc': 0.7055961070559611}
{'fold': 9, 'epoch': 39, 'train_loss': 0.5050568992494087, 'val_loss': 0.5056078277365135, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 40, 'train_loss': 0.5054109068972641, 'val_loss': 0.502593381561502, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 41, 'train_loss': 0.5082242429401462, 'val_loss': 0.4964833851278263, 'test_acc': 0.7226277372262774}
{'fold': 9, 'epoch': 42, 'train_loss': 0.4920012554258036, 'val_loss': 0.4847740697744699, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 43, 'train_loss': 0.4946564052371793, 'val_loss': 0.502275046930986, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4825612062955425, 'val_loss': 0.5157701070001236, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 45, 'train_loss': 0.49228666211566785, 'val_loss': 0.489779790242513, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 46, 'train_loss': 0.4901986821869574, 'val_loss': 0.48934050372047144, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 47, 'train_loss': 0.4771618513875344, 'val_loss': 0.4817355195391207, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 48, 'train_loss': 0.500022904716269, 'val_loss': 0.5076561809456261, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 49, 'train_loss': 0.480222941605134, 'val_loss': 0.5106926822894391, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 50, 'train_loss': 0.4948797051923989, 'val_loss': 0.5056055560889325, 'test_acc': 0.732360097323601}
{'fold': 9, 'epoch': 51, 'train_loss': 0.4912982142174621, 'val_loss': 0.541854213334058, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 52, 'train_loss': 0.48558396797110565, 'val_loss': 0.493204360460713, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 53, 'train_loss': 0.4840382233153295, 'val_loss': 0.5033086326580558, 'test_acc': 0.708029197080292}
{'fold': 9, 'epoch': 54, 'train_loss': 0.48112907106568925, 'val_loss': 0.5099685859216101, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 55, 'train_loss': 0.4785057304839438, 'val_loss': 0.5023692564952692, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 56, 'train_loss': 0.47907412907792996, 'val_loss': 0.4640156706464262, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 57, 'train_loss': 0.4686797029200552, 'val_loss': 0.4789219413070493, 'test_acc': 0.7128953771289538}
{'fold': 9, 'epoch': 58, 'train_loss': 0.47600600884778654, 'val_loss': 0.492435466924143, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 59, 'train_loss': 0.4619492815213772, 'val_loss': 0.4943107340457666, 'test_acc': 0.7420924574209246}
{'fold': 9, 'epoch': 60, 'train_loss': 0.46283584057269594, 'val_loss': 0.48092037859914366, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 61, 'train_loss': 0.4464096457418734, 'val_loss': 0.45668589227681033, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 62, 'train_loss': 0.47237763396144783, 'val_loss': 0.4908519591728266, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 63, 'train_loss': 0.4939897626710924, 'val_loss': 0.4870565975959574, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 64, 'train_loss': 0.4791439382989331, 'val_loss': 0.4922615014143524, 'test_acc': 0.7250608272506083}
{'fold': 9, 'epoch': 65, 'train_loss': 0.46053799029684417, 'val_loss': 0.5039619970205637, 'test_acc': 0.7274939172749392}
{'fold': 9, 'epoch': 66, 'train_loss': 0.4742078736170655, 'val_loss': 0.45524867143654185, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 67, 'train_loss': 0.4743335187580174, 'val_loss': 0.47908328397430644, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 68, 'train_loss': 0.46133736029738637, 'val_loss': 0.5070356243718279, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 69, 'train_loss': 0.4633905883978173, 'val_loss': 0.4904884978802535, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 70, 'train_loss': 0.46925859396185027, 'val_loss': 0.5241372649106956, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 71, 'train_loss': 0.467393546498895, 'val_loss': 0.4902886300191392, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 72, 'train_loss': 0.48155111233973447, 'val_loss': 0.507023980727741, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 73, 'train_loss': 0.4881620595634999, 'val_loss': 0.48141004743367216, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 74, 'train_loss': 0.48310732029360287, 'val_loss': 0.4947096808403368, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 75, 'train_loss': 0.46612926338710925, 'val_loss': 0.496055484688195, 'test_acc': 0.7445255474452555}
{'fold': 9, 'epoch': 76, 'train_loss': 0.4625863097154891, 'val_loss': 0.5035241560924372, 'test_acc': 0.7566909975669099}
{'fold': 9, 'epoch': 77, 'train_loss': 0.4789153383886147, 'val_loss': 0.47014550861070914, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 78, 'train_loss': 0.45767760414566727, 'val_loss': 0.4843882075771508, 'test_acc': 0.7372262773722628}
{'fold': 9, 'epoch': 79, 'train_loss': 0.454389844497625, 'val_loss': 0.46701218031908764, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 80, 'train_loss': 0.4393177238404026, 'val_loss': 0.46776871089517635, 'test_acc': 0.7688564476885644}
{'fold': 9, 'epoch': 81, 'train_loss': 0.4409426727724191, 'val_loss': 0.4504744954352831, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 82, 'train_loss': 0.4353648168907258, 'val_loss': 0.44687739834008133, 'test_acc': 0.7469586374695864}
{'fold': 9, 'epoch': 83, 'train_loss': 0.44670854657525855, 'val_loss': 0.4796564468792175, 'test_acc': 0.7615571776155717}
{'fold': 9, 'epoch': 84, 'train_loss': 0.4416049685501414, 'val_loss': 0.46020506188237176, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 85, 'train_loss': 0.4399507770138065, 'val_loss': 0.47071299587723114, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 86, 'train_loss': 0.44830154161673685, 'val_loss': 0.4583155569368905, 'test_acc': 0.7591240875912408}
{'fold': 9, 'epoch': 87, 'train_loss': 0.4360030832226839, 'val_loss': 0.4859838532125283, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 88, 'train_loss': 0.4442006772307004, 'val_loss': 0.4663166524139924, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 89, 'train_loss': 0.4353528126457892, 'val_loss': 0.4801186030218491, 'test_acc': 0.7177615571776156}
{'fold': 9, 'epoch': 90, 'train_loss': 0.4225942099471452, 'val_loss': 0.4975485001167242, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 91, 'train_loss': 0.46440376635015446, 'val_loss': 0.4831575593229048, 'test_acc': 0.7153284671532847}
{'fold': 9, 'epoch': 92, 'train_loss': 0.5562703538405054, 'val_loss': 0.537476667232467, 'test_acc': 0.7347931873479319}
{'fold': 9, 'epoch': 93, 'train_loss': 0.5534683897402455, 'val_loss': 0.5564408917206627, 'test_acc': 0.6982968369829684}
{'fold': 9, 'epoch': 94, 'train_loss': 0.534254354064482, 'val_loss': 0.5201482007103245, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 95, 'train_loss': 0.4823580165269021, 'val_loss': 0.4753143305906124, 'test_acc': 0.7493917274939172}
{'fold': 9, 'epoch': 96, 'train_loss': 0.46015291514187834, 'val_loss': 0.489209298089763, 'test_acc': 0.7518248175182481}
{'fold': 9, 'epoch': 97, 'train_loss': 0.4796165401193057, 'val_loss': 0.6496359019963991, 'test_acc': 0.7396593673965937}
{'fold': 9, 'epoch': 98, 'train_loss': 0.4597824784380966, 'val_loss': 0.594542644899837, 'test_acc': 0.754257907542579}
{'fold': 9, 'epoch': 99, 'train_loss': 0.44987543039658356, 'val_loss': 0.5187518358810684, 'test_acc': 0.7299270072992701}
{'fold': 9, 'epoch': 100, 'train_loss': 0.473513634535518, 'val_loss': 0.49265236634117554, 'test_acc': 0.754257907542579}
Val Loss: 0.4918, Test Accuracy: 0.758 ± 0.019, Duration: 63.638
Best result - 0.758 ± 0.019
--
PROTEINS - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 1.082474788297574, 'val_loss': 0.6613236160965653, 'test_acc': 0.6666666666666666}
{'fold': 9, 'epoch': 2, 'train_loss': 0.6544735479167548, 'val_loss': 0.6177159214878941, 'test_acc': 0.6126126126126126}
{'fold': 9, 'epoch': 3, 'train_loss': 0.618577550557326, 'val_loss': 0.5428891568570524, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 4, 'train_loss': 0.6144415335489309, 'val_loss': 0.5536671200314084, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 5, 'train_loss': 0.5993062109925797, 'val_loss': 0.5609319188573338, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 6, 'train_loss': 0.5788998302787242, 'val_loss': 0.5710519670366166, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 7, 'train_loss': 0.577993549883165, 'val_loss': 0.5198246036563907, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 8, 'train_loss': 0.5812892770660177, 'val_loss': 0.5361051129865216, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 9, 'train_loss': 0.5849987043676151, 'val_loss': 0.5331425709767385, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 10, 'train_loss': 0.5561378323938175, 'val_loss': 0.5168000985910227, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 11, 'train_loss': 0.5445556245936556, 'val_loss': 0.5152666590235255, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5533450381656823, 'val_loss': 0.52162758079735, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 13, 'train_loss': 0.5322673184301717, 'val_loss': 0.5347325093037373, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 14, 'train_loss': 0.5293651177024199, 'val_loss': 0.5462731713647241, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 15, 'train_loss': 0.5243447406591657, 'val_loss': 0.5118419887783291, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 16, 'train_loss': 0.5129708174101832, 'val_loss': 0.5188521651534347, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 17, 'train_loss': 0.5068709344909381, 'val_loss': 0.4965858115806236, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 18, 'train_loss': 0.5025603555134518, 'val_loss': 0.5253063236270938, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 19, 'train_loss': 0.5090910772281865, 'val_loss': 0.5125064334353885, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 20, 'train_loss': 0.4998639504917543, 'val_loss': 0.503149565275725, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 21, 'train_loss': 0.50585801178625, 'val_loss': 0.5248278368700732, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 22, 'train_loss': 0.510399706034548, 'val_loss': 0.5090514174452773, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 23, 'train_loss': 0.5007132195463084, 'val_loss': 0.5220520431930954, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 24, 'train_loss': 0.49918570261611683, 'val_loss': 0.5121193275795327, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 25, 'train_loss': 0.51173931099349, 'val_loss': 0.5202467291204779, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 26, 'train_loss': 0.5045450996402672, 'val_loss': 0.5167575870548282, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 27, 'train_loss': 0.48072800277727085, 'val_loss': 0.5812232215125281, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 28, 'train_loss': 0.48248381800656903, 'val_loss': 0.5102790626319679, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 29, 'train_loss': 0.4816656233084322, 'val_loss': 0.5305089177312078, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 30, 'train_loss': 0.48090102576246163, 'val_loss': 0.5397070120046804, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 31, 'train_loss': 0.46633927696347904, 'val_loss': 0.5768966674804688, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 32, 'train_loss': 0.4728360451699374, 'val_loss': 0.5339754121797579, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 33, 'train_loss': 0.48580679026517, 'val_loss': 0.5315377175270974, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 34, 'train_loss': 0.4918859263863226, 'val_loss': 0.5319556158942145, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 35, 'train_loss': 0.46384165068935734, 'val_loss': 0.6083208376222903, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 36, 'train_loss': 0.4837284676153652, 'val_loss': 0.5222524694494299, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 37, 'train_loss': 0.489250986564039, 'val_loss': 0.5676007657437712, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 38, 'train_loss': 0.4895674566227178, 'val_loss': 0.5243050515114724, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 39, 'train_loss': 0.46811441025214834, 'val_loss': 0.48556436074746623, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 40, 'train_loss': 0.4574186520276781, 'val_loss': 0.5354443111935178, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 41, 'train_loss': 0.4657036352371661, 'val_loss': 0.5326448045335375, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 42, 'train_loss': 0.46693115998331275, 'val_loss': 0.5079033997681763, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 43, 'train_loss': 0.45590358514309465, 'val_loss': 0.5041551675882425, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 44, 'train_loss': 0.4490810917684125, 'val_loss': 0.4921695262462169, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 45, 'train_loss': 0.465348905452039, 'val_loss': 0.5074532654908326, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 46, 'train_loss': 0.45387743625844235, 'val_loss': 0.5364296715538781, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 47, 'train_loss': 0.44212998765887634, 'val_loss': 0.5128450823259784, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 48, 'train_loss': 0.4935325063706515, 'val_loss': 0.5319100113602372, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 49, 'train_loss': 0.4880366091241205, 'val_loss': 0.5173025388975401, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 50, 'train_loss': 0.4643459071882914, 'val_loss': 0.5055202449764218, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 51, 'train_loss': 0.4760257263911426, 'val_loss': 0.4718190786000845, 'test_acc': 0.7387387387387387}
{'fold': 9, 'epoch': 52, 'train_loss': 0.446606660130048, 'val_loss': 0.50006306278813, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 53, 'train_loss': 0.4555926153689255, 'val_loss': 0.48754728162610855, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 54, 'train_loss': 0.4624442505649177, 'val_loss': 0.5003711597339527, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 55, 'train_loss': 0.4477772432746309, 'val_loss': 0.5098785194190772, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 56, 'train_loss': 0.4625891416963905, 'val_loss': 0.5200599120543884, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 57, 'train_loss': 0.45088162804158316, 'val_loss': 0.4874191627846108, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 58, 'train_loss': 0.45151477473485885, 'val_loss': 0.5005422712446334, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 59, 'train_loss': 0.42493522297653685, 'val_loss': 0.5159833409764745, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 60, 'train_loss': 0.42943500920578287, 'val_loss': 0.5257171768325943, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 61, 'train_loss': 0.4468228621983234, 'val_loss': 0.5156869286889428, 'test_acc': 0.7927927927927928}
{'fold': 9, 'epoch': 62, 'train_loss': 0.43465102297555985, 'val_loss': 0.5234032708245355, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 63, 'train_loss': 0.4313788083533513, 'val_loss': 0.5691614236917582, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 64, 'train_loss': 0.42480636465830435, 'val_loss': 0.5389950039150478, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 65, 'train_loss': 0.4118370703106911, 'val_loss': 0.5138868383459143, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 66, 'train_loss': 0.39215071594674983, 'val_loss': 0.5201060664546382, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 67, 'train_loss': 0.4029005470567547, 'val_loss': 0.46602726841832065, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 68, 'train_loss': 0.40550427307718667, 'val_loss': 0.4676257984058277, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 69, 'train_loss': 0.38914034991419544, 'val_loss': 0.5243989755441477, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 70, 'train_loss': 0.4090116869587154, 'val_loss': 0.47847218556447074, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 71, 'train_loss': 0.40635848580519895, 'val_loss': 0.5241599211821685, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 72, 'train_loss': 0.43488285922174635, 'val_loss': 0.5018492690077773, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 73, 'train_loss': 0.43576807248605753, 'val_loss': 0.5573507085576788, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 74, 'train_loss': 0.43575575556417906, 'val_loss': 0.658013696069116, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 75, 'train_loss': 0.4303919816124185, 'val_loss': 0.6239417136252463, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 76, 'train_loss': 0.3919041898649282, 'val_loss': 0.5192100937302048, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 77, 'train_loss': 0.39680334129585026, 'val_loss': 0.5588982728150513, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 78, 'train_loss': 0.39180955635310555, 'val_loss': 0.5210956539119687, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 79, 'train_loss': 0.39354018233172955, 'val_loss': 0.5980094016135276, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 80, 'train_loss': 0.389188474577284, 'val_loss': 0.5630531654701577, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 81, 'train_loss': 0.37251250029412986, 'val_loss': 0.5800425898921382, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 82, 'train_loss': 0.36373486904182817, 'val_loss': 0.5876899925438134, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 83, 'train_loss': 0.4157759468630374, 'val_loss': 0.47917223406267595, 'test_acc': 0.7207207207207207}
{'fold': 9, 'epoch': 84, 'train_loss': 0.4021193533634096, 'val_loss': 0.5613672754786037, 'test_acc': 0.7747747747747747}
{'fold': 9, 'epoch': 85, 'train_loss': 0.3920347012587803, 'val_loss': 0.5785184293179899, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 86, 'train_loss': 0.39865639939586456, 'val_loss': 0.6146368078283362, 'test_acc': 0.7117117117117117}
{'fold': 9, 'epoch': 87, 'train_loss': 0.41464032147468544, 'val_loss': 0.5844906643704251, 'test_acc': 0.6846846846846847}
{'fold': 9, 'epoch': 88, 'train_loss': 0.4018264395152412, 'val_loss': 0.5686224172781179, 'test_acc': 0.7657657657657657}
{'fold': 9, 'epoch': 89, 'train_loss': 0.39877570622028863, 'val_loss': 0.6367509859102266, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 90, 'train_loss': 0.37015182301652955, 'val_loss': 0.6832115242073128, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 91, 'train_loss': 0.4160621462914545, 'val_loss': 0.6311212488122888, 'test_acc': 0.7567567567567568}
{'fold': 9, 'epoch': 92, 'train_loss': 0.3786986126709734, 'val_loss': 0.5903534073013443, 'test_acc': 0.7027027027027027}
{'fold': 9, 'epoch': 93, 'train_loss': 0.396276445735317, 'val_loss': 0.5821374429238809, 'test_acc': 0.7837837837837838}
{'fold': 9, 'epoch': 94, 'train_loss': 0.3917094953734465, 'val_loss': 0.4898292953903611, 'test_acc': 0.6936936936936937}
{'fold': 9, 'epoch': 95, 'train_loss': 0.42331936340005577, 'val_loss': 0.5562161282376126, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 96, 'train_loss': 0.3914784841933502, 'val_loss': 0.5121309435045397, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 97, 'train_loss': 0.37689152744616455, 'val_loss': 0.5815711493964668, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 98, 'train_loss': 0.3715251792560924, 'val_loss': 0.6524463000598254, 'test_acc': 0.7297297297297297}
{'fold': 9, 'epoch': 99, 'train_loss': 0.36535738289824504, 'val_loss': 0.6607975315403294, 'test_acc': 0.7477477477477478}
{'fold': 9, 'epoch': 100, 'train_loss': 0.36579851975077066, 'val_loss': 0.6037717252164274, 'test_acc': 0.7657657657657657}
Val Loss: 0.5028, Test Accuracy: 0.741 ± 0.035, Duration: 17.308
Best result - 0.741 ± 0.035
--
COLLAB - GCN
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\data\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
D:\Program\Anaconda\envs\pyg\lib\site-packages\torch_geometric\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
{'fold': 9, 'epoch': 1, 'train_loss': 29.38701151752472, 'val_loss': 1.2103809356689452, 'test_acc': 0.646}
{'fold': 9, 'epoch': 2, 'train_loss': 3.7962533922195436, 'val_loss': 0.8389760971069335, 'test_acc': 0.672}
{'fold': 9, 'epoch': 3, 'train_loss': 0.9774426536560059, 'val_loss': 0.6588756408691406, 'test_acc': 0.718}
{'fold': 9, 'epoch': 4, 'train_loss': 0.7425100183486939, 'val_loss': 0.7175459518432618, 'test_acc': 0.71}
{'fold': 9, 'epoch': 5, 'train_loss': 0.9016332092285156, 'val_loss': 0.6220507202148438, 'test_acc': 0.706}
{'fold': 9, 'epoch': 6, 'train_loss': 0.6770855464935303, 'val_loss': 0.6470729370117188, 'test_acc': 0.694}
{'fold': 9, 'epoch': 7, 'train_loss': 1.4520068731307982, 'val_loss': 0.6819199066162109, 'test_acc': 0.728}
{'fold': 9, 'epoch': 8, 'train_loss': 0.67758966588974, 'val_loss': 0.5905439987182617, 'test_acc': 0.734}
{'fold': 9, 'epoch': 9, 'train_loss': 0.609349009513855, 'val_loss': 0.61364599609375, 'test_acc': 0.732}
{'fold': 9, 'epoch': 10, 'train_loss': 0.6068364686965942, 'val_loss': 0.61858056640625, 'test_acc': 0.746}
{'fold': 9, 'epoch': 11, 'train_loss': 0.6854505395889282, 'val_loss': 0.6236649169921875, 'test_acc': 0.716}
{'fold': 9, 'epoch': 12, 'train_loss': 0.5986025514602661, 'val_loss': 0.5812158355712891, 'test_acc': 0.728}
{'fold': 9, 'epoch': 13, 'train_loss': 2.0145116319656373, 'val_loss': 0.6664032669067382, 'test_acc': 0.71}
{'fold': 9, 'epoch': 14, 'train_loss': 0.6701081433296203, 'val_loss': 0.6465578308105469, 'test_acc': 0.7}
{'fold': 9, 'epoch': 15, 'train_loss': 0.6683394732475281, 'val_loss': 0.6643284683227539, 'test_acc': 0.704}
{'fold': 9, 'epoch': 16, 'train_loss': 0.7194312686920166, 'val_loss': 0.6553950576782227, 'test_acc': 0.69}
{'fold': 9, 'epoch': 17, 'train_loss': 0.6415896849632263, 'val_loss': 0.5915071258544922, 'test_acc': 0.72}
{'fold': 9, 'epoch': 18, 'train_loss': 0.7943196830749512, 'val_loss': 0.6235944595336914, 'test_acc': 0.708}
{'fold': 9, 'epoch': 19, 'train_loss': 0.6701365685462952, 'val_loss': 0.6119804611206054, 'test_acc': 0.724}
{'fold': 9, 'epoch': 20, 'train_loss': 0.6294117474555969, 'val_loss': 0.6382007598876953, 'test_acc': 0.728}
{'fold': 9, 'epoch': 21, 'train_loss': 0.6743972411155701, 'val_loss': 0.6467738952636719, 'test_acc': 0.698}
{'fold': 9, 'epoch': 22, 'train_loss': 0.6672249283790589, 'val_loss': 0.6620889663696289, 'test_acc': 0.682}
{'fold': 9, 'epoch': 23, 'train_loss': 2.571371386528015, 'val_loss': 0.6550954971313476, 'test_acc': 0.714}
{'fold': 9, 'epoch': 24, 'train_loss': 28.22205462741852, 'val_loss': 0.9533194580078125, 'test_acc': 0.532}
{'fold': 9, 'epoch': 25, 'train_loss': 23.01686628675461, 'val_loss': 0.922272689819336, 'test_acc': 0.52}
{'fold': 9, 'epoch': 26, 'train_loss': 0.9927565612792969, 'val_loss': 0.9103375549316406, 'test_acc': 0.53}
{'fold': 9, 'epoch': 27, 'train_loss': 43.65329646492005, 'val_loss': 0.9191038513183594, 'test_acc': 0.532}
{'fold': 9, 'epoch': 28, 'train_loss': 19.96462836074829, 'val_loss': 0.8718172607421875, 'test_acc': 0.532}
{'fold': 9, 'epoch': 29, 'train_loss': 3.328391285896301, 'val_loss': 0.8872269744873047, 'test_acc': 0.526}
{'fold': 9, 'epoch': 30, 'train_loss': 0.9225193047523499, 'val_loss': 0.8714831237792968, 'test_acc': 0.538}
{'fold': 9, 'epoch': 31, 'train_loss': 0.87046613073349, 'val_loss': 0.9123213500976562, 'test_acc': 0.534}
{'fold': 9, 'epoch': 32, 'train_loss': 0.8684385447502136, 'val_loss': 0.8624248657226562, 'test_acc': 0.542}
{'fold': 9, 'epoch': 33, 'train_loss': 0.8398778524398803, 'val_loss': 0.891452133178711, 'test_acc': 0.544}
{'fold': 9, 'epoch': 34, 'train_loss': 0.847641809463501, 'val_loss': 0.8653267059326172, 'test_acc': 0.548}
{'fold': 9, 'epoch': 35, 'train_loss': 0.8437374053001404, 'val_loss': 0.888245361328125, 'test_acc': 0.544}
{'fold': 9, 'epoch': 36, 'train_loss': 0.8657706327438355, 'val_loss': 0.8691027526855468, 'test_acc': 0.552}
{'fold': 9, 'epoch': 37, 'train_loss': 0.8138248324394226, 'val_loss': 0.8714689025878907, 'test_acc': 0.552}
{'fold': 9, 'epoch': 38, 'train_loss': 0.8069362292289733, 'val_loss': 0.8611532897949219, 'test_acc': 0.558}
{'fold': 9, 'epoch': 39, 'train_loss': 0.8384735698699951, 'val_loss': 0.8823251037597656, 'test_acc': 0.56}
{'fold': 9, 'epoch': 40, 'train_loss': 0.9226308641433716, 'val_loss': 0.8769360656738281, 'test_acc': 0.574}
{'fold': 9, 'epoch': 41, 'train_loss': 0.7873652982711792, 'val_loss': 0.8509706726074219, 'test_acc': 0.566}
{'fold': 9, 'epoch': 42, 'train_loss': 0.7976049175262451, 'val_loss': 0.8286830291748047, 'test_acc': 0.568}
{'fold': 9, 'epoch': 43, 'train_loss': 0.7629016699790955, 'val_loss': 0.8401265716552735, 'test_acc': 0.576}
{'fold': 9, 'epoch': 44, 'train_loss': 0.7757397646903992, 'val_loss': 0.8491063232421875, 'test_acc': 0.568}
{'fold': 9, 'epoch': 45, 'train_loss': 0.7643352560997009, 'val_loss': 0.8394673614501953, 'test_acc': 0.564}
{'fold': 9, 'epoch': 46, 'train_loss': 0.7606561055183411, 'val_loss': 0.8220145263671875, 'test_acc': 0.568}
{'fold': 9, 'epoch': 47, 'train_loss': 0.7542906975746155, 'val_loss': 0.8286269073486328, 'test_acc': 0.564}
{'fold': 9, 'epoch': 48, 'train_loss': 0.7466543788909912, 'val_loss': 0.8174141845703125, 'test_acc': 0.568}
{'fold': 9, 'epoch': 49, 'train_loss': 1.0196782503128052, 'val_loss': 0.8112063446044921, 'test_acc': 0.564}
{'fold': 9, 'epoch': 50, 'train_loss': 8.408341549396514, 'val_loss': 0.8415811614990234, 'test_acc': 0.576}
{'fold': 9, 'epoch': 51, 'train_loss': 0.7856336841583252, 'val_loss': 0.9060853729248047, 'test_acc': 0.58}
{'fold': 9, 'epoch': 52, 'train_loss': 1.0377528347969056, 'val_loss': 0.8617549133300781, 'test_acc': 0.568}
{'fold': 9, 'epoch': 53, 'train_loss': 0.9860602574348449, 'val_loss': 0.8241897583007812, 'test_acc': 0.54}
{'fold': 9, 'epoch': 54, 'train_loss': 0.7836640582084656, 'val_loss': 0.8231975402832031, 'test_acc': 0.548}
{'fold': 9, 'epoch': 55, 'train_loss': 2.338050058364868, 'val_loss': 0.8410017242431641, 'test_acc': 0.554}
{'fold': 9, 'epoch': 56, 'train_loss': 0.7888473005294799, 'val_loss': 0.8515973358154297, 'test_acc': 0.552}
{'fold': 9, 'epoch': 57, 'train_loss': 1.3372099947929383, 'val_loss': 0.88295654296875, 'test_acc': 0.548}
{'fold': 9, 'epoch': 58, 'train_loss': 0.7994316115379333, 'val_loss': 0.8712030029296876, 'test_acc': 0.558}
{'fold': 9, 'epoch': 59, 'train_loss': 0.8354979252815247, 'val_loss': 0.8418717956542969, 'test_acc': 0.564}
{'fold': 9, 'epoch': 60, 'train_loss': 0.7731658039093018, 'val_loss': 0.8185788879394531, 'test_acc': 0.564}
{'fold': 9, 'epoch': 61, 'train_loss': 0.7609639239311218, 'val_loss': 0.8265538635253906, 'test_acc': 0.56}
{'fold': 9, 'epoch': 62, 'train_loss': 0.7430832872390747, 'val_loss': 0.8471159210205078, 'test_acc': 0.554}
{'fold': 9, 'epoch': 63, 'train_loss': 0.7353112726211548, 'val_loss': 0.8453473510742188, 'test_acc': 0.554}
{'fold': 9, 'epoch': 64, 'train_loss': 0.7321512532234192, 'val_loss': 0.8125858917236328, 'test_acc': 0.564}
{'fold': 9, 'epoch': 65, 'train_loss': 5.918564344882965, 'val_loss': 0.8252313232421875, 'test_acc': 0.56}
{'fold': 9, 'epoch': 66, 'train_loss': 0.7147485566139221, 'val_loss': 0.831458023071289, 'test_acc': 0.572}
{'fold': 9, 'epoch': 67, 'train_loss': 0.7122307209968567, 'val_loss': 0.8519388732910156, 'test_acc': 0.566}
{'fold': 9, 'epoch': 68, 'train_loss': 0.7163795714378357, 'val_loss': 0.8163081359863281, 'test_acc': 0.572}
{'fold': 9, 'epoch': 69, 'train_loss': 0.7033919100761413, 'val_loss': 0.8190372619628906, 'test_acc': 0.564}
{'fold': 9, 'epoch': 70, 'train_loss': 0.6950437669754028, 'val_loss': 0.8388788146972657, 'test_acc': 0.56}
{'fold': 9, 'epoch': 71, 'train_loss': 0.6857768287658691, 'val_loss': 0.8743398132324218, 'test_acc': 0.566}
{'fold': 9, 'epoch': 72, 'train_loss': 0.720217490196228, 'val_loss': 0.8484466552734375, 'test_acc': 0.564}
{'fold': 9, 'epoch': 73, 'train_loss': 0.6900922839641571, 'val_loss': 0.8637152404785157, 'test_acc': 0.556}
{'fold': 9, 'epoch': 74, 'train_loss': 0.7061119194030762, 'val_loss': 0.8692410125732422, 'test_acc': 0.562}
{'fold': 9, 'epoch': 75, 'train_loss': 0.6880809359550476, 'val_loss': 0.9071546173095704, 'test_acc': 0.57}
{'fold': 9, 'epoch': 76, 'train_loss': 0.6904595203399658, 'val_loss': 0.9193286895751953, 'test_acc': 0.572}
{'fold': 9, 'epoch': 77, 'train_loss': 0.6885011930465699, 'val_loss': 0.9033391571044922, 'test_acc': 0.572}
{'fold': 9, 'epoch': 78, 'train_loss': 0.6741658563613891, 'val_loss': 0.8902020874023437, 'test_acc': 0.584}
{'fold': 9, 'epoch': 79, 'train_loss': 0.6783220472335816, 'val_loss': 0.8913107452392578, 'test_acc': 0.578}
{'fold': 9, 'epoch': 80, 'train_loss': 0.6728865480422974, 'val_loss': 0.8832555999755859, 'test_acc': 0.558}
{'fold': 9, 'epoch': 81, 'train_loss': 0.9927861108779907, 'val_loss': 0.9358379211425781, 'test_acc': 0.56}
{'fold': 9, 'epoch': 82, 'train_loss': 0.6967048268318177, 'val_loss': 0.9294143524169922, 'test_acc': 0.564}
{'fold': 9, 'epoch': 83, 'train_loss': 98.14303901004791, 'val_loss': 0.8955439147949219, 'test_acc': 0.56}
{'fold': 9, 'epoch': 84, 'train_loss': 0.7225998139381409, 'val_loss': 0.8411190338134765, 'test_acc': 0.564}
{'fold': 9, 'epoch': 85, 'train_loss': 0.9200655484199524, 'val_loss': 0.8523311767578124, 'test_acc': 0.558}
{'fold': 9, 'epoch': 86, 'train_loss': 0.7089251232147217, 'val_loss': 0.8619379272460937, 'test_acc': 0.556}
{'fold': 9, 'epoch': 87, 'train_loss': 0.69495703125, 'val_loss': 0.9147877044677735, 'test_acc': 0.564}
{'fold': 9, 'epoch': 88, 'train_loss': 0.7083721523284912, 'val_loss': 0.8475607299804687, 'test_acc': 0.556}
{'fold': 9, 'epoch': 89, 'train_loss': 0.7021819462776184, 'val_loss': 0.8483115081787109, 'test_acc': 0.568}
{'fold': 9, 'epoch': 90, 'train_loss': 0.6828937153816224, 'val_loss': 0.8692189025878906, 'test_acc': 0.562}
{'fold': 9, 'epoch': 91, 'train_loss': 1.9464364914894103, 'val_loss': 0.9044579772949218, 'test_acc': 0.562}
{'fold': 9, 'epoch': 92, 'train_loss': 0.66948619556427, 'val_loss': 0.9181453247070313, 'test_acc': 0.55}
{'fold': 9, 'epoch': 93, 'train_loss': 0.6657697553634644, 'val_loss': 0.9313527374267578, 'test_acc': 0.566}
{'fold': 9, 'epoch': 94, 'train_loss': 0.6573118224143982, 'val_loss': 0.9293292999267578, 'test_acc': 0.566}
{'fold': 9, 'epoch': 95, 'train_loss': 0.6430976634025574, 'val_loss': 0.9467908172607422, 'test_acc': 0.57}
{'fold': 9, 'epoch': 96, 'train_loss': 0.6402650918960572, 'val_loss': 0.9467108154296875, 'test_acc': 0.566}
{'fold': 9, 'epoch': 97, 'train_loss': 0.6493286476135254, 'val_loss': 0.917505859375, 'test_acc': 0.568}
{'fold': 9, 'epoch': 98, 'train_loss': 0.684055675983429, 'val_loss': 0.9028382568359375, 'test_acc': 0.574}
{'fold': 9, 'epoch': 99, 'train_loss': 0.6588165106773376, 'val_loss': 0.8746994781494141, 'test_acc': 0.576}
{'fold': 9, 'epoch': 100, 'train_loss': 0.8393991794586182, 'val_loss': 0.9846388244628906, 'test_acc': 0.558}
Val Loss: 0.6301, Test Accuracy: 0.692 ± 0.074, Duration: 158.767
Best result - 0.692 ± 0.074
--
IMDB-MULTI - GCN(
  (conv1): GraphConv(89, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=3, bias=True)
): 0.461 ± 0.034
MUTAG - GCN(
  (conv1): GraphConv(7, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=2, bias=True)
): 0.782 ± 0.090
IMDB-BINARY - GCN(
  (conv1): GraphConv(136, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=2, bias=True)
): 0.705 ± 0.049
REDDIT-BINARY - GCN(
  (conv1): GraphConv(1, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=2, bias=True)
): 0.860 ± 0.032
DD - GCN(
  (conv1): GraphConv(89, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=2, bias=True)
): 0.754 ± 0.027
NCI1 - GCN(
  (conv1): GraphConv(37, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=2, bias=True)
): 0.758 ± 0.019
PROTEINS - GCN(
  (conv1): GraphConv(3, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=2, bias=True)
): 0.741 ± 0.035
COLLAB - GCN(
  (conv1): GraphConv(492, 128)
  (pool1): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv2): GraphConv(128, 128)
  (pool2): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (conv3): GraphConv(128, 128)
  (pool3): TopKPooling(128, ratio=0.8, multiplier=1.0)
  (lin1): Linear(in_features=256, out_features=128, bias=True)
  (lin2): Linear(in_features=128, out_features=64, bias=True)
  (lin3): Linear(in_features=64, out_features=3, bias=True)
): 0.692 ± 0.074

Process finished with exit code 0
